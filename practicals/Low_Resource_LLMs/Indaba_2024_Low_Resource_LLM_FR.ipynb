{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP Ã  ressources limitÃ©es**\n",
    "\n",
    "<img src=\"https://dli2024prac.blob.core.windows.net/content/lr_llm_header.png\" width=\"60%\" allign =\"center\"/>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Indaba_2024_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
    "\n",
    "Â© Deep Learning Indaba 2024. Licence Apache 2.0.\n",
    "\n",
    "**Auteurs :**\n",
    "- Ali Zaidi\n",
    "- Aya Salama\n",
    "- Khalil Mrini\n",
    "- Steven Kolawole \n",
    "\n",
    "**Introduction :**\n",
    "\n",
    "Le TAL Ã  ressources limitÃ©es (Traitement Automatique des Langues, ou NLP en anglais, Natural Language Processing) fait rÃ©fÃ©rence Ã  l'Ã©tude et au dÃ©veloppement de modÃ¨les et de systÃ¨mes TAL pour des langues, des tÃ¢ches ou des domaines qui disposent de peu de donnÃ©es et de ressources. Cela peut inclure des langues avec moins de corpus textes numÃ©riques, d'outils informatiques limitÃ©s ou de recherches linguistiques moins dÃ©veloppÃ©es.\n",
    "\n",
    "**Principaux dÃ©fis du TAL Ã  ressources limitÃ©es**\n",
    "\n",
    "1. **RaretÃ© des donnÃ©es :**\n",
    "   - **DonnÃ©es d'entraÃ®nement limitÃ©es :** De nombreuses langues manquent de grands corpus annotÃ©s nÃ©cessaires pour entraÃ®ner les modÃ¨les TAL.\n",
    "   - **Absence de modÃ¨les prÃ©-entraÃ®nÃ©s :** Les modÃ¨les TAL populaires comme BERT, GPT et autres ne sont souvent pas disponibles pour les langues Ã  ressources limitÃ©es.\n",
    "\n",
    "2. **DiversitÃ© linguistique :**\n",
    "   - **ComplexitÃ© morphologique :** Certaines langues ont des structures grammaticales complexes et une richesse morphologique.\n",
    "   - **Variations dialectales :** Un manque de versions standardisÃ©es peut compliquer les tÃ¢ches TAL.\n",
    "\n",
    "3. **Limitations des ressources :**\n",
    "   - **Contraintes computationnelles :** Les scÃ©narios Ã  faibles ressources impliquent souvent un accÃ¨s limitÃ© Ã  la puissance de calcul et au stockage.\n",
    "   - **Expertise et outils :** Moins d'experts linguistiques et moins d'outils TAL sont adaptÃ©s Ã  ces langues.\n",
    "\n",
    "**Sujets :**\n",
    "\n",
    "Contenu : [Traitement Automatique des Langues, Ressources LimitÃ©es, ModÃ¨les de Langages, RÃ©glage Fin Efficace en ParamÃ¨tres, Adaptation]  \n",
    "Niveau : [IntermÃ©diaire]\n",
    "\n",
    "\n",
    "**Objectifs/CompÃ©tences visÃ©es :**\n",
    "\n",
    "- Explorer les dÃ©fis de la raretÃ© des donnÃ©es\n",
    "- Explorer les limitations des ressources de calcul et les aborder avec un finetuning efficace des paramÃ¨tres\n",
    "- Comparer les performances entre les petits (BERT) et les grands (GPT) modÃ¨les de langage sur des langues/tÃ¢ches Ã  faibles ressources.\n",
    "  \n",
    "\n",
    "**PrÃ©requis :**\n",
    "\n",
    "[Connaissances requises pour cette pratique. Vous pouvez lier une session de track parallÃ¨le pertinente, des blogs, des articles, des cours, des sujets, etc.]\n",
    "_ lier des ressources sur les LLMs, BERT, articles de Masakhane sur les conversations Ã  faibles ressources\n",
    "\n",
    "**Plan :**\n",
    "\n",
    "[Points qui lient chaque section. Auto-gÃ©nÃ©rer en suivant les instructions [ici](https://stackoverflow.com/questions/67458990/how-to-automatically-generate-a-table-of-contents-in-colab-notebook).]\n",
    "\n",
    "**Avant de commencer :**\n",
    "\n",
    "[TÃ¢ches juste avant de commencer.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExÃ©cutez la cellule pour configurer les packages Python et ressources nÃ©cessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dossiers de ressources** :\n",
    "\n",
    "Les ressources dont vous aurez besoin pour rÃ©aliser ce pratique seront tÃ©lÃ©chargÃ©es lorsque vous exÃ©cuterez la prochaine cellule.\n",
    "Une fois le tÃ©lÃ©chargement et l'extraction terminÃ©s, vous aurez les dossiers suivants prÃ©sents dans le dossier \"resources\" dans le rÃ©pertoire parent :\n",
    "\n",
    "- *models* : ce dossier contient les modÃ¨les prÃ©-entraÃ®nÃ©s qui seront utilisÃ©s dans le pratique\n",
    "- *dataset* : ce dossier contient le jeu de donnÃ©es Goud-sum que nous utiliserons dans le pratique\n",
    "- *generated_responses* : ce dossier contient des rÃ©sumÃ©s prÃ©-gÃ©nÃ©rÃ©s qui seront utilisÃ©s dans la Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Installer les packages requis\n",
    "utils.install_requirements()\n",
    "\n",
    "# TÃ©lÃ©chargez et extrayez le fichier zip contenant les ressources\n",
    "utils.download_and_extract_zip(\"https://dli2024prac.blob.core.windows.net/resources/resources.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, load_metric, load_from_disk\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftConfig, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from huggingface_hub import login\n",
    "from rouge_metric import PyRouge\n",
    "import wandb\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/alizaidi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malizaidi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_HUB_TOKEN\"))\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce pratique, nous nous intÃ©ressons Ã  gÃ©nÃ©rer des titres pour les articles de presse prÃ©sentÃ©s sur le site d'actualitÃ©s [Goud.ma](www.Gound.ma).\n",
    "\n",
    "Nous aborderons cela comme une tÃ¢che de rÃ©sumÃ© oÃ¹ l'entrÃ©e est le corps d'un article de presse et la sortie est un titre appropriÃ©. L'[ensemble de donnÃ©es Goud](https://github.com/issam9/goud-summarization-dataset) contient 158k articles et leurs titres. Tous les titres sont en Darija marocaine, tandis que les articles peuvent Ãªtre en Darija marocaine, en arabe standard moderne, ou un mÃ©lange des deux (Darija marocaine en alternance de code).\n",
    "\n",
    "**Champs de donnÃ©es**\n",
    "- *article* : une chaÃ®ne de caractÃ¨res contenant le corps de l'article de presse\n",
    "- *headline* : une chaÃ®ne de caractÃ¨res contenant le titre de l'article\n",
    "- *categories* : une liste de chaÃ®nes de caractÃ¨res des catÃ©gories d'articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce que nous allons faire :\n",
    "<img src=\"./content/DLI_LR_llm_prac_1.png\" width=\"40%\" />\n",
    "\n",
    "**Dossiers de ressources** :\n",
    "\n",
    "Les ressources dont vous aurez besoin pour rÃ©aliser cet exercice font partie du dÃ©pÃ´t, vous les trouverez dans le dossier parent :\n",
    "\n",
    "- Dossier *Data* : ce dossier contient le dataset Goud-sum que nous utiliserons dans la pratique\n",
    "- Dossier *genrated_responses* : ce dossier contient des rÃ©sumÃ©s prÃ©-gÃ©nÃ©rÃ©s qui seront utilisÃ©s dans la Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MÃ©trique d'Ã©valuation : ROUGE\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) est un ensemble de mÃ©triques utilisÃ© pour Ã©valuer la qualitÃ© des rÃ©sumÃ©s en les comparant aux rÃ©sumÃ©s de rÃ©fÃ©rence (ou ground truth). ROUGE est largement utilisÃ© dans les tÃ¢ches de Traitement Automatique du Langage Naturel (NLP), en particulier pour Ã©valuer la performance des modÃ¨les de rÃ©sumÃ© de texte.\n",
    "\n",
    "![ROUGE-Base](https://i0.wp.com/blog.uptrain.ai/wp-content/uploads/2024/01/rouge-n.webp?resize=700%2C228&ssl=1)\n",
    "\n",
    "### Variants clÃ©s de ROUGE\n",
    "\n",
    "1. **ROUGE-N** : Mesure le chevauchement des n-grams entre le rÃ©sumÃ© candidat et le rÃ©sumÃ© de rÃ©fÃ©rence.\n",
    "\n",
    "![ROUGE-1](https://clementbm.github.io/assets/2021-12-23/rouge-unigrams.png)\n",
    "\n",
    "*caption:*\n",
    "$ROUGE_1 = \\frac{7}{10} = 0.7$\n",
    "\n",
    "   - **ROUGE-1** : Chevauchement des unigrams (1-gram).\n",
    "   - **ROUGE-2** : Chevauchement des bigrams (2-grams).\n",
    "   - **ROUGE-L** : Mesure la plus longue sous-sÃ©quence commune (LCS) entre les rÃ©sumÃ©s candidat et de rÃ©fÃ©rence.\n",
    "\n",
    "2. **ROUGE-L** : Mesure la plus longue sous-sÃ©quence commune (LCS) entre le rÃ©sumÃ© candidat et le rÃ©sumÃ© de rÃ©fÃ©rence. Contrairement Ã  ROUGE-N, ROUGE-L considÃ¨re la similaritÃ© de la structure au niveau de la phrase en identifiant la plus longue sÃ©quence de mots co-occurrences dans les deux rÃ©sumÃ©s.\n",
    "\n",
    "3. **ROUGE-W** : Une version pondÃ©rÃ©e de ROUGE-L qui donne plus d'importance Ã  la LCS contiguÃ«.\n",
    "\n",
    "4. **ROUGE-S** : Mesure le chevauchement des skip-bigrams, qui sont des paires de mots dans leur ordre d'apparition pouvant avoir n'importe quel nombre d'Ã©carts entre eux.\n",
    "\n",
    "### Comment ROUGE est calculÃ©\n",
    "\n",
    "Les mÃ©triques ROUGE peuvent Ãªtre calculÃ©es en termes de trois mesures :\n",
    "\n",
    "- **Recall** : Le ratio des unitÃ©s chevauchantes (n-grams, LCS, ou skip-bigrams) entre le rÃ©sumÃ© candidat et le rÃ©sumÃ© de rÃ©fÃ©rence par rapport au nombre total d'unitÃ©s dans le rÃ©sumÃ© de rÃ©fÃ©rence. Cela rÃ©pond Ã  la question : \"Quelle part du rÃ©sumÃ© de rÃ©fÃ©rence est capturÃ©e par le rÃ©sumÃ© candidat ?\".\n",
    "\n",
    "- **PrÃ©cision** : Le ratio des unitÃ©s chevauchantes entre le rÃ©sumÃ© candidat et le rÃ©sumÃ© de rÃ©fÃ©rence par rapport au nombre total d'unitÃ©s dans le rÃ©sumÃ© candidat. Cela rÃ©pond Ã  la question : \"Quelle part du rÃ©sumÃ© candidat est pertinente par rapport au rÃ©sumÃ© de rÃ©fÃ©rence ?\".\n",
    "\n",
    "- **F1-Score** : La moyenne harmonique de PrÃ©cision et Recall. Cela donne une mesure Ã©quilibrÃ©e qui considÃ¨re Ã  la fois la prÃ©cision et le recall.\n",
    "\n",
    "### Importance de ROUGE\n",
    "\n",
    "ROUGE est essentiel pour les tÃ¢ches de rÃ©sumÃ© car il fournit un moyen normalisÃ© pour Ã©valuer et comparer diffÃ©rents modÃ¨les de rÃ©sumÃ©. Des scores ROUGE plus Ã©levÃ©s indiquent gÃ©nÃ©ralement que le rÃ©sumÃ© candidat est plus similaire au rÃ©sumÃ© de rÃ©fÃ©rence, ce qui signifie que le modÃ¨le fonctionne probablement bien.\n",
    "\n",
    "#### NOTE: Mise en garde\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*8ZNpaag-Nr2GLs3A-sz0aQ.png\" alt=\"limitation 1\" width=\"250\"/>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*CLIKeyKYiR6sNA4yjIkCWg.png\" alt=\"limitation 2\" width=\"250\"/>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*667HMbjSLJhwR_xqBau3JQ.png\" alt=\"limitation 3\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "Bien que ROUGE et d'autres mÃ©triques d'Ã©valuation (par exemple, BLEU, METEOR, etc.) servent d'outils prÃ©cieux pour une Ã©valuation rapide et simple des modÃ¨les de langue, elles prÃ©sentent certaines limitations qui les rendent moins idÃ©ales. Pour commencer, elles ne parviennent pas Ã  Ã©valuer la fluiditÃ©, la cohÃ©rence et le sens gÃ©nÃ©ral des passages. Elles sont Ã©galement relativement insensibles Ã  l'ordre des mots. ROUGE mesure principalement le chevauchement lexical et peut ne pas capturer pleinement le sens sÃ©mantique ou la qualitÃ© d'un rÃ©sumÃ©. Pour ces raisons, les chercheurs cherchent encore Ã  trouver des mÃ©triques amÃ©liorÃ©es.\n",
    "\n",
    "Par consÃ©quent, ces mÃ©triques ne remplacent pas totalement l'Ã©valuation humaine, mais elles sont mieux utilisÃ©es conjointement avec les Ã©valuations humaines pour une Ã©valuation plus complÃ¨te de la qualitÃ© des rÃ©sumÃ©s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "# Load the dataset from disk\n",
    "dataset = load_from_disk(\"./data/Goud-sum/Goud-sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VÃ©rifier le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'headline', 'categories'],\n",
      "        num_rows: 139288\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'headline', 'categories'],\n",
      "        num_rows: 9497\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'headline', 'categories'],\n",
      "        num_rows: 9497\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': 'Ù…Ù†ÙŠØ± Ø§Ù„Ø¹Ù„Ù…ÙŠ Ù…Ù† Ù…Ø±Ø§ÙƒØ´: ØªØ­ÙˆÙ„ ÙØ¶Ø§Ø¡ Ù…Ù‚Ø± Ø§Ù„ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­ÙŠØ© Ø¨Ù…Ø¯ÙŠÙ†Ø© Ù…Ø±Ø§ÙƒØ´ØŒ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªØ¶Ù† ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ù”Ø«Ù†Ø§Ø¡ØŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙŠÙ”ÙŠØ³ ÙˆØ§Ù”Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ¨ Ø§Ù„Ù…Ø³ÙŠØ± Ù„Ù„ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­ÙŠØ© Ø¨Ø¬Ù‡Ø© Ù…Ø±Ø§ÙƒØ´ Ø§Ù“Ø³ÙÙŠØŒ Ø§Ù•Ù„Ù‰ Ø­Ù„Ø¨Ø© Ù„Ù„Ø§Ø´ØªØ¨Ø§ÙƒØ§Øª ÙˆØ§Ù„Ù…Ù„Ø§Ø³Ù†Ø§ØªØŒ Ø¨Ø¹Ø¯ Ø§Ø´ØªØ¯Ø§Ø¯ Ø§Ù„Ø®Ù„Ø§Ù Ø¨ÙŠÙ† Ø§Ù„Ø¨Ø±Ù„Ù…Ø§Ù†ÙŠÙŠÙ† Ø­Ù…ÙŠØ¯ Ø§Ù„Ø¹ÙƒØ±ÙˆØ¯ ÙˆØ¹Ù…Ø± Ø®ÙÙŠÙØŒ Ø§Ù„Ù„Ø°ÙŠÙ† ÙŠÙ†ØªÙ…ÙŠØ§Ù† Ø§Ù•Ù„Ù‰ Ø­Ø²Ø¨ Ø§Ù„ØªØ¬Ù…Ø¹ Ø§Ù„ÙˆØ·Ù†ÙŠ Ù„Ù„Ø§Ù”Ø­Ø±Ø§Ø±ØŒ Ù…Ø§ ÙƒØ§Ø¯ ÙŠØ¹ØµÙ Ø¨Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ Ø¨Ø¹Ø¯ Ø§Ù†Ø·Ù„Ø§Ù‚ Ø´Ø±Ø§Ø±Ø© Ø§Ù„Ø§Ø´ØªØ¨Ø§Ùƒ Ø¨Ø§Ù„Ø§Ù”ÙŠØ§Ø¯ÙŠ Ø§Ù„ØªÙŠ Ø§Ù”Ø¬Ù‡Ø¶Øª ÙÙŠ Ù…Ù‡Ø¯Ù‡Ø§ Ø¨ØªØ¯Ø®Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ø¶Ø±ÙŠÙ†. ÙˆØ­Ø³Ø¨ Ø´Ù‡ÙˆØ¯ Ø¹ÙŠØ§Ù†ØŒ ÙØ§Ù•Ù† Ø¹Ù…Ø± Ø®ÙÙŠÙØŒ Ø§Ù„Ø°ÙŠ ÙŠØ´ØºÙ„ Ø±ÙŠÙ”ÙŠØ³ Ø¬Ù…Ø§Ø¹Ø© Ø§Ù”ÙƒÙØ§ÙŠØŒ ÙˆÙ…Ø¯Ø¹Ù… Ø§Ù„Ø­Ø¨ÙŠØ¨ Ø¨Ù† Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù…Ù†Ø³Ù‚ Ø§Ù„Ø§Ù‚Ù„ÙŠÙ…ÙŠ Ù„Ø­Ø²Ø¨ Ø§Ù„Ø§Ù”ØµØ§Ù„Ø© ÙˆØ§Ù„Ù…Ø¹Ø§ØµØ± Ø§Ù„Ø°ÙŠ ÙŠØªØ¬Ù‡ Ù„ØªÙˆÙ„ÙŠ Ø±ÙŠÙ”Ø§Ø³Ø© Ø§Ù„ØºØ±ÙØ© Ù„ÙˆÙ„Ø§ÙŠØ© ØªØ§Ù†ÙŠØ©ØŒ Ø±ÙØ¶ Ø¯Ø®ÙˆÙ„ Ø­Ù…ÙŠØ¯ Ø§Ù„Ø¹ÙƒØ±ÙˆØ¯ Ù„Ù„Ù…Ù†Ø§ÙØ³Ø© Ø¹Ù„Ù‰ Ø±ÙŠÙ”Ø§Ø³Ø© Ø§Ù„ØºØ±ÙØ©ØŒ ÙˆØ§ØµÙØ§ Ø§Ù•ÙŠØ§Ù‡ Ø¨Ù€ â€œØ§Ù„Ø§Ù”Ù…ÙŠ Ø§Ù„Ø°ÙŠ Ù„Ø§ÙŠÙÙ‚Ù‡ Ø´ÙŠÙŠÙ”Ø§â€ØŒ Ù„ÙŠØ¯Ø®Ù„ Ø§Ù„Ø·Ø±ÙØ§Ù† ÙÙŠ Ù…Ù„Ø§Ø³Ù†Ø§Øª ÙƒÙ„Ø§Ù…ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù”Ù† ÙŠØªØ­ÙˆÙ„ Ø§Ù„ØµØ±Ø§Ø¹ Ø§Ù•Ù„Ù‰ ØªØ´Ø§Ø¨Ùƒ Ø¨Ø§Ù„Ø§Ù”ÙŠØ¯ÙŠ. ',\n",
       " 'headline': 'Ø¨Ø±Ù„Ù…Ø§Ù†ÙŠÙŠÙ† Ù…Ù† Ø­Ø²Ø¨ Ø§Ù„Ø­Ù…Ø§Ù…Ø© Ù‚Ù„Ø¨ÙˆÙ‡Ø§ Ø¨ÙˆÙ†ÙŠØ§ Ù‚Ø¨Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø±Ø¦ÙŠØ³ ÙˆØ£Ø¹Ø¶Ø§Ø¡ ØºØ±ÙØ© Ø§Ù„ÙÙ„Ø§Ø­Ø© Ø¨Ø¬Ù‡Ø© Ù…Ø±Ø§ÙƒØ´ Ø¢Ø³ÙÙŠ (ØµÙˆØ±)',\n",
       " 'categories': \"['Ø¢Ø´ ÙˆØ§Ù‚Ø¹', 'Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©']\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section1: Affiner efficacement les modÃ¨les Seq2Seq avec Low Rank Adaptation (LoRA)\n",
    "\n",
    "Nous allons utiliser Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), et [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "Vous apprendrez Ã  :\n",
    "\n",
    "1. Configurer l'environnement de dÃ©veloppement\n",
    "2. Charger et prÃ©parer le jeu de donnÃ©es\n",
    "3. Affiner Multilingual BERT avec LoRA et bnb int-8\n",
    "4. Ã‰valuer et exÃ©cuter une infÃ©rence\n",
    "5. Comparaison des performances de coÃ»t\n",
    "\n",
    "### Introduction rapide Ã  PEFT ou Affinage de paramÃ¨tres efficace\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: flex-start;\">    <figure style=\"text-align: center;\">\n",
    "        <a href=\"https://arxiv.org/abs/2303.15647#\" target=\"_blank\">\n",
    "            <img src=\"./content/PEFT_method.png\" width=\"90%\" />\n",
    "        </a>\n",
    "        <figcaption><a href=\"https://arxiv.org/abs/2303.15647#\" target=\"_blank\">MÃ©thodes PEFT, de l'article \"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\"\n",
    "</a></figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), ou Affinage de paramÃ¨tres efficace, est une nouvelle bibliothÃ¨que open-source de Hugging Face permettant l'adaptation efficace des modÃ¨les de langue prÃ©-entrainÃ©s (PLMs) Ã  diverses applications en aval sans affiner tous les paramÃ¨tres du modÃ¨le. PEFT inclut actuellement des techniques pour :\n",
    "\n",
    "- LoRA:Â [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning:Â [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning:Â [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning:Â [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation Ã  Basse RangÃ©e (LoRA)\n",
    "\n",
    "Bien que les grands modÃ¨les de langage (LLMs) aient montrÃ© des performances remarquables dans une large gamme de tÃ¢ches de NLP, ils nÃ©cessitent des ressources de calcul importantes pour l'entraÃ®nement, le fine-tuning et le dÃ©ploiement. De plus, de nombreux cas d'utilisation du monde rÃ©el nÃ©cessitent l'adaptation des LLMs disponibles Ã  leur tÃ¢che cible afin d'atteindre les performances souhaitÃ©es.\n",
    "\n",
    "Alors que le fine-tuning d'un LLM complet est prohibitivement coÃ»teux, mÃªme sur de petits ensembles de donnÃ©es. Par exemple, le fine-tuning complet du modÃ¨le Llama7B nÃ©cessite 112 Go de VRAM, soit au moins deux GPU A100 de 80 Go. Heureusement, des mÃ©thodes de fine-tuning efficaces en termes de paramÃ¨tres comme LoRA permettent aux utilisateurs avec des ressources limitÃ©es d'adapter efficacement et efficacement un LLM Ã  leur tÃ¢che cible.\n",
    "\n",
    "Dans ce tutoriel, nous explorons QLoRA, qui est une technique de fine-tuning efficace en termes de paramÃ¨tres qui rÃ©duit le nombre de paramÃ¨tres ajustÃ©s pendant le processus d'adaptation, et introduit en plus une quantification pour rÃ©duire encore l'empreinte mÃ©moire du modÃ¨le adaptÃ©.\n",
    "\n",
    "### Comment Fonctionne LoRA ?\n",
    "\n",
    "L'article [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) s'inspire de la conjecture selon laquelle les modÃ¨les surparamÃ©trÃ©s couvrent une dimension intrinsÃ¨que Ã  basse rangÃ©e. Une basse dimension intrinsÃ¨que signifie que les donnÃ©es peuvent Ãªtre efficacement reprÃ©sentÃ©es ou approchÃ©es par un espace de dimension infÃ©rieure tout en conservant la plupart de leurs informations ou structures essentielles. En d'autres termes, cela signifie que nous pouvons dÃ©composer la nouvelle matrice de poids pour la tÃ¢che adaptÃ©e en matrices de dimension infÃ©rieure (plus petites) sans perdre d'informations significatives.\n",
    "\n",
    "ConcrÃ¨tement, supposons que $\\delta W$ soit la mise Ã  jour des poids pour une matrice de poids de $A\\times B$. Alors, une dÃ©composition Ã  basse rangÃ©e de $\\delta W$ peut Ãªtre exprimÃ©e comme : $\\delta W = W_A W_B$, oÃ¹ $W_A$ est une matrice de $A\\times k$ et $W_B$ est une matrice de $k\\times B$. Ici, $k$ est le rang de la dÃ©composition, et est gÃ©nÃ©ralement beaucoup plus petit que $A$ et $B$.\n",
    "\n",
    "![Image courtoisie du tutoriel de Sebastian Raschka sur LoRA de Lightning.AI](https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-4-300x226@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RÃ©sumÃ© en utilisant mT5\n",
    "\n",
    "Avant l'affinage de notre modÃ¨le, nous devons sÃ©lectionner le modÃ¨le que nous utiliserons comme modÃ¨le de base. Dans ce cas, nous utiliserons le modÃ¨le [mT5](https://huggingface.co/google/mt5-small), qui est une variante multilingue du modÃ¨le T5. Le modÃ¨le mT5 est entraÃ®nÃ© sur un large corpus multilingue et est capable de rÃ©aliser une vaste gamme de tÃ¢ches en PNL, y compris le rÃ©sumÃ©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous prÃ©parons nos jeux de donnÃ©es pour l'entraÃ®nement. Cela nÃ©cessite de tokenizer les sÃ©quences d'entrÃ©e et de sortie, de les complÃ©ter jusqu'Ã  la longueur souhaitÃ©e, puis de les convertir en objets Dataset de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"article\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"categories\", \"headline\"],\n",
    ")\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 571\n"
     ]
    }
   ],
   "source": [
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 50\n"
     ]
    }
   ],
   "source": [
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(\n",
    "    lambda x: tokenizer(x[\"headline\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\"article\", \"categories\"],\n",
    ")\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # # add prefix to the input for t5\n",
    "    # inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(sample['article'], max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `summary` keyword argument\n",
    "    labels = tokenizer(sample[\"headline\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"headline\", \"article\", \"categories\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9239cc1fc6694dd1b0edd5613e98d9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9497 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset[\"test\"].save_to_disk(\"arabic-goud-data/eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, nous devons dÃ©finir notre configuration pour LoRA. Les principaux paramÃ¨tres pour LoRA sont :\n",
    "\n",
    "* `r` : il s'agit du rang des matrices dÃ©composÃ©es $A$ et $B$ Ã  apprendre pendant le fine-tuning. Un nombre plus petit Ã©conomisera plus de mÃ©moire GPU mais pourrait diminuer les performances.\n",
    "* `lora_alpha` : il s'agit du poids de la perte de bas-rang dans la fonction de perte totale, ou du coefficient pour le facteur $\\Delta W$ appris. Un nombre plus grand entraÃ®nera gÃ©nÃ©ralement un changement de comportement plus significatif aprÃ¨s le fine-tuning.\n",
    "* `lora_dropout` : le ratio de dropout pour les couches dans les adaptateurs LoRA $A$ et $B$.\n",
    "* `target_modules` : les modules pour lesquels apprendre la dÃ©composition de bas-rang. Cela pourrait Ãªtre toutes les couches linÃ©aires, par exemple, ou des modules spÃ©cifiques du rÃ©seau de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 300,864,896 || trainable%: 0.2287\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "  r=16,\n",
    "  lora_alpha=32,\n",
    "  target_modules=[\"q\", \"v\"],\n",
    "  lora_dropout=0.05,\n",
    "  bias=\"none\",\n",
    "  task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "output_dir = \"lora-goud-mt5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"lora-mt5-goud\",\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"validation\"].select(range(20)),\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/alizaidi/dev/nlp/llms/indaba/indaba-low-resource-nlp-prac/wandb/run-20240816_012514-brfaca9p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlora-mt5-goud\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/alizaidi/huggingface\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/alizaidi/huggingface/runs/brfaca9p\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='174110' max='174110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [174110/174110 4:54:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.364100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.733600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.573900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.309900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.112100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>4.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>4.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.977800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>4.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.962000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.950300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.942800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.926900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.943500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.933100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.914700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.934100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.883700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.869100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.912900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.917400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.874800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.874900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.855400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.864000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.822500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.864700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>3.828100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.858900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>3.821300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>3.850200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>3.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>3.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>3.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.831800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>3.844700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>3.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>3.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>3.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>3.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>3.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>3.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>3.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>3.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>3.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>3.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>3.802900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>3.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>3.799100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>3.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>3.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>3.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>3.793900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>3.777100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>3.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>3.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>3.804400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>3.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>3.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>3.804200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>3.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>3.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>3.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>3.785500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>3.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>3.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>3.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>3.803400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>3.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>3.774300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>3.775800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>3.811800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>3.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>3.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>3.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>3.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>3.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>3.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>3.752600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>3.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>3.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>3.731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>3.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>3.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>3.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>3.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>3.749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>3.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>3.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>3.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>3.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>3.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>3.770900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>3.771400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>3.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>3.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>3.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>3.753500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>3.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>3.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>3.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>3.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>3.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>3.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>3.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>3.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>3.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>3.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>3.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>3.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>3.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>3.728800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>3.727000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>3.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>3.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>3.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>3.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>3.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>3.698400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>3.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>3.753100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>3.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>3.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>3.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>3.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>3.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>3.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>3.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>3.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>3.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>3.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>3.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>3.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>3.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>3.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>3.713200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>3.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>3.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>3.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>3.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102500</td>\n",
       "      <td>3.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103000</td>\n",
       "      <td>3.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103500</td>\n",
       "      <td>3.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>3.710600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104500</td>\n",
       "      <td>3.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>3.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105500</td>\n",
       "      <td>3.695500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>3.697400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106500</td>\n",
       "      <td>3.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107000</td>\n",
       "      <td>3.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107500</td>\n",
       "      <td>3.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>3.657400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108500</td>\n",
       "      <td>3.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109000</td>\n",
       "      <td>3.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109500</td>\n",
       "      <td>3.687900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>3.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110500</td>\n",
       "      <td>3.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>3.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111500</td>\n",
       "      <td>3.666200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>3.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112500</td>\n",
       "      <td>3.669500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113000</td>\n",
       "      <td>3.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113500</td>\n",
       "      <td>3.674900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>3.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114500</td>\n",
       "      <td>3.649200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>3.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115500</td>\n",
       "      <td>3.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>3.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116500</td>\n",
       "      <td>3.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>3.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117500</td>\n",
       "      <td>3.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>3.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118500</td>\n",
       "      <td>3.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119000</td>\n",
       "      <td>3.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119500</td>\n",
       "      <td>3.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>3.672200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120500</td>\n",
       "      <td>3.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121000</td>\n",
       "      <td>3.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121500</td>\n",
       "      <td>3.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>3.649900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122500</td>\n",
       "      <td>3.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>3.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123500</td>\n",
       "      <td>3.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>3.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124500</td>\n",
       "      <td>3.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>3.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125500</td>\n",
       "      <td>3.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>3.634400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126500</td>\n",
       "      <td>3.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127000</td>\n",
       "      <td>3.645700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127500</td>\n",
       "      <td>3.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>3.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128500</td>\n",
       "      <td>3.650900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>3.620500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129500</td>\n",
       "      <td>3.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>3.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130500</td>\n",
       "      <td>3.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131000</td>\n",
       "      <td>3.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131500</td>\n",
       "      <td>3.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>3.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132500</td>\n",
       "      <td>3.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133000</td>\n",
       "      <td>3.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133500</td>\n",
       "      <td>3.636100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>3.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134500</td>\n",
       "      <td>3.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>3.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135500</td>\n",
       "      <td>3.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>3.672900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136500</td>\n",
       "      <td>3.644700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137000</td>\n",
       "      <td>3.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137500</td>\n",
       "      <td>3.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>3.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138500</td>\n",
       "      <td>3.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139000</td>\n",
       "      <td>3.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139500</td>\n",
       "      <td>3.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>3.620800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140500</td>\n",
       "      <td>3.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>3.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141500</td>\n",
       "      <td>3.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>3.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142500</td>\n",
       "      <td>3.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143000</td>\n",
       "      <td>3.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143500</td>\n",
       "      <td>3.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>3.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144500</td>\n",
       "      <td>3.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>3.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145500</td>\n",
       "      <td>3.586200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>3.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146500</td>\n",
       "      <td>3.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>3.630300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147500</td>\n",
       "      <td>3.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>3.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148500</td>\n",
       "      <td>3.605800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149000</td>\n",
       "      <td>3.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149500</td>\n",
       "      <td>3.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>3.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150500</td>\n",
       "      <td>3.590200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151000</td>\n",
       "      <td>3.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151500</td>\n",
       "      <td>3.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>3.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152500</td>\n",
       "      <td>3.626200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>3.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153500</td>\n",
       "      <td>3.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>3.641200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154500</td>\n",
       "      <td>3.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>3.620600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155500</td>\n",
       "      <td>3.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>3.607400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156500</td>\n",
       "      <td>3.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157000</td>\n",
       "      <td>3.587200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157500</td>\n",
       "      <td>3.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158500</td>\n",
       "      <td>3.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>3.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159500</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>3.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160500</td>\n",
       "      <td>3.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161000</td>\n",
       "      <td>3.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161500</td>\n",
       "      <td>3.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>3.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162500</td>\n",
       "      <td>3.581900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163000</td>\n",
       "      <td>3.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163500</td>\n",
       "      <td>3.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>3.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164500</td>\n",
       "      <td>3.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>3.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165500</td>\n",
       "      <td>3.584300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>3.587500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166500</td>\n",
       "      <td>3.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167000</td>\n",
       "      <td>3.598800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>3.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>3.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168500</td>\n",
       "      <td>3.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>3.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169500</td>\n",
       "      <td>3.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>3.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170500</td>\n",
       "      <td>3.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>3.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171500</td>\n",
       "      <td>3.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>3.559900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>3.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>3.594700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173500</td>\n",
       "      <td>3.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>3.613900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=174110, training_loss=3.7825945418129274, metrics={'train_runtime': 17642.8655, 'train_samples_per_second': 78.949, 'train_steps_per_second': 9.869, 'total_flos': 8.31857984054231e+17, 'train_loss': 3.7825945418129274, 'epoch': 10.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id=\"peft-lora-mt5-goud-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('peft-lora-mt5-goud-results/tokenizer_config.json',\n",
       " 'peft-lora-mt5-goud-results/special_tokens_map.json',\n",
       " 'peft-lora-mt5-goud-results/spiece.model',\n",
       " 'peft-lora-mt5-goud-results/added_tokens.json',\n",
       " 'peft-lora-mt5-goud-results/tokenizer.json')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8860c96231496a83995df93aa5b870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d8f0fc17824720a34d4e75b778b1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e2bd214de94322ac57fa72945058aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77fa9fe51ed4b2f9b73db57b14eeecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4aec675d46d456d9826e38d3879f121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/alizaidi/lora-mt5-goud/commit/fee52adbeda959a97d9b839e0b8f0a5f315b0713', commit_message='alizaidi/lora-mt5-goud-ar', commit_description='', oid='fee52adbeda959a97d9b839e0b8f0a5f315b0713', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"alizaidi/lora-mt5-goud-ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alizaidi/micromamba/envs/peft/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load base LLM model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø¨Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ø·Ø§Ù„Ø¨Ø© ÙƒÙ„ÙŠØ§Øª ÙØ§Ø³ Ø®Ø±Ø¬Ø§Øª Ù„Ù„Ø§Ø­ØªØ¬Ø§Ø¬ Ø¹Ù„Ù‰ ÙØªØ­ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ ÙˆØ§Ù„Ø¬Ø§Ù…Ø¹Ø§Øª\n"
     ]
    }
   ],
   "source": [
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "\n",
    "text = dataset[\"test\"][0][\"article\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=128)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ø±ÙˆØ¨ÙˆØ±Ø·Ø§Ø¬.. Ø§Ù„Ø¨ÙŠØ§ØªØ© Ø¨Ø§Ù„Ù„ÙŠÙ„ Ø¹Ù„Ù‰ Ø¨Ø±Ø§ Ø´ÙƒÙ„ Ø§Ø­ØªØ¬Ø§Ø¬ÙŠ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨ÙØªØ­ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠØ© Ø¨ÙØ§Ø³'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"test\"][0][\"headline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"\n",
    "    split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(\n",
    "    dataset,\n",
    "    metric,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=16,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    column_text=\"article\",\n",
    "    column_summary=\"highlights\",\n",
    "):\n",
    "    article_batches = list(\n",
    "        generate_batch_sized_chunks(dataset[column_text], batch_size)\n",
    "    )\n",
    "    target_batches = list(\n",
    "        generate_batch_sized_chunks(dataset[column_summary], batch_size)\n",
    "    )\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)\n",
    "    ):\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            article_batch,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        summaries = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "            length_penalty=0.8,\n",
    "            num_beams=8,\n",
    "            max_length=128,\n",
    "        )\n",
    "        \"\"\" parameter for length penalty ensures that the model does not generate sequences that are too long. \"\"\"\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [\n",
    "            tokenizer.decode(\n",
    "                s, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            for s in summaries\n",
    "        ]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "\n",
    "def evaluation(tokenizer, model, dataset):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # loading data\n",
    "    rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    score = calculate_metric_on_test_ds(\n",
    "        dataset[\"test\"][0:10],\n",
    "        rouge_metric,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        batch_size=2,\n",
    "        column_text=\"article\",\n",
    "        column_summary=\"headline\",\n",
    "    )\n",
    "\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\n",
    "    return rouge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_498243/3742215907.py:70: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                    | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                | 1/5 [00:02<00:08,  2.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                    | 2/5 [00:04<00:06,  2.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 3/5 [00:06<00:04,  2.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 4/5 [00:10<00:02,  2.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.1, 'rouge2': 0.0, 'rougeL': 0.1, 'rougeLsum': 0.1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation(tokenizer, model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ã‰valuation1 : point de contrÃ´le prÃ©coce du modÃ¨le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load an early checkpoint\n",
    "#run evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ã‰valuation2 : modÃ¨le final entraÃ®nÃ© AraBERT, DarijaBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ActivitÃ© : RÃ©sumÃ© d'Article en Langue Maternelle\n",
    "\n",
    "**TÃ¢che :** Rechercher un article dans votre langue maternelle et Ã©valuer les capacitÃ©s de rÃ©sumÃ© et de gÃ©nÃ©ration de titres de ChatGPT.\n",
    "\n",
    "**Ã‰tapes :**\n",
    "\n",
    "1. **SÃ©lectionner un Article :** Choisissez un article pertinent et rÃ©cent rÃ©digÃ© dans votre langue maternelle. Assurez-vous qu'il soit de longueur courte ou moyenne.\n",
    "\n",
    "2. **RÃ©sumer avec ChatGPT :** Utilisez ChatGPT pour gÃ©nÃ©rer un rÃ©sumÃ© de l'article sÃ©lectionnÃ©.\n",
    "\n",
    "3. **Ã‰valuer la QualitÃ© du RÃ©sumÃ© :**\n",
    "    - **Impression :** Partagez votre impression sur la qualitÃ© du rÃ©sumÃ©. ConsidÃ©rez les points suivants :\n",
    "        - **Exactitude :** Le rÃ©sumÃ© capture-t-il les points principaux et l'essence de l'article ?\n",
    "        - **ClartÃ© :** Le rÃ©sumÃ© est-il clair et facile Ã  comprendre ?\n",
    "        - **Couverture :** Le rÃ©sumÃ© inclut-il toutes les informations cruciales de l'article ?\n",
    "\n",
    "4. **Fournir un Retour :** Offrez un retour constructif sur le rÃ©sumÃ©. Soulignez les divergences Ã©ventuelles ou les domaines Ã  amÃ©liorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "\tsrc=\"https://forms.gle/sggLJWMFQ4JQCmHL8\",\n",
       "  width=\"80%\"\n",
       "\theight=\"320px\" >\n",
       "\tLoading...\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Generate Quiz Form. (Run Cell)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<iframe\n",
    "\tsrc=\"https://forms.gle/sggLJWMFQ4JQCmHL8\",\n",
    "  width=\"80%\"\n",
    "\theight=\"320px\" >\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section2: RÃ©sumÃ© en utilisant GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cette section, nous allons utiliser Ã  la fois les grands et petits modÃ¨les de langue d'OpenAI pour accomplir la mÃªme tÃ¢che de rÃ©sumÃ© textuel abstractive. Nous Ã©valuerons ensuite leurs performances et comparerons les rÃ©sultats avec ceux obtenus Ã  partir des modÃ¨les discutÃ©s dans la Section 1.\n",
    "\n",
    "Contrairement aux approches traditionnelles de fine-tuning qui impliquent la mise Ã  jour des poids du modÃ¨le, la premiÃ¨re Ã©tape de l'adaptation d'un modÃ¨le basÃ© sur GPT pour une tÃ¢che spÃ©cifique est l'ingÃ©nierie des prompts, qui ne nÃ©cessite pas de mise Ã  jour des poids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "    <figure style=\"margin-right: 10px; text-align: center;\">\n",
    "        <a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">\n",
    "            <img src=\"./content/traditional-finetuning.png\" width=\"80%\" />\n",
    "        </a>\n",
    "        <figcaption><a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">Traditional Fine-Tuning</a></figcaption>\n",
    "    </figure>\n",
    "    <figure style= \"text-align: center;\">\n",
    "        <a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">\n",
    "            <img src=\"./content/prompting.png\" width=\"80%\" />\n",
    "        </a>\n",
    "        <figcaption><a href=\"https://arxiv.org/pdf/2005.14165\" target=\"_blank\">Prompting</a></figcaption>\n",
    "    </figure>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_metric import PyRouge\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge(hypotheses, references):\n",
    "    these_refs = [[ref.strip().lower()] for ref in references]\n",
    "    rouge = PyRouge(rouge_n=(1, 2), rouge_l=True)\n",
    "    scores = rouge.evaluate(hypotheses, these_refs)\n",
    "    print(scores)\n",
    "\n",
    "def substring_after_colon(input_string):\n",
    "    colon_index = input_string.find(':')\n",
    "    if colon_index != -1:\n",
    "        return input_string[colon_index + 1:]\n",
    "    else:\n",
    "        return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset and paths\n",
    "DATASET = \"Goud\"\n",
    "MAX_TRAIN = 0\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "output_filename = f\"./{DATASET}_{model_name}_test_generated_{MAX_TRAIN}.csv\"\n",
    "\n",
    "# Load dataset\n",
    "goud_data = dataset\n",
    "train_source = goud_data[\"train\"][\"article\"]\n",
    "train_target = goud_data[\"train\"][\"headline\"]\n",
    "test_source = goud_data[\"test\"][\"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction pour rÃ©sumer des articles de presse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_news_article(MAX_TRAIN=20):\n",
    "       \n",
    "    client = OpenAI(api_key=key)\n",
    "    rewritten_prompt_count = 0\n",
    "    line_count = 0\n",
    "    wait_time = 1\n",
    "    df_lines = []\n",
    "    tokens_consumption = 0\n",
    "    existing_len = 0\n",
    "    if os.path.exists(output_filename):\n",
    "        existing_df = pd.read_csv(output_filename)\n",
    "        existing_len = existing_df.shape[0]\n",
    "        rewritten_prompt_count = existing_len\n",
    "        line_count = existing_len\n",
    "        df_lines = existing_df.to_dict('records')\n",
    "    \n",
    "    for data in tqdm(test_source[existing_len:], desc=f\"Lines processed from {existing_len}-th line\"):\n",
    "        news_article = data.strip()\n",
    "        line_count += 1\n",
    "        made_error = True\n",
    "        num_error = 0\n",
    "        while made_error:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are asked to summarize a news article written in Modern Standard Arabic and Moroccan Darija, and write that summary as a clickbait headline, in Moroccan Darija only.\\n\"}]\n",
    "            if MAX_TRAIN - num_error > 0:\n",
    "                for _ in range(MAX_TRAIN - num_error):\n",
    "                    idx = random.choice(range(len(train_source)))\n",
    "                    train_src = train_source[idx]\n",
    "                    train_tgt = train_target[idx]\n",
    "                    messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{train_src}\\\"\"})\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": f\"Absolutely! Here is the headline summarizing your news article:\\n\\\"{train_tgt}\\\"\"})\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"Summarize the following news article into a headline in Moroccan Darija only:\\n\\\"{news_article}\\\"\"})\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=model_name,\n",
    "                )\n",
    "                headline = response.choices[0].message.content\n",
    "                df_lines.append({\"article\": news_article,\"generated_headline\": headline,\"prompt_messages\":messages})\n",
    "\n",
    "                rewritten_prompt_count += 1\n",
    "                made_error = False\n",
    "            except Exception as e:\n",
    "                if isinstance(e, openai.RateLimitError):\n",
    "                    print(\"Rate limit error\")\n",
    "                    print(f\"Wait for {wait_time} seconds because all calls failed: \", flush=True)\n",
    "                    time.sleep(wait_time)\n",
    "                    wait_time *= 2\n",
    "                else:\n",
    "                    print(e)\n",
    "                    num_error += 1\n",
    "                    print(\"May be too long, reducing context to:\", MAX_TRAIN - num_error)\n",
    "            #time.sleep(1)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(df_lines)\n",
    "    df.to_csv(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExÃ©cuter la synthÃ¨se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lines processed from 0-th line: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9497/9497 [2:34:22<00:00,  1.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9263.82370686531 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#record cell running time\n",
    "import time\n",
    "start_time = time.time()\n",
    "summarize_news_article(0)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger le rÃ©sultat gÃ©nÃ©rÃ© et Ã©valuer le ROUGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le dossier \"generated_responses\", vous trouverez les rÃ©ponses gpt correspondant aux invites de 0, 1, 5, 20 coups.\n",
    "\n",
    "Ã‰valuez les rÃ©sumÃ©s de titres gÃ©nÃ©rÃ©s en exÃ©cutant l'Ã©valuation ROUGE et ajoutez les rÃ©sultats au tableau des rÃ©sultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RÃ©sultats du MÃ©trique ROUGE : 0 Shot\n",
    "\n",
    "| MÃ©trique | Rappel (r)         | PrÃ©cision (p)      | Score F1 (f)      |\n",
    "|----------|-------------------|-------------------|-------------------|\n",
    "| ROUGE-1  | 0.1228            | 0.1069            | 0.1143            |\n",
    "| ROUGE-2  | 0.0282            | 0.0235            | 0.0256            |\n",
    "| ROUGE-L  | 0.1128            | 0.0980            | 0.1049            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.11884854492859614, 'p': 0.12977184865821972, 'f': 0.12407023545586798}, 'rouge-2': {'r': 0.0325784137233658, 'p': 0.03389945881811894, 'f': 0.03322581039836068}, 'rouge-l': {'r': 0.11128523451125509, 'p': 0.12142149826298465, 'f': 0.11613260817866634}}\n"
     ]
    }
   ],
   "source": [
    "shot_count = 0\n",
    "hypotheses = pd.read_csv(f\".\\generated_responses\\{model_name}\\Goud_{model_name}_test_generated_{str(shot_count)}.csv\", encoding = \"UTF-8\")[\"generated_headline\"].tolist()\n",
    "hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
    "references = goud_data[\"test\"][\"headline\"]\n",
    "evaluate_rouge(hypotheses, references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RÃ©sultats du Metric ROUGE: 20 Exemple\n",
    "Choisissez un ou plusieurs des fichiers contenant les rÃ©ponses GPT N-shot prÃ©cÃ©demment gÃ©nÃ©rÃ©es, prÃ©sents dans le dossier \"generated_responses\", exÃ©cutez l'Ã©valuation, puis remplissez le tableau ci-dessous\n",
    "\n",
    "| Metric   | Rappel (r)        | PrÃ©cision (p)     | Score-F (f)       |\n",
    "|----------|-------------------|-------------------|-------------------|\n",
    "| ROUGE-1  |                   |                   |                   |\n",
    "| ROUGE-2  |                   |                   |                   |\n",
    "| ROUGE-L  |                   |                   |                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.13750397869020445, 'p': 0.1306205269799418, 'f': 0.13397389480280544}, 'rouge-2': {'r': 0.03387859852289136, 'p': 0.03194819653833152, 'f': 0.032885092553751126}, 'rouge-l': {'r': 0.1264128630910928, 'p': 0.11986076103165903, 'f': 0.12304965282629712}}\n"
     ]
    }
   ],
   "source": [
    "shot_count = 20\n",
    "model_name  = \"gpt4\"  #\"C:\\Users\\salamaaya\\OneDrive - Microsoft\\Desktop\\DLI\\Indaba2024-practical\\indaba-low-resource-nlp-prac\\generated_responses\\gpt4\\Goud_test_generated_5.csv\"\n",
    "hypotheses = pd.read_csv(f\".\\generated_responses\\{model_name}\\Goud_test_generated_{str(shot_count)}.csv\", encoding = \"UTF-8\")[\"generated_headline\"].tolist()\n",
    "hypotheses = [substring_after_colon(hypo).replace(\"\\\"\", \"\").strip() for hypo in hypotheses]\n",
    "references = goud_data[\"test\"][\"headline\"]\n",
    "evaluate_rouge(hypotheses, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate Quiz Form. (Run Cell)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<iframe\n",
    "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
    "  width=\"80%\"\n",
    "\theight=\"1200px\" >\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "**RÃ©sumÃ© :**\n",
    "\n",
    "[RÃ©sumÃ© des points principaux/Ã  retenir de la pratique.]\n",
    "\n",
    "**Prochaines Ã‰tapes :**\n",
    "\n",
    "[Prochaines Ã©tapes pour les personnes ayant terminÃ© la pratique, comme des lectures optionnelles (par exemple, blogs, articles, cours, vidÃ©os YouTube). Cela pourrait Ã©galement renvoyer Ã  d'autres pratiques.]\n",
    "\n",
    "**Annexe :**\n",
    "\n",
    "[Tout ce qui (probablement des trucs mathÃ©matiques lourds) ne trouve pas de place dans les sections pratiques principales.]\n",
    "\n",
    "**RÃ©fÃ©rences :**\n",
    "\n",
    "[RÃ©fÃ©rences pour tout contenu utilisÃ© dans le notebook.]\n",
    "\n",
    "Pour d'autres pratiques du Deep Learning Indaba, veuillez visiter [ici](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donnez-nous votre avis sur notre session!\n",
    "\n",
    "Veuillez fournir des retours que nous pouvons utiliser pour amÃ©liorer nos travaux pratiques Ã  l'avenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
       "  width=\"80%\"\n",
       "\theight=\"1200px\" >\n",
       "\tLoading...\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title Generate Feedback Form. (Run Cell)\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<iframe\n",
    "\tsrc=\"https://forms.gle/WUpRupqfhFtbLXtN6\",\n",
    "  width=\"80%\"\n",
    "\theight=\"1200px\" >\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "WILOYJH4gCnD"
   ],
   "name": "Indaba_2022_Prac_Template.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
