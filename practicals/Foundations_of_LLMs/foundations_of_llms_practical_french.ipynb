{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# LLMs pour tous\n",
        "\n",
        "<img src=\"https://www.marktechpost.com/wp-content/uploads/2023/05/Blog-Banner-3.jpg\" width=\"60%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Foundations_of_LLMs/foundations_of_llms_practical_french.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "¬© Deep Learning Indaba 2024. Apache License 2.0.\n",
        "\n",
        "**Authors: Jabez Magomere, Harry Mayne, Khalil Mrini, Nabra Rizvi, Doudou Ba, Ruan van der Merwe, Marianne Monteiro, Everlyn Asiko Chimoto**\n",
        "\n",
        "**Reviewers: Seid Muhie Yimam, Foutse Yuehgoh**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Bienvenue √† \"LLMs pour Tous\"‚Äîvotre porte d'entr√©e vers le monde fascinant des Mod√®les de Langage de Grande Taille (LLMs) ! Pour commencer, voici un fait amusant : toute cette introduction a √©t√© g√©n√©r√©e par ChatGPT, l'un des nombreux LLMs puissants que vous allez d√©couvrir. ü§ñ‚ú®\n",
        "\n",
        "Dans ce tutoriel, vous allez plonger au c≈ìur des principes fondamentaux des transformateurs, la technologie de pointe derri√®re des mod√®les comme GPT. Vous aurez √©galement l'occasion de vous exercer √† entra√Æner votre propre Mod√®le de Langage ! Pr√©parez-vous √† explorer comment ces syst√®mes d'IA impressionnants cr√©ent des textes aussi r√©alistes et captivants. Partons ensemble pour ce voyage passionnant et d√©verrouillons les secrets des LLMs ! üöÄüìö\n",
        "\n",
        "**Sujets :**\n",
        "\n",
        "Contenu : [<font color='orange'>Introduction √† Hugging Face</font>, <font color='green'>M√©canisme d'Attention</font>, <font color='green'>Architecture du Transformeur</font>, <font color='green'>Entra√Æner votre propre LLM depuis z√©ro</font>, <font color='orange'>Ajustement fin d'un LLM pour la Classification de Texte</font>]\n",
        "\n",
        "Niveau : <font color='orange'>D√©butant</font>, <font color='green'>Interm√©diaire</font>, <font color='blue'>Avanc√©</font>\n",
        "\n",
        "**Objectifs d'apprentissage :**\n",
        "\n",
        "* Comprendre l'id√©e derri√®re [l'Attention](https://arxiv.org/abs/1706.03762) et pourquoi elle est utilis√©e.\n",
        "* Pr√©senter et d√©crire les blocs de construction fondamentaux de l'[Architecture du Transformeur](https://arxiv.org/abs/1706.03762) ainsi qu'une intuition sur la conception de cette architecture.\n",
        "* Construire et entra√Æner un simple LLM inspir√© de Shakespeare.\n",
        "\n",
        "**Pr√©requis :**\n",
        "\n",
        "* Connaissances introductives en Apprentissage Profond.\n",
        "* Connaissances introductives en NLP.\n",
        "* Connaissances introductives des mod√®les s√©quence √† s√©quence.\n",
        "* Compr√©hension de base en alg√®bre lin√©aire.\n",
        "\n",
        "**Plan :**\n",
        ">[LLMs pour tous](#scrollTo=m2s4kN_QPQVe)\n",
        "\n",
        ">>[Installations, Importations et Fonctions Utilitaires](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">>[Commen√ßons avec une D√©mo Hugging Face ! D√©butant](#scrollTo=4zu5cg-YG4XU)\n",
        "\n",
        ">>>[Hugging Face](#scrollTo=AwjIIipOG4fz)\n",
        "\n",
        ">>>[C'est l'heure de la d√©mo ! ‚è∞‚ö° Charger un mod√®le Hugging Face et ex√©cuter un √©chantillon](#scrollTo=eq46TV_0G4f0)\n",
        "\n",
        ">>[1. Attention](#scrollTo=-ZUp8i37dFbU)\n",
        "\n",
        ">>>[Intuition - D√©butant](#scrollTo=ygdi884ugGcu)\n",
        "\n",
        ">>>[Comprendre l'Attention en termes simples](#scrollTo=ygdi884ugGcu)\n",
        "\n",
        ">>>[M√©canismes d'attention s√©quence √† s√©quence - Interm√©diaire](#scrollTo=aQfqM1EJyDXI)\n",
        "\n",
        ">>>[De l'auto-attention √† l'attention multi-t√™tes - Interm√©diaire](#scrollTo=J-MU6rrny8Nj)\n",
        "\n",
        ">>>>[Auto-attention](#scrollTo=0AFUEFZGzCTv)\n",
        "\n",
        ">>>>>[Requ√™tes, cl√©s et valeurs](#scrollTo=pwOIMtdZzdTf)\n",
        "\n",
        ">>>>>[Attention par produit scalaire avec √©chelle](#scrollTo=OhGZHFsHz_Qp)\n",
        "\n",
        ">>>>>[Attention masqu√©e](#scrollTo=D7B-AgO80gIt)\n",
        "\n",
        ">>>>>[Attention multi-t√™tes](#scrollTo=OWDubQwCs4zG)\n",
        "\n",
        ">>[2. Construire votre propre LLM](#scrollTo=e9NW58_3hAg2)\n",
        "\n",
        ">>>[2.1 Vue d'ensemble g√©n√©rale D√©butant](#scrollTo=bA_2coZvhAg3)\n",
        "\n",
        ">>>[2.2 Tokenisation + Encodage positionnel D√©butant](#scrollTo=fbTsk0MdhAhC)\n",
        "\n",
        ">>>>[2.2.1 Tokenisation](#scrollTo=DehUpfym_RF8)\n",
        "\n",
        ">>>>[2.2.2 Encodages positionnels](#scrollTo=639s7Zuk_RF9)\n",
        "\n",
        ">>>>>[Fonctions sinus et cosinus](#scrollTo=rklY-aL-_RF9)\n",
        "\n",
        ">>>[2.3 Bloc Transformer Interm√©diaire](#scrollTo=SdNPg0pnhAhG)\n",
        "\n",
        ">>>>[2.3.1 R√©seau de neurones Feed Forward (FFN) / Perceptron multicouche (MLP) D√©butant](#scrollTo=kTURbfr__RF-)\n",
        "\n",
        ">>>>[2.3.2 Bloc Ajouter et Normaliser D√©butant](#scrollTo=TWUpf8wt_RF-)\n",
        "\n",
        ">>>[2.4 Construction du D√©codeur Transformer / LLM Interm√©diaire](#scrollTo=91dXd29b_RF_)\n",
        "\n",
        ">>>[2.5 Entra√Ænement de votre LLM](#scrollTo=wmt3tp38G90A)\n",
        "\n",
        ">>>>[2.5.1 Objectif d'entra√Ænement Interm√©diaire](#scrollTo=agLIpsoh_RGA)\n",
        "\n",
        ">>>>[2.5.1 Objectif d'entra√Ænement Interm√©diaire](#scrollTo=QOSv1-3B_RGA)\n",
        "\n",
        ">>>>[2.5.2 Entra√Ænement des mod√®les Avanc√©](#scrollTo=4CSfvGj__RGA)\n",
        "\n",
        ">>>>[2.5.3 Inspecter le LLM entra√Æn√© D√©butant](#scrollTo=pGv9c2AFmF4V)\n",
        "\n",
        ">>[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        ">[Retours - Avis - Suggestions](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "\n",
        "\n",
        "**Avant de commencer :**\n",
        "\n",
        "Pour ce TP, vous aurez besoin d'utiliser un GPU pour acc√©l√©rer l'entra√Ænement. Pour ce faire, allez dans le menu \"Ex√©cution\" de Colab, s√©lectionnez \"Modifier le type d'ex√©cution\", puis dans le menu popup, choisissez \"GPU\" dans la bo√Æte \"Acc√©l√©rateur mat√©riel\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952qogb79nnY"
      },
      "source": [
        "**Niveau d'exp√©rience sugg√©r√© dans ce sujet :**\n",
        "\n",
        "| Niveau        | Exp√©rience                            |\n",
        "| --- | --- |\n",
        "`D√©butant`      | C'est la premi√®re fois que je suis introduit √† ce travail. |\n",
        "`Interm√©diaire` | J'ai suivi quelques cours de base/introductions sur ce sujet. |\n",
        "`Avanc√©`        | Je travaille quotidiennement dans ce domaine/sujet. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBdDHcI_ArCR"
      },
      "outputs": [],
      "source": [
        "# @title **Chemins √† suivre :** Quel est votre niveau d'exp√©rience dans les sujets pr√©sent√©s dans ce notebook ? (Ex√©cutez la cellule)\n",
        "experience = \"d√©butant\" #@param [\"d√©butant\", \"interm√©diaire\", \"avanc√©\"]\n",
        "sections_to_follow=\"\"\n",
        "\n",
        "\n",
        "if experience == \"d√©butant\": sections_to_follow = \"\"\"nous vous recommandons de ne pas essayer de faire toutes les t√¢ches de codage mais plut√¥t de passer √† chaque section et de vous assurer d'interagir avec le LLM affin√© avec LoRA pr√©sent√© dans la derni√®re section ainsi qu'avec le LLM pr√©entra√Æn√© pour obtenir une compr√©hension pratique du comportement de ces mod√®les\"\"\"\n",
        "\n",
        "elif experience == \"interm√©diaire\": sections_to_follow = \"\"\"nous vous recommandons de parcourir chaque section de ce notebook et d'essayer les t√¢ches de codage √©tiquet√©es comme d√©butant ou interm√©diaire. Si vous √™tes bloqu√© sur le code, demandez de l'aide √† un tuteur ou passez √† autre chose pour mieux utiliser le temps de la pratique\"\"\"\n",
        "\n",
        "elif experience == \"avanc√©\": sections_to_follow = \"\"\"nous vous recommandons de parcourir chaque section et d'essayer chaque t√¢che de codage jusqu'√† ce que vous r√©ussissiez √† la faire fonctionner\"\"\"\n",
        "\n",
        "\n",
        "print(f\"En fonction de votre exp√©rience, {sections_to_follow}.\\nNote : ceci est juste une ligne directrice, n'h√©sitez pas √† explorer le colab comme bon vous semble si vous vous sentez √† l'aise !\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installations, Importations et Fonctions Utilitaires\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "# Installer les biblioth√®ques n√©cessaires pour le deep learning, le NLP et la visualisation\n",
        "!pip install transformers datasets  # Biblioth√®ques Transformers et datasets pour les t√¢ches de NLP\n",
        "!pip install seaborn umap-learn     # Seaborn pour la visualisation, UMAP pour la r√©duction dimensionnelle\n",
        "!pip install livelossplot           # LiveLossPlot pour suivre les progr√®s de l'entra√Ænement du mod√®le\n",
        "!pip install -q transformers[torch] # Transformers avec le backend PyTorch\n",
        "!pip install -q peft                # Biblioth√®que de fine-tuning efficient en param√®tres\n",
        "!pip install accelerate -U          # Biblioth√®que Accelerate pour les performances\n",
        "\n",
        "# Installer des utilitaires pour le d√©bogage et le formatage de la sortie console\n",
        "!pip install -q ipdb                # D√©bogueur interactif Python\n",
        "!pip install -q colorama            # Sortie de texte color√©e dans le terminal\n",
        "\n",
        "# Importer des utilitaires syst√®me et math√©matiques\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# V√©rifier les acc√©l√©rateurs connect√©s (GPU ou TPU) et configurer en cons√©quence\n",
        "if os.environ.get(\"COLAB_GPU\") and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"Un GPU est connect√©.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"Un TPU est connect√©.\")\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Seul le processeur (CPU) est connect√©.\")\n",
        "\n",
        "# √âviter que l'allocation de m√©moire GPU soit effectu√©e par JAX\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
        "\n",
        "# Importer les biblioth√®ques pour le deep learning bas√© sur JAX\n",
        "import chex\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import optax\n",
        "\n",
        "# Importer les biblioth√®ques li√©es au NLP et aux mod√®les\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "# Importer les biblioth√®ques pour le traitement d'images et la visualisation\n",
        "from PIL import Image\n",
        "from livelossplot import PlotLosses\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Importer des utilitaires suppl√©mentaires pour travailler avec le texte et les mod√®les\n",
        "import torch\n",
        "import torchvision\n",
        "import itertools\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# T√©l√©charger une image d'exemple √† utiliser dans le notebook\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
        "    \"cat.png\",\n",
        ")\n",
        "\n",
        "# Importer les biblioth√®ques pour le pr√©traitement NLP et le travail avec des mod√®les pr√©-entra√Æn√©s\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "nltk.download(\"word2vec_sample\")\n",
        "\n",
        "# Importer les outils Hugging Face et les widgets IPython\n",
        "import huggingface_hub\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import colorama\n",
        "\n",
        "# Configurer Matplotlib pour g√©n√©rer des graphiques au format SVG pour une meilleure qualit√©\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "locqqlIQvTrK"
      },
      "outputs": [],
      "source": [
        "# @title Fonctions d'aide pour les trac√©s. (Ex√©cuter la cellule)\n",
        "\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "    \"\"\"\n",
        "    Trace la matrice des encodages de position.\n",
        "\n",
        "    Args:\n",
        "        P: Matrice des encodages de position (tableau 2D).\n",
        "        max_tokens: Nombre maximum de tokens (lignes) √† tracer.\n",
        "        d_model: Dimensionnalit√© du mod√®le (colonnes) √† tracer.\n",
        "    \"\"\"\n",
        "\n",
        "    # D√©finir la taille du trac√© en fonction du nombre de tokens et de la dimension du mod√®le\n",
        "    plt.figure(figsize=(20, np.min([8, max_tokens])))\n",
        "\n",
        "    # Tracer la matrice des encodages de position avec une carte de couleurs pour une meilleure visualisation\n",
        "    im = plt.imshow(P, aspect=\"auto\", cmap=\"Blues_r\")\n",
        "\n",
        "    # Ajouter une barre de couleur pour indiquer les valeurs d'encodage\n",
        "    plt.colorbar(im, cmap=\"blue\")\n",
        "\n",
        "    # Afficher les indices d'embedding si la dimensionnalit√© est petite\n",
        "    if d_model <= 64:\n",
        "        plt.xticks(range(d_model))\n",
        "\n",
        "    # Afficher les indices de position si le nombre de tokens est petit\n",
        "    if max_tokens <= 32:\n",
        "        plt.yticks(range(max_tokens))\n",
        "\n",
        "    # √âtiqueter les axes\n",
        "    plt.xlabel(\"Indice d'embedding\")\n",
        "    plt.ylabel(\"Indice de position\")\n",
        "\n",
        "    # Afficher le trac√©\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "    \"\"\"\n",
        "    Fonction qui prend une liste de patches d'image et les trace.\n",
        "\n",
        "    Args:\n",
        "        patches: Une liste ou un tableau de patches d'image √† tracer.\n",
        "    \"\"\"\n",
        "\n",
        "    # D√©finir la figure pour tracer les patches\n",
        "    fig = plt.figure(figsize=(25, 25))\n",
        "\n",
        "    # Cr√©er un sous-plot pour chaque patch et l'afficher\n",
        "    axes = []\n",
        "    for a in range(patches.shape[1]):\n",
        "        axes.append(fig.add_subplot(1, patches.shape[1], a + 1))\n",
        "        plt.imshow(patches[0][a])\n",
        "\n",
        "    # Ajuster la disposition pour √©viter le chevauchement et afficher le trac√©\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "    \"\"\"\n",
        "    Projette des embeddings de haute dimension en 2D et les trace.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Vecteurs d'embeddings de haute dimension √† projeter.\n",
        "        labels: √âtiquettes correspondant √† chaque embedding pour les couleurs du trac√©.\n",
        "    \"\"\"\n",
        "\n",
        "    # Importer UMAP et Seaborn pour la r√©duction dimensionnelle et le trac√©\n",
        "    import umap\n",
        "    import seaborn as sns\n",
        "\n",
        "    # R√©duire la dimensionnalit√© des embeddings en 2D avec UMAP\n",
        "    projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "    # Tracer les projections 2D avec des √©tiquettes en utilisant Seaborn pour une meilleure esth√©tique\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.title(\"Projections des embeddings textuels\")\n",
        "    sns.scatterplot(\n",
        "        x=projected_embeddings[:, 0], y=projected_embeddings[:, 1], hue=labels\n",
        "    )\n",
        "\n",
        "    # Afficher le trac√©\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "    \"\"\"\n",
        "    Trace une matrice de poids d'attention avec des ticks d'axe personnalis√©s.\n",
        "\n",
        "    Args:\n",
        "        weight_matrix: La matrice de poids d'attention √† tracer.\n",
        "        x_ticks: √âtiquettes pour l'axe des x (g√©n√©ralement les tokens de la requ√™te).\n",
        "        y_ticks: √âtiquettes pour l'axe des y (g√©n√©ralement les tokens cl√©s).\n",
        "    \"\"\"\n",
        "\n",
        "    # D√©finir la taille du trac√©\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Tracer la matrice de poids d'attention sous forme de carte thermique\n",
        "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
        "\n",
        "    # D√©finir des ticks personnalis√©s pour les axes x et y\n",
        "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
        "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
        "\n",
        "    # √âtiqueter le trac√©\n",
        "    plt.title(\"Matrice d'attention\")\n",
        "    plt.xlabel(\"Score d'attention\")\n",
        "\n",
        "    # Afficher le trac√©\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMkaKekB_pR4"
      },
      "outputs": [],
      "source": [
        "# @title Fonctions Utilitaires pour le Traitement de Texte. (Ex√©cutez la Cellule)\n",
        "\n",
        "def get_word2vec_embedding(words):\n",
        "    \"\"\"\n",
        "    Fonction qui prend une liste de mots et retourne une liste de leurs int√©grations,\n",
        "    bas√©e sur un encodeur word2vec pr√©entra√Æn√©.\n",
        "    \"\"\"\n",
        "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        word2vec_sample, binary=False\n",
        "    )\n",
        "\n",
        "    output = []\n",
        "    words_pass = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            output.append(jnp.array(model.word_vec(word)))\n",
        "            words_pass.append(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    embeddings = jnp.array(output)\n",
        "    del model  # lib√©rer de l'espace √† nouveau\n",
        "    return embeddings, words_pass\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Fonction qui prend une cha√Æne de caract√®res et supprime toute la ponctuation.\"\"\"\n",
        "    import re\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def print_sample(prompt: str, sample: str):\n",
        "  \"\"\"Fonction qui prend une instruction de prompt et une r√©ponse de mod√®le et\n",
        "  les affiche en diff√©rentes couleurs pour montrer une distinction\"\"\"\n",
        "  print(colorama.Fore.MAGENTA + prompt, end=\"\")\n",
        "  print(colorama.Fore.BLUE + sample)\n",
        "  print(colorama.Fore.RESET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zu5cg-YG4XU"
      },
      "source": [
        "## Commen√ßons avec une D√©mo Hugging Face ! <font color='orange'>D√©butant</font>\n",
        "\n",
        "Nous sommes ravis de vous avoir √† bord ! üéâ Avant de plonger dans la partie pratique de notre voyage, faisons un petit d√©tour dans le monde fascinant de [Hugging Face](https://huggingface.co/)‚Äîune plateforme open-source incroyable pour construire et d√©ployer des mod√®les de langage √† la pointe de la technologie. üåê\n",
        "\n",
        "Comme avant-go√ªt de ce que nous allons cr√©er aujourd'hui, nous allons commencer par charger un *petit* mod√®le de langage (*en comparaison avec les mod√®les d'aujourd'hui) et lui donner une instruction simple. Cela vous donnera un aper√ßu de la mani√®re d'interagir avec ces biblioth√®ques puissantes. üí° Pr√©parez-vous √† d√©bloquer le potentiel des mod√®les de langage avec juste quelques lignes de code !\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwjIIipOG4fz"
      },
      "source": [
        "### Hugging Face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2DSHiuhG4f0"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png\" width=\"10%\">\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) est une startup fond√©e en 2016 et, selon leurs propres mots : \"ils ont pour mission de d√©mocratiser le machine learning de qualit√©, un commit √† la fois.\" Actuellement, ils sont une v√©ritable mine d'or pour les outils permettant de travailler avec les Mod√®les de Langage de Grande Taille (LLMs).\n",
        "\n",
        "Ils ont d√©velopp√© divers packages open-source et permettent aux utilisateurs d'interagir facilement avec un large corpus de mod√®les transformeurs pr√©entra√Æn√©s (dans toutes les modalit√©s) et de datasets pour entra√Æner ou ajuster finement ces transformeurs pr√©entra√Æn√©s. Leur logiciel est largement utilis√© dans l'industrie et la recherche. Pour plus de d√©tails sur eux et leur utilisation, r√©f√©rez-vous √† [l'exercice pratique sur l'attention et les transformeurs de 2022](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/practicals/attention_and_transformers.ipynb#scrollTo=qFBw8kRx-4Mk).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xdt9PQ6G4f0"
      },
      "source": [
        "Dans ce colab, nous affichons les prompts en <font color='HotPink'><b>rose</b></font> et les √©chantillons g√©n√©r√©s par un mod√®le en <font color='blue'><b>bleu</b></font> comme dans l'exemple ci-dessous :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-8C9SJCG4f0"
      },
      "outputs": [],
      "source": [
        "print_sample(prompt='Mon faux prompt', sample=' est g√©nial !')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq46TV_0G4f0"
      },
      "source": [
        "### C'est l'heure de la d√©mo ! ‚è∞‚ö° Charger un mod√®le Hugging Face et ex√©cuter un √©chantillon\n",
        "\n",
        "Plongeons dans la simplicit√© de charger et d'interagir avec un mod√®le de Hugging Face !\n",
        "\n",
        "Pour ce tutoriel, nous avons pr√©configur√© deux options de mod√®les :\n",
        "\n",
        "- **`gpt-neo-125M`** : Un mod√®le plus petit avec 125 millions de param√®tres. Il est plus rapide et utilise moins de m√©moire‚Äîparfait pour commencer ! Nous vous recommandons d'essayer celui-ci en premier.\n",
        "- **`gpt2-medium`** : Un mod√®le plus grand avec 355 millions de param√®tres pour une utilisation plus avanc√©e.\n",
        "\n",
        "Si vous souhaitez changer de mod√®le, il vous suffit de red√©marrer le noyau Colab et de mettre √† jour le nom du mod√®le dans la cellule ci-dessous.\n",
        "\n",
        "**Remarque** : Les √©tapes que nous allons montrer fonctionnent non seulement pour ces mod√®les, mais aussi pour [tous les mod√®les](https://huggingface.co/models?pipeline_tag=text-generation) sur Hugging Face qui prennent en charge les pipelines de g√©n√©ration de texte.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVV28V-TG4f1"
      },
      "outputs": [],
      "source": [
        "# D√©finir le nom du mod√®le sur \"EleutherAI/gpt-neo-125M\" (cela peut √™tre chang√© via les options du menu d√©roulant)\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"  # @param [\"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
        "\n",
        "# D√©finir le prompt pour le mod√®le de g√©n√©ration de texte\n",
        "test_prompt = \"Qu'est-ce que l'amour ?\"  # @param {type: \"string\"}\n",
        "\n",
        "# Cr√©er un pipeline de g√©n√©ration de texte en utilisant le mod√®le sp√©cifi√©\n",
        "generator = transformers.pipeline('text-generation', model=model_name)\n",
        "\n",
        "# G√©n√©rer du texte bas√© sur le prompt fourni\n",
        "# 'do_sample=True' permet l'√©chantillonnage pour introduire de l'al√©atoire dans la g√©n√©ration, et 'min_length=30' garantit qu'au moins 30 tokens sont g√©n√©r√©s\n",
        "model_output = generator(test_prompt, do_sample=True, min_length=30)\n",
        "\n",
        "# Afficher l'√©chantillon de texte g√©n√©r√©, en supprimant le prompt original de la sortie\n",
        "print_sample(test_prompt, model_output[0]['generated_text'].split(test_prompt)[1].rstrip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5IEKl4iG4f1"
      },
      "source": [
        "**üí° Astuce :** Essayez d'ex√©cuter le code ci-dessus avec diff√©rents prompts ou avec le m√™me prompt plusieurs fois !\n",
        "\n",
        "**ü§î Discussion :** Pourquoi pensez-vous que le texte g√©n√©r√© change √† chaque fois, m√™me avec le m√™me prompt ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfV0Qk6yG4f1"
      },
      "source": [
        "Cr√©ons notre propre fonction `generator` pour faciliter le chargement de diff√©rents poids de mod√®les et configurer la g√©n√©ration de texte. Il vous suffit d'ex√©cuter les cellules ci-dessous pour commencer ! üòÄ\n",
        "\n",
        "Pour l'instant, ne vous inqui√©tez pas trop de comprendre les d√©tails du tokenizer. Consid√©rez-le simplement comme une √©tape pour convertir l'entr√©e dans un format que le mod√®le de langage peut comprendre. Nous approfondirons la tokenisation plus tard dans le notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxs5bO_sG4f1"
      },
      "outputs": [],
      "source": [
        "# V√©rifiez si le nom du mod√®le contient 'gpt2' et chargez le tokenizer et le mod√®le appropri√©s\n",
        "if 'gpt2' in model_name:\n",
        "    # Charger le tokenizer et le mod√®le GPT-2\n",
        "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "# Si le nom du mod√®le est 'EleutherAI/gpt-neo-125M', chargez le tokenizer et le mod√®le correspondants\n",
        "elif model_name == \"EleutherAI/gpt-neo-125M\":\n",
        "    # Charger l'AutoTokenizer et l'AutoModel pour le mod√®le GPT-Neo sp√©cifi√©\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
        "# Lever une erreur si le nom du mod√®le n'est pas pris en charge\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Si un GPU est disponible, d√©placer le mod√®le sur le GPU pour un traitement plus rapide\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "# D√©finir l'ID du token de remplissage pour qu'il soit le m√™me que l'ID du token de fin de s√©quence\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsZEwoZJG4f1"
      },
      "outputs": [],
      "source": [
        "def run_sample(\n",
        "    model,  # Le mod√®le de langage que nous utiliserons pour g√©n√©rer du texte\n",
        "    tokenizer,  # Le tokenizer qui convertit le texte dans un format que le mod√®le comprend\n",
        "    prompt: str,  # Le texte d'invite que nous donnerons au mod√®le pour commencer la g√©n√©ration de texte\n",
        "    seed: int | None = None,  # Optionnel : Un nombre pour rendre les r√©sultats pr√©visibles √† chaque ex√©cution\n",
        "    temperature: float = 0.6,  # Contr√¥le le caract√®re al√©atoire de la sortie du mod√®le ; des valeurs plus basses rendent la sortie plus cibl√©e\n",
        "    top_p: float = 0.9,  # Contr√¥le combien des mots les plus probables sont pris en compte ; des valeurs plus √©lev√©es consid√®rent plus d'options\n",
        "    max_new_tokens: int = 64,  # Le nombre maximum de mots ou de tokens que le mod√®le ajoutera √† l'invite\n",
        ") -> str:\n",
        "    # Cette fonction g√©n√®re du texte en fonction d'une invite donn√©e en utilisant un mod√®le de langage,\n",
        "    # avec des options pour contr√¥ler l'al√©atoire, le nombre de tokens g√©n√©r√©s, et la reproductibilit√©.\n",
        "\n",
        "    # Convertir le texte d'invite en tokens que le mod√®le peut traiter\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Extraire les tokens (input IDs) et le masque d'attention (pour se concentrer sur les parties importantes) des entr√©es\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # D√©placer les tokens et le masque d'attention vers le m√™me appareil que le mod√®le (comme un GPU si disponible)\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    attention_mask = attention_mask.to(model.device)\n",
        "\n",
        "    # Configurer la mani√®re dont nous voulons que le mod√®le g√©n√®re du texte\n",
        "    generation_config = transformers.GenerationConfig(\n",
        "        do_sample=True,  # Permet au mod√®le d'ajouter un peu d'al√©atoire √† sa g√©n√©ration de texte\n",
        "        temperature=temperature,  # Ajuste le caract√®re al√©atoire de la sortie ; des valeurs plus basses rendent la sortie plus cibl√©e\n",
        "        top_p=top_p,  # Consid√®re les mots les plus probables qui constituent les 90% des possibilit√©s\n",
        "        pad_token_id=tokenizer.pad_token_id,  # Utilise l'ID de token qui repr√©sente le remplissage (espace suppl√©mentaire)\n",
        "        top_k=0,  # Nous ne limitons pas aux top-k mots, donc nous le r√©glons √† 0\n",
        "    )\n",
        "\n",
        "    # Si une graine (seed) est fournie, la d√©finir pour que les r√©sultats soient reproductibles (m√™me sortie √† chaque ex√©cution)\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # G√©n√©rer du texte en utilisant le mod√®le avec les param√®tres que nous avons d√©finis\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,  # Fournir les tokens d'entr√©e au mod√®le\n",
        "        attention_mask=attention_mask,  # Fournir le masque d'attention pour aider le mod√®le √† se concentrer\n",
        "        return_dict_in_generate=True,  # Demander au mod√®le de renvoyer des informations d√©taill√©es\n",
        "        output_scores=True,  # Inclure les scores (niveaux de confiance) pour les tokens g√©n√©r√©s\n",
        "        max_new_tokens=max_new_tokens,  # D√©finir le nombre maximum de tokens √† g√©n√©rer\n",
        "        generation_config=generation_config,  # Appliquer nos param√®tres personnalis√©s de g√©n√©ration de texte\n",
        "    )\n",
        "\n",
        "    # S'assurer qu'une seule s√©quence (sortie) est g√©n√©r√©e, pour simplifier les choses\n",
        "    assert len(generation_output.sequences) == 1\n",
        "\n",
        "    # Obtenir la s√©quence de tokens g√©n√©r√©e\n",
        "    output_sequence = generation_output.sequences[0]\n",
        "\n",
        "    # Convertir les tokens g√©n√©r√©s en texte lisible\n",
        "    output_string = tokenizer.decode(output_sequence)\n",
        "\n",
        "    # Imprimer l'invite et la r√©ponse g√©n√©r√©e\n",
        "    print_sample(prompt, output_string)\n",
        "\n",
        "    # Retourner le texte g√©n√©r√©\n",
        "    return output_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yme6VzW4G4f1"
      },
      "outputs": [],
      "source": [
        "_ = run_sample(model, tokenizer, prompt=\"Qu'est-ce que l'amour ?\", temperature=0.5, seed=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vnUawyG4f1"
      },
      "source": [
        "Assez incroyable, n'est-ce pas ? ü§© Essayez de jouer avec les valeurs de **prompt**, **temperature** et **seed** ci-dessus et voyez les diff√©rentes sorties que vous obtenez. Que remarquez-vous lorsque vous augmentez la temp√©rature ? Bien que cela ait pu √™tre √©poustouflant en 2021, la plupart d'entre vous ont probablement d√©j√† interagi avec des mod√®les de langage de grande taille. Aujourd'hui, nous allons aller plus loin en entra√Ænant notre propre **mod√®le de langage inspir√© de Shakespeare**. Cela nous permettra de comprendre concr√®tement comment ces mod√®les fonctionnent en coulisses.\n",
        "\n",
        "Mais avant de passer √† l'entra√Ænement, construisons d'abord une solide compr√©hension de ce que sont les **mod√®les de langage de grande taille** et des concepts cl√©s de **l'apprentissage automatique** qui rendent cette technologie r√©volutionnaire possible. Au c≈ìur des LLMs √† la pointe de la technologie (SoTA) d'aujourd'hui se trouvent le **m√©canisme d'attention** et l'**architecture Transformer**. Nous allons explorer ces concepts essentiels dans les prochaines sections de ce tutoriel. üöÄüí°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **1. Attention**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acgW1ofF_RFz"
      },
      "source": [
        "Le m√©canisme d'attention est inspir√© par la mani√®re dont les humains regardent une image ou lisent une phrase.\n",
        "\n",
        "Prenons l'image du chien en v√™tements humains ci-dessous (image et exemple [source](https://lilianweng.github.io/posts/2018-06-24-attention/)). Lorsque nous pr√™tons *attention* aux blocs de pixels rouges, nous dirons que le bloc jaune des oreilles pointues est quelque chose que nous attendions (corr√©l√©), mais que les blocs gris des v√™tements humains sont inattendus pour nous (non corr√©l√©). Ceci est *bas√© sur ce que nous avons vu dans le pass√©* en regardant des photos de chiens, sp√©cifiquement d'un Shiba Inu.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iEU7Cph2D2PCXp3YEHj30-EndhHAeB5T\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Supposons que nous voulons identifier la race de chien dans cette image. Lorsque nous regardons les blocs de pixels rouges, nous avons tendance √† pr√™ter plus d'*attention* aux pixels pertinents qui sont plus similaires ou li√©s √† eux, ce qui pourrait √™tre ceux dans la bo√Æte jaune. Nous ignorons presque compl√®tement la neige en arri√®re-plan et les v√™tements humains pour cette t√¢che.\n",
        "\n",
        "D'un autre c√¥t√©, lorsque nous commen√ßons √† regarder l'arri√®re-plan dans une tentative d'identifier ce qu'il contient, nous ignorons inconsciemment les pixels du chien car ils sont sans importance pour la t√¢che actuelle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usLBF2g0x5gH"
      },
      "source": [
        "La m√™me chose se produit lorsque nous lisons. Pour comprendre toute la phrase, nous apprendrons √† corr√©ler et √† *pr√™ter attention √†* certains mots en fonction du contexte de la phrase enti√®re.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j23kcfu_c3wINU6DUvxzMYNmp4alhHc9\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        "Par exemple, dans la premi√®re phrase de l'image ci-dessus, en regardant le mot \"coding\", nous pr√™tons plus d'attention aux mots \"Apple\" et \"computer\" car nous savons que lorsque nous parlons de codage, \"Apple\" fait en fait r√©f√©rence √† l'entreprise. Cependant, dans la deuxi√®me phrase, nous r√©alisons que nous ne devrions pas consid√©rer \"apple\" en regardant \"code\" car, compte tenu du contexte du reste de la phrase, nous savons que cette pomme fait r√©f√©rence √† une pomme r√©elle et non √† un ordinateur.\n",
        "\n",
        "Nous pouvons construire de meilleurs mod√®les en d√©veloppant des m√©canismes qui imitent l'attention. Cela permettra √† nos mod√®les d'apprendre de meilleures repr√©sentations de nos donn√©es d'entr√©e en contextualisant ce qu'ils savent sur certaines parties de l'entr√©e en fonction d'autres parties. Dans les sections suivantes, nous explorerons les m√©canismes qui nous permettent d'entra√Æner des mod√®les d'apprentissage profond √† pr√™ter attention aux donn√©es d'entr√©e dans le contexte d'autres donn√©es d'entr√©e.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygdi884ugGcu"
      },
      "source": [
        "### Intuition - <font color='orange'>D√©butant</font>\n",
        "\n",
        "Imaginez l'attention comme un m√©canisme qui permet √† un r√©seau neuronal de se concentrer davantage sur certaines parties des donn√©es. En faisant cela, le r√©seau peut am√©liorer sa compr√©hension du probl√®me sur lequel il travaille, en mettant √† jour sa compr√©hension ou ses repr√©sentations en cons√©quence.\n",
        "\n",
        "### Comprendre l'Attention en termes simples\n",
        "\n",
        "Une fa√ßon de mettre en ≈ìuvre l'attention dans les r√©seaux neuronaux est de repr√©senter chaque mot (ou m√™me des parties d'un mot) comme un vecteur.\n",
        "\n",
        "Alors, qu'est-ce qu'un vecteur ? Un vecteur est simplement un tableau de nombres (appel√©s nombres r√©els) qui peut avoir diff√©rentes longueurs. Pensez-y comme √† une liste de valeurs qui d√©crivent certaines propri√©t√©s d'un mot. Ces vecteurs nous permettent de mesurer √† quel point deux mots sont similaires. Une fa√ßon courante de mesurer cette similarit√© est de calculer ce qu'on appelle le **produit scalaire**.\n",
        "\n",
        "Le r√©sultat de ce calcul de similarit√© est ce que nous appelons **l'attention.** Cette valeur d'attention aide le mod√®le √† d√©cider dans quelle mesure un mot doit influencer la repr√©sentation d'un autre mot.\n",
        "\n",
        "En termes plus simples, si deux mots ont des repr√©sentations vectorielles similaires, cela signifie qu'ils sont probablement li√©s ou importants l'un pour l'autre. En raison de cette relation, ils affectent les repr√©sentations de chacun dans le r√©seau neuronal, permettant au mod√®le de mieux comprendre le contexte. üéØ\n",
        "\n",
        "Pour illustrer comment le produit scalaire peut cr√©er des poids d'attention significatifs, nous utiliserons des embeddings [word2vec](https://jalammar.github.io/illustrated-word2vec/) pr√©-entra√Æn√©s. Ces embeddings word2vec sont g√©n√©r√©s par un r√©seau neuronal qui a appris √† cr√©er des embeddings similaires pour des mots ayant des significations similaires.\n",
        "\n",
        "En calculant la matrice des produits scalaires entre tous les vecteurs, nous obtenons une matrice d'attention. Cela indiquera quels mots sont corr√©l√©s et devraient donc \"se pr√™ter attention\" mutuellement.\n",
        "\n",
        "[1] Vous pouvez trouver plus de d√©tails sur la fa√ßon dont cela est fait pour les LLMs dans la session \"Construire votre propre LLM\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvBYShCFk6WC"
      },
      "source": [
        "**T√¢che de code** <font color='blue'>Interm√©diaire</font> : Compl√©tez la fonction d'attention par produit scalaire ci-dessous.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrbITGPnk7Ce"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "    \"\"\"\n",
        "    Calcule le produit scalaire entre les √©tats cach√©s et les √©tats pr√©c√©dents.\n",
        "\n",
        "    Args:\n",
        "        hidden_states: Un tenseur de forme [T_hidden, dm]\n",
        "        previous_state: Un tenseur de forme [T_previous, dm]\n",
        "    \"\"\"\n",
        "\n",
        "    # Indice : Pour calculer les scores d'attention, r√©fl√©chissez √† la mani√®re dont vous pouvez utiliser le vecteur `previous_state`\n",
        "    # et la matrice `hidden_states`. Vous voulez d√©terminer √† quel point chaque √©l√©ment de `previous_state`\n",
        "    # doit \"pr√™ter attention\" √† chaque √©l√©ment de `hidden_states`. Rappelez-vous que dans la multiplication matricielle,\n",
        "    # vous pouvez trouver la relation entre deux ensembles de vecteurs en multipliant l'un par la transpos√©e de l'autre.\n",
        "    # Indice : Utilisez `jnp.matmul` pour effectuer la multiplication matricielle entre `previous_state` et la\n",
        "    # transpos√©e de `hidden_states` (`hidden_states.T`).\n",
        "    scores = ...  # FINISH ME\n",
        "\n",
        "    # Indice : Maintenant que vous avez les scores, vous devez les convertir en probabilit√©s.\n",
        "    # Une fonction softmax est g√©n√©ralement utilis√©e dans les m√©canismes d'attention pour transformer les scores bruts en probabilit√©s\n",
        "    # qui s'additionnent pour donner 1. Cela aidera √† d√©terminer sur quel √©tat cach√© se concentrer.\n",
        "    # Indice : Utilisez `jax.nn.softmax` pour appliquer la fonction softmax aux `scores`.\n",
        "    w_n = ...  # FINISH ME\n",
        "\n",
        "    # Multipliez les poids par les √©tats cach√©s pour obtenir le vecteur de contexte\n",
        "    # Indice : Utilisez √† nouveau `jnp.matmul` pour multiplier les poids d'attention `w_n` par `hidden_states`\n",
        "    # pour obtenir le vecteur de contexte.\n",
        "    c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "    return w_n, c_t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QARgTrNZlIqH"
      },
      "outputs": [],
      "source": [
        "# @title Ex√©cutez-moi pour tester votre code\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [2, 2])\n",
        "\n",
        "try:\n",
        "  w_n, c_t = dot_product_attention(x, x)\n",
        "\n",
        "  w_n_correct = jnp.array([[0.9567678, 0.04323225], [0.00121029, 0.99878967]])\n",
        "  c_t_correct = jnp.array([[0.11144122, 0.95290256], [-1.5571996, -1.5321486]])\n",
        "  assert jnp.allclose(w_n_correct, w_n), \"w_n n'est pas calcul√© correctement\"\n",
        "  assert jnp.allclose(c_t_correct, c_t), \"c_t n'est pas calcul√© correctement\"\n",
        "\n",
        "  print(\"Cela semble correct. Regardez la r√©ponse ci-dessous pour comparer les m√©thodes.\")\n",
        "except:\n",
        "  print(\"Il semble que la fonction n'est pas encore compl√®tement impl√©ment√©e. Essayez de la modifier.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa6PyKYnkzUJ"
      },
      "outputs": [],
      "source": [
        "# Lors du changement de ces mots, notez que si le mot n'est pas dans le corpus\n",
        "# d'entra√Ænement original, il ne sera pas affich√© dans le graphique de la matrice des poids.\n",
        "# @title R√©ponse √† la t√¢che de code (Essayez de ne pas regarder avant d'avoir bien essay√© !')\n",
        "def dot_product_attention(hidden_states, previous_state):\n",
        "    # Calculer les scores d'attention :\n",
        "    # Multiplier le vecteur de l'√©tat pr√©c√©dent par la transpos√©e de la matrice des √©tats cach√©s.\n",
        "    # Cela nous donne une matrice de scores qui montre √† quel point chaque √©l√©ment de l'√©tat pr√©c√©dent\n",
        "    # doit pr√™ter attention √† chaque √©l√©ment des √©tats cach√©s.\n",
        "    # Le r√©sultat est une matrice de forme [T, N], o√π :\n",
        "    # T est le nombre d'√©l√©ments dans les √©tats cach√©s,\n",
        "    # N est le nombre d'√©l√©ments dans l'√©tat pr√©c√©dent.\n",
        "    scores = jnp.matmul(previous_state, hidden_states.T)\n",
        "\n",
        "    # Appliquer la fonction softmax aux scores pour les convertir en probabilit√©s.\n",
        "    # Cela normalise les scores pour qu'ils soient additionn√©s √† 1 pour chaque √©l√©ment,\n",
        "    # ce qui nous permet de les interpr√©ter comme le degr√© d'attention √† accorder √† chaque √©tat cach√©.\n",
        "    w_n = jax.nn.softmax(scores)\n",
        "\n",
        "    # Calculer le vecteur de contexte (c_t) :\n",
        "    # Multiplier les poids d'attention (w_n) par les √©tats cach√©s.\n",
        "    # Cela combine les √©tats cach√©s en fonction du degr√© d'attention que chacun m√©rite,\n",
        "    # ce qui donne un nouveau vecteur qui repr√©sente la somme pond√©r√©e des √©tats cach√©s.\n",
        "    # La forme r√©sultante est [T, d], o√π :\n",
        "    # T est le nombre d'√©l√©ments dans l'√©tat pr√©c√©dent,\n",
        "    # d est la dimension des √©tats cach√©s.\n",
        "    c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "    # Retourner les poids d'attention et le vecteur de contexte.\n",
        "    return w_n, c_t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlHL3e_QhLfq"
      },
      "outputs": [],
      "source": [
        "mots = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
        "embeddings_mots, mots = get_word2vec_embedding(mots)\n",
        "poids, _ = dot_product_attention(embeddings_mots, embeddings_mots)\n",
        "plot_attention_weight_matrix(poids, mots, mots)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tItZU09YlhEZ"
      },
      "source": [
        "En regardant la matrice, nous pouvons voir quels mots ont des significations similaires. Le groupe de mots \"royal\" a des scores d'attention plus √©lev√©s les uns avec les autres que les mots \"nourriture\", qui s'attendent tous mutuellement. Nous voyons √©galement que \"computers\" ont des scores d'attention tr√®s bas pour tous, ce qui montre qu'ils ne sont ni tr√®s li√©s aux mots \"royal\" ni aux mots \"nourriture\".\n",
        "\n",
        "**T√¢che de groupe :**\n",
        "  - Jouez avec les s√©lections de mots ci-dessus. Voyez si vous pouvez trouver des combinaisons de mots dont les valeurs d'attention semblent contre-intuitives. Pensez √† des explications possibles. Quel sens d'un mot les scores d'attention ont-ils captur√© ?\n",
        "  - Demandez √† votre ami s'il a trouv√© des exemples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3iB8hf0hJdX"
      },
      "source": [
        "**Remarque** : Le produit scalaire n'est qu'une des fa√ßons de mettre en ≈ìuvre la fonction de score pour les m√©canismes d'attention, il existe une liste plus √©tendue dans cet [article de blog](https://lilianweng.github.io/posts/2018-06-24-attention/#summary) de Dr Lilian Weng.\n",
        "\n",
        "Plus de ressources :\n",
        "\n",
        "[Un mod√®le de base encodeur-d√©codeur pour la traduction automatique](https://www.youtube.com/watch?v=gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=1)\n",
        "\n",
        "[Entra√Ænement et perte pour les mod√®les encodeur-d√©codeur](https://www.youtube.com/watch?v=aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=2)\n",
        "\n",
        "[Attention de base](https://www.youtube.com/watch?v=BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQfqM1EJyDXI"
      },
      "source": [
        "### M√©canismes d'attention s√©quence √† s√©quence - <font color='green'>Interm√©diaire</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68QBeG-4yDZ9"
      },
      "source": [
        "Les premiers m√©canismes d'attention ont √©t√© utilis√©s dans les mod√®les s√©quence √† s√©quence. Ces mod√®les √©taient g√©n√©ralement des structures encodeur et d√©codeur RNN. La s√©quence d'entr√©e √©tait trait√©e s√©quentiellement par un RNN, encodant la s√©quence dans un seul vecteur de contexte, qui √©tait ensuite aliment√© dans un autre RNN g√©n√©rant une nouvelle s√©quence. Voici un exemple de cela ([source](https://lilianweng.github.io/posts/2018-06-24-attention/)).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FKfaArN1rsLjzVWaJGpMLEcxEshSLXd6\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "√âtant donn√© qu'il n'y a qu'un seul vecteur de contexte, il est difficile pour l'encodeur de repr√©senter de longues s√©quences et l'information est g√©n√©ralement perdue. Le m√©canisme d'attention introduit dans [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) a √©t√© propos√© pour r√©soudre ce probl√®me.\n",
        "\n",
        "Ici, au lieu de se fier √† un seul vecteur de contexte statique, qui est √©galement utilis√© une seule fois dans le processus de d√©codage, nous fournissons des informations sur toute la s√©quence d'entr√©e √† chaque √©tape de d√©codage en utilisant un vecteur de contexte dynamique. Ce faisant, le d√©codeur peut acc√©der √† une plus grande \"banque\" de m√©moire et pr√™ter attention aux informations n√©cessaires de l'entr√©e en fonction de l'√©tat actuel du RNN d√©codeur, $s_t$. Cela est illustr√© ci-dessous.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fB5KObXcKo5x35xlIDIcjHTq1q75ejIB\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "En apprentissage profond, l'attention peut √™tre interpr√©t√©e comme un vecteur \"d'importance\". Pour pr√©dire ou inf√©rer un √©l√©ment, tel qu'un pixel dans une image ou un mot dans une phrase, nous estimons √† quel point il est corr√©l√© avec, ou \"attend\", d'autres √©l√©ments en utilisant le vecteur/poids d'attention. Ces poids d'attention sont ensuite utilis√©s pour g√©n√©rer une nouvelle somme pond√©r√©e des √©l√©ments restants, ce qui repr√©sente la cible [(source)](https://lilianweng.github.io/posts/2018-06-24-attention/).\n",
        "\n",
        "Cela consiste g√©n√©ralement en trois √©tapes pour chaque √©tape de d√©codage $t$ :\n",
        "\n",
        "1. Calculer le score (importance) pour chaque $h_n$, √©tant donn√© $s_{t-1}$, et utiliser la fonction softmax pour transformer cela en un vecteur d'attention, $w_{n}$.\n",
        "  - $\\text{score} = a(s_{t‚àí1}, h_{n})$, o√π $a$ peut √™tre n'importe quelle fonction diff√©rentiable, telle que le produit scalaire.\n",
        "  - $w_{n} = \\frac{\\exp \\left\\{a\\left(s_{t-1}, h_{n}\\right)\\right\\}}{\\sum_{j=1}^{N} \\exp \\left\\{a\\left(s_{t-1}, h_{j}\\right)\\right\\}}$, o√π nous utilisons la fonction softmax pour transformer les scores bruts en poids d'attention relatifs.\n",
        "2. G√©n√©rer le vecteur de contexte final, $c_t$, en sommant les produits des poids d'attention et des vecteurs de contexte de l'encodeur.\n",
        "  - $c_t=\\sum_{n=1}^{N} w_n h_{n}$\n",
        "3. G√©n√©rer l'√©tat d√©codeur suivant $s_{t+1}$ en combinant l'√©tat d√©codeur actuel, $s_t$, avec le vecteur de contexte, $c_t$, via une fonction, $f$.\n",
        "\n",
        "  - $s_{t+1} = f\\left ( c_t, s_t \\right)$\n",
        "\n",
        "  Dans Bahdanau et al., 2015, $f$ √©tait une couche feedforward apprise prenant en entr√©e le vecteur concat√©n√© $[c_t; s_t]$, avec $a(s_{t‚àí1}, h_{n})$ √©tant le produit scalaire.\n",
        "  \n",
        "Ensuite, construisons ce sch√©ma d'attention, tel qu'il est utilis√© dans l'architecture du transformeur. Nous avons d√©j√† calcul√© une simple attention par produit scalaire, o√π le score √©tait donn√© par $a(s_{t-1}, h_n)=s_{t-1} h_n^\\top$ et nous allons r√©utiliser la m√™me id√©e.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-MU6rrny8Nj"
      },
      "source": [
        "### De l'auto-attention √† l'attention multi-t√™tes - <font color='blue'>Interm√©diaire</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRuLtxNey_EQ"
      },
      "source": [
        "L'auto-attention et l'attention multi-t√™tes (MHA) sont des composants fondamentaux de l'architecture du transformeur. Dans cette section, nous expliquerons en d√©tail l'intuition derri√®re ces concepts et leur mise en ≈ìuvre. Plus tard, dans la section **Transformers**, vous apprendrez comment ces m√©canismes d'attention sont utilis√©s pour cr√©er un mod√®le s√©quence √† s√©quence qui repose enti√®rement sur l'attention.\n",
        "\n",
        "√Ä mesure que nous avancerons, nous repr√©senterons les phrases en les d√©composant en mots individuels et en encodant chaque mot en utilisant le mod√®le word2vec discut√© pr√©c√©demment. Dans la section Transformers, nous explorerons plus en d√©tail comment les s√©quences d'entr√©e sont transform√©es en une s√©rie de vecteurs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe1lR5_oynOR"
      },
      "outputs": [],
      "source": [
        "def embed_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Encoder une phrase en utilisant word2vec ; pour des cas d'utilisation d'exemple uniquement.\n",
        "    \"\"\"\n",
        "    # nettoyer la phrase (pas n√©cessaire si vous utilisez un tokenizer LLM appropri√©)\n",
        "    sentence = remove_punctuation(sentence)\n",
        "\n",
        "    # extraire les mots individuels\n",
        "    mots = sentence.split()\n",
        "\n",
        "    # obtenir l'embedding word2vec pour chaque mot de la phrase\n",
        "    s√©quence_vecteur_mots, mots = get_word2vec_embedding(mots)\n",
        "\n",
        "    # retour avec dimension suppl√©mentaire (utile pour cr√©er des lots plus tard)\n",
        "    return jnp.expand_dims(s√©quence_vecteur_mots, axis=0), mots\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AFUEFZGzCTv"
      },
      "source": [
        "#### Auto-attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF2V3KI-za9l"
      },
      "source": [
        "L'auto-attention est un m√©canisme d'attention o√π chaque vecteur d'une s√©quence d'entr√©e donn√©e pr√™te attention √† l'ensemble de la s√©quence. Pour comprendre pourquoi l'auto-attention est importante, pensons √† la phrase suivante (exemple tir√© de [source](https://jalammar.github.io/illustrated-transformer/)) :\n",
        "\n",
        "`\"L'animal n'a pas travers√© la rue parce qu'il √©tait trop fatigu√©.\"`\n",
        "\n",
        "Une question simple concernant cette phrase est de savoir √† quoi le mot \"il\" fait r√©f√©rence ? Bien que cela puisse para√Ætre simple, il peut √™tre difficile pour un algorithme d'apprendre cela. C'est l√† que l'auto-attention intervient, car elle peut apprendre une matrice d'attention pour le mot \"il\" o√π un poids important est attribu√© au mot \"animal\".\n",
        "\n",
        "L'auto-attention permet √©galement au mod√®le d'apprendre √† interpr√©ter les mots ayant les m√™mes embeddings, comme \"pomme\", qui peut d√©signer une entreprise ou un aliment, selon le contexte. Cela est tr√®s similaire √† l'√©tat cach√© que l'on trouve dans un RNN, mais ce processus, comme vous le verrez, permet au mod√®le de pr√™ter attention √† l'ensemble de la s√©quence en parall√®le, permettant ainsi d'utiliser des s√©quences plus longues.\n",
        "\n",
        "L'auto-attention se compose de trois concepts :\n",
        "\n",
        "- Requ√™tes, cl√©s et valeurs\n",
        "- Attention par produit scalaire avec √©chelle\n",
        "- Masques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwOIMtdZzdTf"
      },
      "source": [
        "##### **Requ√™tes, cl√©s et valeurs**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEf7QWIWzdo1"
      },
      "source": [
        "En g√©n√©ral, tous les m√©canismes d'attention peuvent √™tre √©crits en termes de paires `cl√©-valeur` et de `requ√™tes` pour calculer la matrice d'attention et le nouveau vecteur de contexte.\n",
        "\n",
        "Pour se faire une id√©e, on peut interpr√©ter le vecteur de `requ√™te` comme contenant les informations que nous cherchons √† obtenir et les vecteurs de `cl√©` comme ayant des informations. Les vecteurs de `requ√™te` sont compar√©s aux vecteurs de `cl√©` pour obtenir des scores d'attention, o√π un score d'attention plus √©lev√© indique qu'une `cl√©` contenait des informations pertinentes. Ces scores d'attention sont ensuite utilis√©s pour d√©terminer quelles `valeurs` (qui sont associ√©es aux `cl√©s`) nous devons prendre en compte. Ou comme l'explique [Lena Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) :\n",
        "\n",
        "- Requ√™te : demande d'information\n",
        "- Cl√© : indiquant qu'elle poss√®de des informations\n",
        "- Valeur : fournissant les informations\n",
        "\n",
        "Dans les architectures de transformeur, nous utilisons des matrices de poids apprenables, repr√©sent√©es par $W_Q, W_K, W_V$, pour projeter chaque vecteur de s√©quence vers des vecteurs $q$, $k$, et $v$ uniques.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-96YjPxhcqW6FczUYwErGXHp6YpoLltq\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "Vous remarquerez que les vecteurs $q, k, v$ sont plus petits que les vecteurs d'entr√©e. Cela sera abord√© ult√©rieurement, mais sachez simplement qu'il s'agit d'un choix de conception pour les transformeurs et non d'une exigence pour fonctionner.\n",
        "\n",
        "Ce processus peut √©galement √™tre parall√©lis√©, car la s√©quence d'entr√©e peut √™tre repr√©sent√©e sous forme de matrice $X$, qui peut √™tre transform√©e en matrices de requ√™tes, cl√©s et valeurs $Q$, $K$, et $V$ respectivement :\n",
        "\n",
        "$Q=W_QX \\\\ K=W_KX \\\\ V=W_VX$\n",
        "\n",
        "Ci-dessous, nous montrons le code qui cr√©e trois couches lin√©aires, projetant les donn√©es d'entr√©e vers les matrices $Q, K, V$, o√π la taille de la sortie peut √™tre ajust√©e.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc8zjK6eziIV"
      },
      "outputs": [],
      "source": [
        "class SequenceToQKV(nn.Module):\n",
        "  taille_sortie: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, X):\n",
        "\n",
        "    # d√©finir la m√©thode d'initialisation des poids\n",
        "    initialisateur = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "\n",
        "    # initialiser trois couches lin√©aires pour faire les transformations QKV.\n",
        "    # note : cela pourrait aussi √™tre une seule couche, comment pensez-vous que vous le feriez ?\n",
        "    couche_q = nn.Dense(self.taille_sortie, kernel_init=initialisateur)\n",
        "    couche_k = nn.Dense(self.taille_sortie, kernel_init=initialisateur)\n",
        "    couche_v = nn.Dense(self.taille_sortie, kernel_init=initialisateur)\n",
        "\n",
        "    # transformer et retourner les matrices\n",
        "    Q = couche_q(X)\n",
        "    K = couche_k(X)\n",
        "    V = couche_v(X)\n",
        "\n",
        "    return Q, K, V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhGZHFsHz_Qp"
      },
      "source": [
        "##### **Attention par produit scalaire avec √©chelle**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxycHDUW0BVE"
      },
      "source": [
        "Maintenant que nous avons nos matrices de `requ√™te`, `cl√©` et `valeur`, il est temps de calculer la matrice d'attention. N'oubliez pas que dans tous les m√©canismes d'attention, nous devons d'abord trouver un score pour chaque vecteur de la s√©quence, puis utiliser ces scores pour cr√©er un nouveau vecteur de contexte. Dans l'auto-attention, le scoring est effectu√© en utilisant l'attention par produit scalaire avec √©chelle, puis les scores normalis√©s sont utilis√©s comme poids pour sommer les vecteurs de valeur et cr√©er le vecteur de contexte.\n",
        "\n",
        "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$\n",
        "\n",
        "o√π les scores d'attention sont calcul√©s par $\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)$ et les scores sont ensuite multipli√©s par $V$ pour obtenir le vecteur de contexte.\n",
        "\n",
        "Ce qui se passe ici est similaire √† ce que nous avons fait dans l'attention par produit scalaire dans la section pr√©c√©dente, sauf que nous appliquons le m√©canisme √† la s√©quence elle-m√™me. Pour chaque √©l√©ment de la s√©quence, nous calculons la matrice de poids d'attention entre $q_i$ et $K$. Nous multiplions ensuite $V$ par chaque poids et enfin, nous additionnons tous les vecteurs pond√©r√©s $v_{weighted}$ ensemble pour former une nouvelle repr√©sentation de $q_i$. Ce faisant, nous √©touffons essentiellement les vecteurs non pertinents et mettons en avant les vecteurs importants de la s√©quence lorsque notre attention est sur $q_1$.\n",
        "\n",
        "$QK^\\top$ est mis √† l'√©chelle par la racine carr√©e de la dimension des vecteurs, $\\sqrt{d_k}$, pour assurer des gradients plus stables pendant l'entra√Ænement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_UYNzrS0Hga"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    \"\"\"\n",
        "    Formule pour retourner l'attention √† produit scalaire √©chelonn√© donn√©e les matrices QKV\n",
        "    \"\"\"\n",
        "    d_k = key.shape[-1]\n",
        "\n",
        "    # obtenir les scores bruts (logits) en effectuant le produit scalaire des requ√™tes et des cl√©s\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "\n",
        "    # √©chelonner les scores bruts et appliquer la fonction softmax pour obtenir les scores/poids d'attention\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "\n",
        "    # multiplier les poids par la matrice de valeur pour obtenir la sortie\n",
        "    output = jnp.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuNaEjIm0PhV"
      },
      "source": [
        "Voyons maintenant l'attention par produit scalaire avec √©chelle en action. Nous allons prendre une phrase, encoder chaque mot en utilisant word2vec, et voir √† quoi ressemblent les poids finaux de l'auto-attention.\n",
        "\n",
        "Nous n'utiliserons pas les couches de projection lin√©aire que nous aurions besoin d'entra√Æner. √Ä la place, nous allons simplifier les choses et utiliser $X=Q=V=K$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oy2sWzR0Ok5"
      },
      "outputs": [],
      "source": [
        "# define a sentence\n",
        "sentence = \"I drink coke, but eat steak\"\n",
        "\n",
        "# embed and create QKV matrices\n",
        "word_embeddings, words = embed_sentence(sentence)\n",
        "Q = K = V = word_embeddings\n",
        "\n",
        "# calculate weights and plot\n",
        "outputs, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# plot the words and the attention weights between them\n",
        "words = remove_punctuation(sentence).split()\n",
        "plot_attention_weight_matrix(attention_weights[0], words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG1Kxljr0Vzw"
      },
      "source": [
        "Gardez √† l'esprit que nous n'avons pas encore entra√Æn√© notre matrice d'attention. Cependant, en utilisant les vecteurs word2vec comme s√©quence, nous pouvons d√©j√† observer que l'attention √† produit scalaire √©chelonn√© est capable de se concentrer sur \"manger\" lorsque \"steak\" est notre requ√™te, et que la requ√™te \"boire\" se concentre davantage sur \"coca\" et \"manger\".\n",
        "\n",
        "Ressources suppl√©mentaires :\n",
        "\n",
        "[Attention avec Q,K,V](https://www.youtube.com/watch?v=k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7B-AgO80gIt"
      },
      "source": [
        "##### **Attention masqu√©e**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdRoKsu70gGW"
      },
      "source": [
        "Il existe des cas o√π appliquer l'auto-attention sur l'ensemble de la s√©quence n'est pas pratique. Ceux-ci peuvent inclure :\n",
        "\n",
        "- S√©quences de longueurs in√©gales regroup√©es ensemble.\n",
        "  - Lors de l'envoi d'un lot de s√©quences √† travers un r√©seau, l'auto-attention s'attend √† ce que chaque s√©quence soit de la m√™me longueur. On g√®re cela en remplissant la s√©quence. Lors du calcul de l'attention, id√©alement, ces tokens de remplissage ne devraient pas √™tre pris en compte.\n",
        "- Entra√Ænement d'un mod√®le d√©codeur.\n",
        "  - Lors de l'entra√Ænement de mod√®les d√©codeurs, tels que GPT-3, le d√©codeur a acc√®s √† toute la s√©quence cible lors de l'entra√Ænement (car l'entra√Ænement est effectu√© en parall√®le). Pour √©viter que la m√©thode ne triche en regardant les tokens futurs, nous devons masquer les donn√©es de la s√©quence future afin que les donn√©es ant√©rieures ne puissent pas y pr√™ter attention.\n",
        "\n",
        "En appliquant un masque au score final calcul√© entre les requ√™tes et les cl√©s, nous pouvons att√©nuer l'influence des vecteurs de s√©quence ind√©sirables. Les vecteurs sont masqu√©s en faisant en sorte que le score entre la requ√™te et leurs cl√©s respectives soit une valeur n√©gative TR√àS grande. Cela a pour effet que la fonction softmax pousse le poids d'attention tr√®s pr√®s de z√©ro, et la valeur r√©sultante sera ignor√©e et n'influencera pas la repr√©sentation finale.\n",
        "\n",
        "En r√©unissant tout, l'attention par produit scalaire avec √©chelle masqu√©e ressemble visuellement √† ceci :\n",
        "\n",
        "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"drawing\" width=\"200\"/>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Syx8_5E0eM9"
      },
      "outputs": [],
      "source": [
        "# exemple de cr√©ation d'un masque pour des tokens de taille 32\n",
        "# le masque s'assure que les positions ne pr√™tent attention qu'aux positions pr√©c√©dentes dans l'entr√©e (masque causal)\n",
        "# nous utiliserons cela plus tard pour ins√©rer des valeurs -inf dans les scores bruts\n",
        "masque = jnp.tril(jnp.ones((32, 32)))\n",
        "\n",
        "# tracer\n",
        "sns.heatmap(masque, cmap=\"Blues\")\n",
        "plt.title(\"Exemple de masque qui peut √™tre appliqu√©\");\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwTJrQ20gDw"
      },
      "source": [
        "Lets now adapt our scaled dot product attention function to implement masked attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVHpyNs_0ePh"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Attention √† produit scalaire √©chelonn√© avec un masque causal\n",
        "    (se concentrant uniquement sur les positions pr√©c√©dentes)\n",
        "    \"\"\"\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "\n",
        "    # obtenir les logits √©chelonn√©s en utilisant le produit scalaire comme pr√©c√©demment\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "\n",
        "    # ajouter un masque optionnel o√π les valeurs le long du masque sont d√©finies √† -inf\n",
        "    if mask is not None:\n",
        "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "\n",
        "    # calculer les poids d'attention via softmax\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "\n",
        "    # faire la somme avec les valeurs pour obtenir la sortie\n",
        "    output = jnp.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWDubQwCs4zG"
      },
      "source": [
        "##### **Attention multi-t√™tes**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHkyjyErsYae"
      },
      "source": [
        "Le m√©canisme d'attention que nous avons couvert jusqu'√† pr√©sent permet au mod√®le de se concentrer sur diff√©rentes positions dans l'entr√©e. En pratique, l'architecture du transformeur utilise une variation subtile de ce m√©canisme, appel√©e attention multi-t√™tes (MHA).\n",
        "\n",
        "La distinction est minime ; plut√¥t que de calculer l'attention une seule fois, le m√©canisme MHA ex√©cute l'attention par produit scalaire avec √©chelle plusieurs fois en parall√®le. Selon l'article *Attention is All You Need*, \"l'attention multi-t√™tes permet au mod√®le de **pr√™ter attention conjointement** aux informations provenant de diff√©rents sous-espaces de repr√©sentation √† diff√©rentes positions. Avec une seule t√™te d'attention, la moyenne inhibe cela.\"\n",
        "\n",
        "L'attention multi-t√™tes peut √™tre vue comme une strat√©gie similaire √† l'empilement de noyaux de convolution dans une couche CNN. Cela permet aux noyaux de se concentrer sur et d'apprendre diff√©rentes caract√©ristiques et r√®gles, ce qui explique pourquoi plusieurs t√™tes d'attention fonctionnent √©galement.\n",
        "\n",
        "La figure ci-dessous montre comment fonctionne l'attention multi-t√™tes de base. L'attention par produit scalaire avec √©chelle discut√©e pr√©c√©demment est simplement r√©p√©t√©e $N$ fois ($N=2$ dans cette figure), avec $3N$ matrices apprenables pour chaque t√™te. Les sorties des diff√©rentes t√™tes sont ensuite concat√©n√©es, apr√®s quoi elles sont pass√©es √† travers une projection lin√©aire, qui produit la repr√©sentation finale.\n",
        "\n",
        "En pratique, l'attention multi-t√™tes surpasse largement l'attention √† une seule t√™te.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1q0Oq6IVEkkMfVSpY4LkHBP866mcoIFsh\" alt=\"drawing\" width=\"1000\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtuqNCln9EWW"
      },
      "source": [
        "Voyons comment impl√©menter l'attention multi-t√™tes. En termes simples, l'attention multi-t√™tes consiste √† ex√©cuter le processus d'attention plusieurs fois en parall√®le, en utilisant diff√©rentes copies des matrices Q, K et V pour chaque \"t√™te\". Cela aide le mod√®le √† se concentrer sur diff√©rentes parties de l'entr√©e en m√™me temps. Si vous souhaitez en savoir plus, consultez [ce blog de Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention) pour une explication d√©taill√©e.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY2xXLMQ9CB6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int  # Nombre de t√™tes d'attention\n",
        "    d_m: int  # Dimension des embeddings du mod√®le\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialiser le module de transformation de la s√©quence en QKV (requ√™te, cl√©, valeur)\n",
        "        self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "\n",
        "        # D√©finir l'initialiseur pour les poids de la couche lin√©aire de sortie\n",
        "        initializer = nn.initializers.variance_scaling(\n",
        "            scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\"\n",
        "        )\n",
        "\n",
        "        # Initialiser la couche de projection de sortie Wo (utilis√©e apr√®s l'attention)\n",
        "        self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "    def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "        # Si Q, K ou V ne sont pas fournis, utiliser l'entr√©e X pour les g√©n√©rer\n",
        "        if None in [Q, K, V]:\n",
        "            assert not X is None, \"X doit √™tre fourni si Q, K ou V ne sont pas fournis\"\n",
        "\n",
        "            # G√©n√©rer les matrices Q, K et V √† partir de l'entr√©e X\n",
        "            Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "        # Extraire la taille du lot (B), la longueur de la s√©quence (T) et la taille de l'embedding (d_m)\n",
        "        B, T, d_m = K.shape\n",
        "\n",
        "        # Calculer la taille de l'embedding de chaque t√™te d'attention (d_m / num_heads)\n",
        "        head_size = d_m // self.num_heads\n",
        "\n",
        "        # Reshaper Q, K, V pour avoir des dimensions s√©par√©es pour les t√™tes\n",
        "        # B, T, d_m -> B, T, num_heads, head_size -> B, num_heads, T, head_size\n",
        "        q_heads = Q.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        k_heads = K.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        v_heads = V.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "        # Appliquer l'attention √† produit scalaire √©chelonn√© √† chaque t√™te\n",
        "        attention, attention_weights = scaled_dot_product_attention(\n",
        "            q_heads, k_heads, v_heads, mask\n",
        "        )\n",
        "\n",
        "        # Reshaper la sortie de l'attention √† ses dimensions originales\n",
        "        # (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, d_m)\n",
        "        attention = attention.swapaxes(1, 2).reshape(B, T, d_m)\n",
        "\n",
        "        # Appliquer la transformation lin√©aire de sortie Wo √† la sortie de l'attention\n",
        "        X_new = self.Wo(attention)\n",
        "\n",
        "        # Si return_weights est True, retourner √† la fois la sortie transform√©e et les poids d'attention\n",
        "        if return_weights:\n",
        "            return X_new, attention_weights\n",
        "        else:\n",
        "            # Sinon, retourner uniquement la sortie transform√©e\n",
        "            return X_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **2. Construire votre propre LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA_2coZvhAg3"
      },
      "source": [
        "### 2.1 Vue d'ensemble g√©n√©rale <font color='orange'>D√©butant</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BflycqAw_RF8"
      },
      "source": [
        "L'architecture du Transformeur a √©t√© pr√©sent√©e pour la premi√®re fois dans l'article intitul√© [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) par Vaswani et al.\n",
        "\n",
        "Comme le titre de l'article le sugg√®re, une telle architecture consiste essentiellement uniquement en des m√©canismes d'attention ainsi que des couches de feed-forward et des couches lin√©aires, comme le montre le sch√©ma ci-dessous.\n",
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"350\" />\n",
        "\n",
        "Les Transformeurs et leurs variantes sont au c≈ìur des Mod√®les de Langage de Grande Taille, et il n'est pas exag√©r√© de dire que presque tous les mod√®les de langage existants sont bas√©s sur des architectures de Transformeurs.\n",
        "\n",
        "Comme vous pouvez le voir dans le sch√©ma, l'architecture originale du Transformeur se compose de deux parties, l'une qui re√ßoit les entr√©es, g√©n√©ralement appel√©e encodeur, et l'autre qui re√ßoit les sorties (c'est-√†-dire les cibles), appel√©e d√©codeur. Cela est d√ª au fait que le transformeur a √©t√© con√ßu pour la traduction automatique.\n",
        "\n",
        "L'encodeur re√ßoit une phrase d'entr√©e dans une langue et la traite √† travers plusieurs `blocs d'encodeur` empil√©s. Cela cr√©e une repr√©sentation finale, qui contient des informations utiles n√©cessaires pour la t√¢che de d√©codage. Cette sortie est ensuite aliment√©e dans des `blocs de d√©codeur` empil√©s qui produisent de nouvelles sorties de mani√®re autoregressive.\n",
        "\n",
        "L'encodeur se compose de $N$ blocs identiques, qui traitent une s√©quence de vecteurs de tokens s√©quentiellement. Ces blocs se composent de 3 parties :\n",
        "\n",
        "1. Un bloc d'attention multi-t√™tes. Ce sont la colonne vert√©brale de l'architecture du transformeur. Ils traitent les donn√©es pour g√©n√©rer des repr√©sentations pour chaque token, en s'assurant que les informations n√©cessaires pour la t√¢che √† accomplir sont repr√©sent√©es dans les vecteurs. Ce sont exactement les MHA que nous avons couverts dans la section sur l'attention pr√©c√©demment.\n",
        "2. Un MLP (Perceptron Multi-Couches, c'est-√†-dire un r√©seau neuronal avec plusieurs couches) est appliqu√© √† chaque token d'entr√©e s√©par√©ment et de mani√®re identique.\n",
        "3. Une connexion r√©siduelle qui ajoute les tokens d'entr√©e aux repr√©sentations attentives et une connexion r√©siduelle entre l'entr√©e du MLP et ses sorties. Pour ces deux connexions, le r√©sultat est normalis√© √† l'aide de layernorm. Dans certaines impl√©mentations, ces √©tapes de normalisation sont appliqu√©es aux entr√©es plut√¥t qu'aux sorties. Tout comme un Resnet, les transformeurs sont con√ßus pour √™tre des mod√®les tr√®s profonds, donc ces blocs add and norm sont essentiels pour un flux de gradient fluide.\n",
        "\n",
        "De m√™me, le bloc d√©codeur se compose de $N$ blocs identiques, cependant, il y a quelques variations au sein de ces blocs. Concr√®tement, les diff√©rentes parties sont :\n",
        "\n",
        "1. Un bloc d'attention multi-t√™tes masqu√©. Il s'agit d'un bloc MHA qui effectue l'_auto-attention_ sur la s√©quence de sortie, mais ce calcul est restreint aux entr√©es d√©j√† vues. En d'autres termes, les tokens futurs sont bloqu√©s lors des pr√©dictions.\n",
        "2. Un bloc d'attention multi-t√™tes. Ce bloc re√ßoit la sortie du dernier bloc encodeur, les tokens transform√©s, et l'utilise comme paires cl√©-valeur, tout en utilisant la sortie du premier bloc MHA comme requ√™te. Ce faisant, le mod√®le porte son attention sur l'entr√©e requise pour effectuer la t√¢che de s√©quence. Ce bloc MHA effectue donc une _cross-attention_ en regardant les entr√©es de l'encodeur.\n",
        "3. Un MLP identique √† celui de l'encodeur\n",
        "4. Une connexion r√©siduelle identique √† celle de l'encodeur.\n",
        "\n",
        "√Ä partir de cette architecture originale, plusieurs variations ont √©t√© propos√©es, certaines se concentrant uniquement sur l'encodeur et d'autres uniquement sur le **d√©codeur**. Les grands mod√®les de langage (LLMs) tels que GPT-2, GPT-3 et Turing-NLG sont issus d'architectures uniquement d√©codeur. Ces architectures ressemblent √† ceci :\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "avec le bloc de cross-attention manquant car aucune sortie de l'encodeur n'est disponible. Pour construire un mod√®le de langage, nous nous concentrerons donc sur l'architecture uniquement d√©codeur comme celle illustr√©e ci-dessus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbTsk0MdhAhC"
      },
      "source": [
        "### 2.2 Tokenisation + Encodage positionnel <font color='orange'>D√©butant</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DehUpfym_RF8"
      },
      "source": [
        "#### 2.2.1 Tokenisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBiFpVBu_RF9"
      },
      "source": [
        "Les transformeurs ne peuvent pas traiter des cha√Ænes de texte brutes. Pour traiter le texte, celui-ci est d'abord divis√© en tokens. Les tokens sont ensuite index√©s, et chaque token se voit attribuer un embedding de taille $d_{model}$. Ces embeddings peuvent √™tre appris pendant l'entra√Ænement ou provenir d'un vocabulaire d'embeddings pr√©entra√Æn√©s. Cette nouvelle s√©quence d'embeddings de tokens est ensuite transmise √† l'architecture du transformeur. Cette id√©e est visualis√©e ci-dessous.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16euh4LADP_mcXywFwKKY3QQQkVplepiI\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Ces identifiants de tokens sont g√©n√©ralement pr√©dits lorsqu'un mod√®le g√©n√®re du texte, compl√®te des mots manquants, etc.\n",
        "\n",
        "Ce processus de division du texte en tokens et d'attribution d'un identifiant √† chaque token est appel√© [tokenisation](https://huggingface.co/docs/transformers/tokenizer_summary). Il existe plusieurs fa√ßons de tokeniser le texte, certaines m√©thodes √©tant directement entra√Æn√©es √† partir des donn√©es. Lors de l'utilisation de transformeurs pr√©-entra√Æn√©s, il est crucial d'utiliser le m√™me tokeniseur que celui utilis√© pour entra√Æner le mod√®le. Le lien pr√©c√©dent propose des descriptions approfondies de nombreuses techniques largement connues.\n",
        "\n",
        "Ci-dessous, nous montrons comment le tokeniseur du mod√®le [BERT](https://arxiv.org/abs/1810.04805) tokenise une phrase. Nous utilisons [Hugging Face](https://huggingface.co/) pour cette partie.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJBMvlUA_RF9"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = bert_tokenizer(\"La pratique est tellement amusante\")\n",
        "print(f\"IDs des tokens : {encoded_input['input_ids']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYbtZTVP_RF9"
      },
      "source": [
        "Ici, nous pouvons voir que le tokeniseur retourne les IDs pour chaque token, comme illustr√© dans la figure. Mais en comptant le nombre d'IDs, nous constatons qu'il est plus grand que le nombre de mots dans la phrase. Imprimons les tokens associ√©s √† chaque ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPZjiLis_RF9"
      },
      "outputs": [],
      "source": [
        "print(f\"Tokens: {bert_tokenizer.decode(encoded_input['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3K8UFlR_RF9"
      },
      "source": [
        "Nous pouvons voir que le tokeniseur ajoute de nouveaux tokens, `[CLS]` et `[SEP]`, au d√©but et √† la fin de la s√©quence. Il s'agit d'une exigence sp√©cifique √† BERT pour l'entra√Ænement et l'inf√©rence. Ajouter des tokens sp√©ciaux est une pratique tr√®s courante. Gr√¢ce √† ces tokens sp√©ciaux, nous pouvons indiquer √† un mod√®le quand une phrase commence ou se termine, ou quand une nouvelle partie de l'entr√©e commence. Cela peut √™tre utile pour r√©aliser diff√©rentes t√¢ches.\n",
        "\n",
        "Par exemple, pour pr√©entra√Æner certains transformeurs sp√©cifiques, ils effectuent ce que l'on appelle une pr√©diction masqu√©e. Pour cela, des tokens al√©atoires dans une s√©quence sont remplac√©s par le token `[MASK]`, et le mod√®le est entra√Æn√© √† pr√©dire l'ID correct du token remplac√© par ce token.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djMP4Ijz_RF9"
      },
      "source": [
        "**Inconv√©nient de l'utilisation des tokens bruts** :\n",
        "\n",
        "Un inconv√©nient de l'utilisation des tokens bruts est qu'ils ne fournissent aucune indication sur la position du mot dans la s√©quence. Cela est √©vident lorsqu'on consid√®re des phrases comme \"Je suis heureux\" et \"Suis-je heureux\" - ces deux phrases ont des significations distinctes, et le mod√®le doit saisir l'ordre des mots pour comprendre le message voulu de mani√®re pr√©cise.\n",
        "\n",
        "Pour rem√©dier √† cela, lors de la conversion des entr√©es en vecteurs, des vecteurs de position sont introduits et ajout√©s √† ces vecteurs pour indiquer la **position** de chaque mot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "639s7Zuk_RF9"
      },
      "source": [
        "#### 2.2.2 Encodages positionnels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-hBFVYo_RF9"
      },
      "source": [
        "Dans la plupart des domaines o√π un transformeur peut √™tre utilis√©, il existe un ordre sous-jacent aux tokens produits, qu'il s'agisse de l'ordre des mots dans une phrase, de l'emplacement √† partir duquel des patches sont pris dans une image ou m√™me des √©tapes effectu√©es dans un environnement de RL. Cet ordre est tr√®s important dans tous les cas ; imaginez simplement que vous interpr√©tez la phrase \"Je dois lire ce livre.\" comme \"J'ai ce livre √† lire.\". Les deux phrases contiennent exactement les m√™mes mots, mais elles ont des significations compl√®tement diff√©rentes en fonction de l'ordre.\n",
        "\n",
        "√âtant donn√© que les blocs d'encodeur et de d√©codeur traitent tous les tokens en parall√®le, l'ordre des tokens est perdu dans ces calculs. Pour rem√©dier √† cela, l'ordre de la s√©quence doit √™tre directement inject√© dans les tokens. Cela peut √™tre fait en ajoutant des *encodages positionnels* aux tokens au d√©but des blocs d'encodeur et de d√©codeur (bien que certaines des techniques les plus r√©centes ajoutent des informations positionnelles dans les blocs d'attention). Un exemple de la fa√ßon dont les encodages positionnels modifient les tokens est montr√© ci-dessous.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eSgnVN2hnEsrjdHygDGwk1kxEi8-dcFo\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "Id√©alement, ces encodages devraient avoir les caract√©ristiques suivantes ([source](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)) :\n",
        "* Chaque √©tape temporelle devrait avoir une valeur unique.\n",
        "* La distance entre les √©tapes temporelles doit rester constante.\n",
        "* L'encodage devrait pouvoir se g√©n√©raliser √† des s√©quences plus longues que celles vues pendant l'entra√Ænement.\n",
        "* L'encodage doit √™tre d√©terministe.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rklY-aL-_RF9"
      },
      "source": [
        "##### **Fonctions sinus et cosinus**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLcfkMku_RF9"
      },
      "source": [
        "Dans *Attention is All you Need*, les auteurs ont utilis√© une m√©thode qui peut satisfaire toutes ces exigences. Cela implique de sommer une combinaison d'ondes sinuso√Ødales et cosinuso√Ødales √† diff√©rentes fr√©quences, avec la formule pour un encodage positionnel √† la position $D$ montr√©e ci-dessous, o√π $i$ est l'indice de l'embedding et $d_m$ est la taille de l'embedding du token.\n",
        "\n",
        "\\\\\n",
        "\n",
        "$P_{D}= \\begin{cases}\\sin \\left(\\frac{D}{10000^{i/d_{m}}}\\right), & \\text { si } i \\bmod 2=0 \\\\ \\cos \\left(\\frac{D}{10000^{((i-1)/d_{m}}}\\right), & \\text { sinon } \\end{cases}$\n",
        "\n",
        "\\\n",
        "\n",
        "En supposant que notre mod√®le ait $d_m=8$, l'embedding positionnel ressemblera √† ceci :\n",
        "\n",
        "\\\n",
        "$P_{D}=\\left[\\begin{array}{c}\\sin \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{8/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{8/8}}\\right)\\end{array}\\right]$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Commen√ßons par cr√©er une fonction capable de retourner ces encodages pour comprendre pourquoi cela fonctionne.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5t5D30_RF9"
      },
      "outputs": [],
      "source": [
        "def return_frequency_pe_matrix(longueur_sequence_tokens, taille_embedding_tokens):\n",
        "\n",
        "    assert taille_embedding_tokens % 2 == 0, \"la taille de l'embedding des tokens doit √™tre divisible par deux\"\n",
        "\n",
        "    P = jnp.zeros((longueur_sequence_tokens, taille_embedding_tokens))\n",
        "    positions = jnp.arange(0, longueur_sequence_tokens)[:, jnp.newaxis]\n",
        "\n",
        "    i = jnp.arange(0, taille_embedding_tokens, 2)\n",
        "    pas_frequence = jnp.exp(i * (-math.log(10000.0) / taille_embedding_tokens))\n",
        "    frequences = positions * pas_frequence\n",
        "\n",
        "    P = P.at[:, 0::2].set(jnp.sin(frequences))\n",
        "    P = P.at[:, 1::2].set(jnp.cos(frequences))\n",
        "\n",
        "    return P\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYW-VDOL_RF-"
      },
      "outputs": [],
      "source": [
        "longueur_sequence_tokens = 50  # Nombre de tokens que le mod√®le devra traiter\n",
        "dimension_embedding_tokens = 10000  # Dimensions des embeddings des tokens (et de l'encodage positionnel), s'assurer qu'il est divisible par deux\n",
        "P = return_frequency_pe_matrix(longueur_sequence_tokens, dimension_embedding_tokens)\n",
        "plot_position_encodings(P, longueur_sequence_tokens, dimension_embedding_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mjHEDPO_RF-"
      },
      "source": [
        "En regardant le graphique ci-dessus, nous pouvons voir que pour chaque indice de position, il y a un motif unique qui se forme, o√π chaque indice de position aura toujours le m√™me encodage.\n",
        "\n",
        "**T√¢che de groupe** :\n",
        "\n",
        "- Discutez avec votre ami pourquoi nous voyons ce motif sp√©cifique lorsque `longueur_sequence_tokens` est 1000, et `dimension_embedding_tokens` est 768.\n",
        "- Vous pouvez essayer de jouer avec des valeurs plus petites pour `longueur_sequence_tokens` et `dimension_embedding_tokens` pour obtenir une meilleure intuition pour la discussion ci-dessus.\n",
        "- Demandez √† votre ami pourquoi ils pensent que la constante 10000 est utilis√©e dans les fonctions ci-dessus.\n",
        "- R√©glez `longueur_sequence_tokens` sur 50 et `dimension_embedding_tokens` sur quelque chose de grand, comme 10000. Que remarquez-vous ? Un grand embedding de token est-il toujours n√©cessaire ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNPg0pnhAhG"
      },
      "source": [
        "### 2.3 Bloc Transformer   <font color='green'>Interm√©diaire</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4vSolF2_RF-"
      },
      "source": [
        "Tout comme un MLP (un r√©seau de neurones simple qui traite les donn√©es d'entr√©e √† travers plusieurs couches) ou un CNN (un type de r√©seau de neurones qui excelle dans la reconnaissance de motifs dans les images en utilisant des couches de convolution), les transformers sont constitu√©s d'une pile de blocs transformer. Dans cette section, nous allons construire chacun des composants n√©cessaires pour cr√©er un de ces blocs transformer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTURbfr__RF-"
      },
      "source": [
        "#### 2.3.1 R√©seau de neurones Feed Forward (FFN) / Perceptron multicouche (MLP) <font color='orange'>D√©butant</font>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H1pVFxJiSpM_Ozj1eKWNdcFQ5Hn5XsZz\" alt=\"drawing\" width=\"260\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTtFi9AZ_RF-"
      },
      "source": [
        "Ces blocs sont simplement un MLP (perceptron multicouche) √† 2 couches qui utilise la fonction d'activation ReLU dans le mod√®le original. La fonction GeLU est √©galement devenue tr√®s populaire, et nous l'utiliserons tout au long de la pratique. La formule ci-dessous repr√©sente le r√©seau de neurones feedforward (FFN) avec activation GeLU, o√π l'entr√©e `x` est transform√©e √† travers deux couches lin√©aires avec des poids `W1` et `W2`, suivis de termes de biais `b1` et `b2`, et la fonction `max` repr√©sente la fonction d'activation ReLU.\n",
        "\n",
        "$$\n",
        "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
        "$$\n",
        "\n",
        "On peut interpr√©ter ce bloc comme traitant ce que le bloc MHA a produit, puis projetant ces nouvelles repr√©sentations de tokens dans un espace que le bloc suivant peut utiliser de mani√®re plus optimale. En g√©n√©ral, la premi√®re couche est tr√®s large, dans la gamme de 2 √† 8 fois la taille des repr√©sentations de tokens. Ils le font car il est plus facile de parall√©liser les calculs pour une seule couche plus large pendant l'entra√Ænement que de parall√©liser un bloc feedforward avec plusieurs couches. Ainsi, ils peuvent ajouter plus de complexit√© tout en gardant l'entra√Ænement et l'inf√©rence optimis√©s.\n",
        "\n",
        "**T√¢che de code :** Codez un module Flax qui impl√©mente le bloc feedforward.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsho1CnW_RF-"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Un MLP √† 2 couches qui √©largit puis r√©tr√©cit l'entr√©e.\n",
        "\n",
        "  Args:\n",
        "    widening_factor [optionnel, par d√©faut=4] : La taille de la couche cach√©e sera d_model * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      x: [B, T, d_m]\n",
        "    '''\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "    layer1 = ... # TERMINER\n",
        "    layer2 = ... # TERMINER\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qj0nfhH_RF-"
      },
      "outputs": [],
      "source": [
        "# @title R√©ponse √† la t√¢che de code (Essayez de ne pas regarder avant d'avoir bien r√©fl√©chi !)\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Un MLP √† 2 couches qui √©largit puis r√©duit l'entr√©e.\n",
        "\n",
        "  Args:\n",
        "    widening_factor [optionnel, par d√©faut=4] : La taille de la couche cach√©e sera d_model * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: [B, T, d_m]\n",
        "\n",
        "    Retour:\n",
        "      x: [B, T, d_m]\n",
        "    '''\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "\n",
        "    # Indice : La couche 1 est une couche dense (couche enti√®rement connect√©e) qui augmente la taille de l'entr√©e\n",
        "    # par le facteur d'√©largissement. Utilisez nn.Dense pour cr√©er cette couche avec layer1_size comme taille de sortie.\n",
        "    layer1 = ... # FINISH ME\n",
        "\n",
        "    # Indice : La couche 2 est une autre couche dense qui r√©duit la taille pour revenir √† la dimension d'origine d_m.\n",
        "    # Utilisez nn.Dense avec d_m comme taille de sortie pour cr√©er cette couche.\n",
        "    layer2 = ...# FINISH ME\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))  # Appliquez la fonction d'activation GeLU √† la sortie de la couche 1\n",
        "    x = layer2(x)  # Passez le r√©sultat √† travers la couche 2\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZijQtEMyHxj"
      },
      "outputs": [],
      "source": [
        "# @title R√©ponse √† la t√¢che de codage (Essayez de ne pas regarder avant d'avoir bien essay√© !')\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    \"\"\"Un MLP (Multi-Layer Perceptron) √† 2 couches qui commence par √©largir la taille de l'entr√©e, puis la r√©duit √† nouveau.\"\"\"\n",
        "\n",
        "    # widening_factor contr√¥le l'expansion de la dimension de l'entr√©e dans la premi√®re couche.\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    # init_scale contr√¥le le facteur d'√©chelle pour l'initialisation des poids.\n",
        "    init_scale: float = 0.25\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "      # Obtenir la taille de la derni√®re dimension de l'entr√©e (taille de l'embedding).\n",
        "      d_m = x.shape[-1]\n",
        "\n",
        "      # Calculer la taille de la premi√®re couche en multipliant la taille de l'embedding par le facteur d'√©largissement.\n",
        "      layer1_size = self.widening_factor * d_m\n",
        "\n",
        "      # Initialiser les poids des deux couches en utilisant un initialiseur bas√© sur la variance.\n",
        "      initializer = nn.initializers.variance_scaling(\n",
        "          scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "      )\n",
        "\n",
        "      # D√©finir la premi√®re couche dense, qui √©largit la taille de l'entr√©e.\n",
        "      layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
        "\n",
        "      # D√©finir la deuxi√®me couche dense, qui r√©duit la taille pour revenir √† la dimension d'origine.\n",
        "      layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
        "\n",
        "      # Appliquer la premi√®re couche dense suivie d'une fonction d'activation GELU.\n",
        "      x = jax.nn.gelu(layer1(x))\n",
        "\n",
        "      # Appliquer la deuxi√®me couche dense pour ramener les donn√©es √† leur dimension d'origine.\n",
        "      x = layer2(x)\n",
        "\n",
        "      # Retourner la sortie finale.\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWUpf8wt_RF-"
      },
      "source": [
        "#### 2.3.2 Bloc Ajouter et Normaliser <font color='orange'>D√©butant</font>\n",
        "\n",
        "Pour permettre aux transformateurs de devenir plus profonds, les connexions r√©siduelles sont tr√®s importantes pour permettre un meilleur flux des gradients √† travers le r√©seau. Pour la normalisation, `layer norm` est utilis√©. Cette normalisation est appliqu√©e ind√©pendamment √† chaque vecteur de token dans le lot. Il est constat√© que la normalisation des vecteurs am√©liore la convergence et la stabilit√© des transformateurs.\n",
        "\n",
        "Il y a deux param√®tres apprenables dans la normalisation par couche (`layer norm`), `scale` et `bias`, qui redimensionnent la valeur normalis√©e. Ainsi, pour chaque token d'entr√©e dans un lot, nous calculons la moyenne, $\\mu_{i}$ et la variance $\\sigma_i^2$. Nous normalisons ensuite le token avec :\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_{i}}{\\sigma_i^2 + œµ}\n",
        "$$\n",
        "\n",
        "Puis $\\hat{x}$ est redimensionn√© en utilisant le `scale` appris, $Œ≥$, et le `bias` $Œ≤$, avec :\n",
        "\n",
        "$$\n",
        "y_i = Œ≥\\hat{x}_i + Œ≤ = LN_{Œ≥,Œ≤}(x_i)\n",
        "$$\n",
        "\n",
        "Ainsi, notre bloc ajouter et normaliser peut √™tre repr√©sent√© par $LN(x+f(x))$, o√π $f(x)$ est soit un bloc MLP soit MHA.\n",
        "\n",
        "**T√¢che de code :** Impl√©mentez un module Flax qui r√©alise le bloc ajouter et normaliser. Il doit prendre en entr√©e les tokens trait√©s et non trait√©s. Indice : `hk.LayerNorm`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5bLb5Ly_RF_"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "  \"\"\"Un bloc qui impl√©mente le bloc d'addition et de normalisation\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, processed_x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: S√©quence de tokens avant d'√™tre envoy√©e dans les blocs MHA ou FF, avec une forme [B, T, d_m]\n",
        "      processed_x: S√©quence apr√®s avoir √©t√© trait√©e par les blocs MHA ou FF, avec une forme [B, T, d_m]\n",
        "\n",
        "    Retour:\n",
        "      add_norm_x: Tokens transform√©s avec une forme [B, T, d_m]\n",
        "    '''\n",
        "    # Indice : La premi√®re √©tape consiste √† ajouter l'entr√©e originale `x` √† l'entr√©e trait√©e `processed_x`.\n",
        "    added = ... # FINISH ME\n",
        "\n",
        "    # Indice : La deuxi√®me √©tape n√©cessite l'application d'une normalisation par couche au r√©sultat de l'addition.\n",
        "    # Utilisez `nn.LayerNorm` et d√©finissez `reduction_axes=-1` pour appliquer la normalisation sur la derni√®re dimension.\n",
        "    normalised = ... #FINISH ME\n",
        "    return normalised(added)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nsWUQnlyWIs"
      },
      "outputs": [],
      "source": [
        "# @title R√©ponse √† la t√¢che de codage (Essayez de ne pas regarder avant d'avoir bien essay√© !')\n",
        "\n",
        "class AddNorm(nn.Module):\n",
        "    \"\"\"Un bloc qui impl√©mente l'op√©ration 'Add and Norm' utilis√©e dans les transformers.\"\"\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, processed_x):\n",
        "      # √âtape 1 : Ajouter l'entr√©e originale (x) √† l'entr√©e trait√©e (processed_x).\n",
        "      added = x + processed_x\n",
        "\n",
        "      # √âtape 2 : Appliquer une normalisation par couche au r√©sultat de l'addition.\n",
        "      # - LayerNorm aide √† stabiliser et am√©liorer le processus d'entra√Ænement en normalisant la sortie.\n",
        "      # - reduction_axes=-1 indique que la normalisation est appliqu√©e sur la derni√®re dimension (g√©n√©ralement la dimension de l'embedding).\n",
        "      # - use_scale=True et use_bias=True permettent √† la couche d'apprendre des param√®tres d'√©chelle et de biais pour un ajustement plus pr√©cis.\n",
        "      normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
        "\n",
        "      # Retourner le r√©sultat normalis√©.\n",
        "      return normalised(added)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91dXd29b_RF_"
      },
      "source": [
        "### 2.4 Construction du D√©codeur Transformer / LLM <font color='green'>Interm√©diaire</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl0UAyvM_RF_"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "La plupart des √©l√©ments de base ont √©t√© r√©alis√©s. Nous avons construit le bloc d'encodage positionnel, le bloc MHA, le bloc feed-forward et le bloc add&norm.\n",
        "\n",
        "La seule partie n√©cessaire est de passer les entr√©es √† chaque bloc d√©codeur et d'appliquer le bloc MHA masqu√© trouv√© dans les blocs d√©codeurs.\n",
        "\n",
        "**T√¢che de code :** Codez un module FLAX qui impl√©mente le (FFN(norm(MHA(norm(X))))) pour le bloc d√©codeur\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVmSFKZK_RF_"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Bloc d√©codeur du Transformer.\n",
        "\n",
        "  Args:\n",
        "    num_heads: Le nombre de t√™tes √† utiliser dans le bloc MHA.\n",
        "    d_m: Taille de l'embedding des tokens\n",
        "    widening_factor: La taille de la couche cach√©e sera d_m * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Lot de tokens √©tant pass√©s dans le d√©codeur, avec une forme [B, T_decoder, d_m]\n",
        "      encoder_output: Lot de tokens trait√©s par l'encodeur, avec une forme [B, T_encoder, d_m]\n",
        "      mask [optionnel, par d√©faut=None]: Masque √† appliquer, avec une forme [T_decoder, T_decoder].\n",
        "      return_att_weight [optionnel, par d√©faut=True]: Indique si les poids d'attention doivent √™tre retourn√©s.\n",
        "    \"\"\"\n",
        "\n",
        "    attention, attention_weights_1 = ... # FINISH ME\n",
        "\n",
        "    X = ... # FINISH ME\n",
        "\n",
        "    projection = ... # FINISH ME\n",
        "    X = ... # FINISH ME\n",
        "\n",
        "    return (X, attention_weights_1) if return_att_weight else X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stNZVVv3_RF_"
      },
      "outputs": [],
      "source": [
        "#@title R√©ponse √† la t√¢che de codage (Essayez de ne pas regarder avant d'avoir bien essay√© !')\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bloc d√©codeur du Transformer.\n",
        "\n",
        "    Args:\n",
        "        num_heads: Le nombre de t√™tes d'attention dans le bloc Multi-Head\n",
        "        Attention (MHA).\n",
        "        d_m: La taille des embeddings des tokens.\n",
        "        widening_factor: Le facteur par lequel la taille de la couche cach√©e\n",
        "        est augment√©e dans le MLP.\n",
        "    \"\"\"\n",
        "\n",
        "    num_heads: int\n",
        "    d_m: int\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "      # Initialiser le bloc Multi-Head Attention (MHA)\n",
        "      self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "\n",
        "      # Initialiser les blocs AddNorm pour les connexions r√©siduelles\n",
        "      # et la normalisation\n",
        "      self.add_norm1 = AddNorm()  # Premier bloc AddNorm apr√®s MHA\n",
        "      self.add_norm2 = AddNorm()  # Deuxi√®me bloc AddNorm apr√®s le MLP\n",
        "\n",
        "      # Initialiser le FeedForwardBlock (MLP) qui traite les donn√©es\n",
        "      # apr√®s l'attention\n",
        "      self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weight=True):\n",
        "      \"\"\"\n",
        "      Passage en avant √† travers le DecoderBlock.\n",
        "\n",
        "      Args:\n",
        "          X: Lot de tokens d'entr√©e envoy√©s dans le d√©codeur,\n",
        "          forme [B, T_decoder, d_m]\n",
        "          mask [optionnel, par d√©faut=None]: Masque pour contr√¥ler les positions\n",
        "          que l'attention est autoris√©e √† consid√©rer,\n",
        "          forme [T_decoder, T_decoder].\n",
        "          return_att_weight [optionnel, par d√©faut=True]: Si True,\n",
        "          retourne les poids d'attention avec la sortie.\n",
        "\n",
        "      Returns:\n",
        "          Si return_att_weight est True, retourne un tuple (X,\n",
        "          attention_weights_1).\n",
        "          Sinon, retourne les repr√©sentations des tokens trait√©s X.\n",
        "      \"\"\"\n",
        "      # Appliquer l'attention multi-t√™te aux tokens d'entr√©e (X)\n",
        "      # avec un masquage optionnel\n",
        "      attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "\n",
        "      # Appliquer le premier bloc AddNorm (ajoute l'entr√©e originale X\n",
        "      # et normalise)\n",
        "      X = self.add_norm1(X, attention)\n",
        "\n",
        "      # Passer le r√©sultat √† travers le FeedForwardBlock (MLP)\n",
        "      # pour traiter davantage les donn√©es\n",
        "      projection = self.MLP(X)\n",
        "\n",
        "      # Appliquer le deuxi√®me bloc AddNorm (ajoute l'entr√©e de l'√©tape\n",
        "      # pr√©c√©dente et normalise)\n",
        "      X = self.add_norm2(X, projection)\n",
        "\n",
        "      # Retourner la sortie finale X, et √©ventuellement les poids d'attention\n",
        "      return (X, attention_weights_1) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXXVWd7_RF_"
      },
      "source": [
        "Ensuite, nous allons assembler le tout, en ajoutant les encodages positionnels ainsi qu'en empilant plusieurs blocs de transformateur et en ajoutant notre couche de pr√©diction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XBG24Qs_RF_"
      },
      "outputs": [],
      "source": [
        "class LLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Mod√®le Transformer compos√© de plusieurs couches de blocs d√©codeurs.\n",
        "\n",
        "    Args:\n",
        "        num_heads: Nombre de t√™tes d'attention dans chaque bloc Multi-Head\n",
        "        Attention (MHA).\n",
        "        num_layers: Nombre de blocs d√©codeurs dans le mod√®le.\n",
        "        d_m: Dimensionnalit√© des embeddings des tokens.\n",
        "        vocab_size: Taille du vocabulaire (nombre de tokens uniques).\n",
        "        widening_factor: Facteur par lequel la taille de la couche cach√©e\n",
        "        est augment√©e dans le MLP.\n",
        "    \"\"\"\n",
        "    num_heads: int\n",
        "    num_layers: int\n",
        "    d_m: int\n",
        "    vocab_size: int\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialiser une liste de blocs d√©codeurs, un pour chaque\n",
        "        # couche du mod√®le\n",
        "        self.blocks = [\n",
        "            DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        # Initialiser une couche d'embedding pour convertir les IDs de\n",
        "        # tokens en embeddings de tokens\n",
        "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m)\n",
        "\n",
        "        # Initialiser une couche dense pour pr√©dire le prochain token\n",
        "        # dans la s√©quence\n",
        "        self.pred_layer = nn.Dense(self.vocab_size)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weights=False):\n",
        "        \"\"\"\n",
        "        Passage en avant √† travers le mod√®le LLM.\n",
        "\n",
        "        Args:\n",
        "            X: Lot d'IDs de tokens d'entr√©e, forme [B, T_decoder]\n",
        "            o√π B est la taille du lot et T_decoder est la longueur\n",
        "            de la s√©quence.\n",
        "            mask [optionnel, par d√©faut=None]: Masque pour contr√¥ler les\n",
        "            positions sur lesquelles l'attention peut se concentrer,\n",
        "            forme [T_decoder, T_decoder].\n",
        "            return_att_weights [optionnel, par d√©faut=False]: Indique\n",
        "            si les poids d'attention doivent √™tre retourn√©s.\n",
        "\n",
        "        Returns:\n",
        "            logits: Les probabilit√©s pr√©dites pour chaque token dans\n",
        "            le vocabulaire.\n",
        "            Si return_att_weights est True, retourne √©galement\n",
        "            les poids d'attention.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convertir les IDs de tokens en embeddings (forme\n",
        "        # [B, T_decoder, d_m])\n",
        "        X = self.embedding(X)\n",
        "\n",
        "        # Obtenir la longueur de la s√©quence d'entr√©e\n",
        "        sequence_len = X.shape[-2]\n",
        "\n",
        "        # G√©n√©rer des encodages positionnels et les ajouter aux\n",
        "        # embeddings des tokens\n",
        "        positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "        X = X + positions\n",
        "\n",
        "        # Initialiser une liste pour stocker les poids d'attention\n",
        "        # si n√©cessaire\n",
        "        if return_att_weights:\n",
        "            att_weights = []\n",
        "\n",
        "        # Passer les embeddings √† travers chaque bloc d√©codeur\n",
        "        # en s√©quence\n",
        "        for block in self.blocks:\n",
        "            out = block(X, mask, return_att_weights)\n",
        "            if return_att_weights:\n",
        "                # Si on retourne les poids d'attention, d√©baller la sortie\n",
        "                X = out[0]\n",
        "                att_weights.append(out[1])\n",
        "            else:\n",
        "                # Sinon, mettre √† jour simplement l'entr√©e pour le\n",
        "                # bloc suivant\n",
        "                X = out\n",
        "\n",
        "        # Appliquer une couche dense suivie d'un log softmax pour obtenir\n",
        "        # les logits (probabilit√©s pr√©dites des tokens)\n",
        "        logits = nn.log_softmax(self.pred_layer(X))\n",
        "\n",
        "        # Retourner les logits, et √©ventuellement, les poids d'attention\n",
        "        return logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sClFLLkU_RF_"
      },
      "source": [
        "Si tout est correct, alors si nous ex√©cutons le code ci-dessous, tout devrait fonctionner sans probl√®me.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82CWEa5m_RGA"
      },
      "outputs": [],
      "source": [
        "B, T, d_m, N, vocab_size = 18, 32, 16, 8, 25670\n",
        "\n",
        "llm = LLM(num_heads=1, num_layers=1, d_m=d_m, vocab_size=vocab_size, widening_factor=4)\n",
        "mask = jnp.tril(np.ones((T, T)))\n",
        "\n",
        "# initialise le module et obtient une sortie factice\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.randint(key, [B, T], 0, vocab_size)\n",
        "params = llm.init(key, X, mask=mask)\n",
        "\n",
        "# extrait la sortie du d√©codeur\n",
        "logits, decoder_att_weights = llm.apply(\n",
        "    params,\n",
        "    X,\n",
        "    mask=mask,\n",
        "    return_att_weights=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gve7ssD__RGA"
      },
      "source": [
        "Comme derni√®re v√©rification de coh√©rence, nous pouvons confirmer que nos poids d'attention fonctionnent correctement. Comme le montre la figure ci-dessous, les poids d'attention du d√©codeur ne se concentrent que sur les jetons pr√©c√©dents, comme pr√©vu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4NpywYv_RGA"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "plt.suptitle(\"Poids d'attention du LLM\")\n",
        "sns.heatmap(decoder_att_weights[0, 0, 0, ...], ax=ax, cmap=\"Blues\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt3tp38G90A"
      },
      "source": [
        "### 2.5 Entra√Ænement de votre LLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agLIpsoh_RGA"
      },
      "source": [
        "#### 2.5.1 Objectif d'entra√Ænement <font color='green'>Interm√©diaire</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOSv1-3B_RGA"
      },
      "source": [
        "#### 2.5.1 Objectif d'entra√Ænement <font color='green'>Interm√©diaire</font>\n",
        "\n",
        "Une phrase n'est rien d'autre qu'une cha√Æne de mots. Un LLM vise √† pr√©dire le mot suivant en tenant compte du contexte actuel, c'est-√†-dire des mots qui l'ont pr√©c√©d√©.\n",
        "\n",
        "Voici l'id√©e de base :\n",
        "\n",
        "Pour calculer la probabilit√© d'une phrase compl√®te \"mot1, mot2, ..., dernier mot\" apparaissant dans un contexte donn√© $c$, la proc√©dure consiste √† d√©composer la phrase en mots individuels et √† consid√©rer la probabilit√© de chaque mot √©tant donn√© les mots qui le pr√©c√®dent. Ces probabilit√©s individuelles sont ensuite multipli√©es ensemble :\n",
        "\n",
        "$$\\text{Probabilit√© de la phrase} = \\text{Probabilit√© de mot1} \\times \\text{Probabilit√© de mot2} \\times \\ldots \\times \\text{Probabilit√© du dernier mot}$$\n",
        "\n",
        "Cette m√©thode est semblable √† la construction d'une narration pi√®ce par pi√®ce en fonction de l'histoire pr√©c√©dente.\n",
        "\n",
        "Math√©matiquement, cela s'exprime comme la vraisemblance (probabilit√©) d'une s√©quence de mots $y_1, y_2, ..., y_n$ dans un contexte donn√© $c$, ce qui est r√©alis√© en multipliant les probabilit√©s de chaque mot $y_t$ calcul√©es √©tant donn√© les pr√©d√©cesseurs ($y_{<t}$) et le contexte $c$ :\n",
        "\n",
        "$$\n",
        "P\\left(y_{1}, y_{2}, \\ldots, y_{n}, \\mid c\\right)=\\prod_{t=1}^{n} P\\left(y_{t} \\mid y_{<t}, c\\right)\n",
        "$$\n",
        "\n",
        "Ici $y_{<t}$ repr√©sente la s√©quence $y_1, y_2, ..., y_{t-1}$, tandis que $c$ repr√©sente le contexte.\n",
        "\n",
        "Cela est analogue √† r√©soudre un puzzle o√π la pi√®ce suivante est plac√©e pr√©visiblement en fonction de ce qui est d√©j√† en place.\n",
        "\n",
        "Rappelez-vous que lors de l'entra√Ænement d'un transformateur, nous ne travaillons pas avec des mots, mais avec des tokens. Pendant le processus d'entra√Ænement, les param√®tres du mod√®le sont affin√©s en calculant la perte de l'entropie crois√©e entre le token pr√©dit et le token correct, puis en effectuant une r√©tropropagation. La perte pour l'√©tape temporelle \"t\" est calcul√©e comme suit :\n",
        "\n",
        "$$ \\text{Perte}_t = - \\sum_{w \\in V} y_t\\log (\\hat{y}_t) $$\n",
        "\n",
        "Ici $y_t$ est le token r√©el √† l'√©tape temporelle $t$, et $\\hat{y}_t$ est le token pr√©dit par le mod√®le √† la m√™me √©tape temporelle. La perte pour l'ensemble de la phrase est ensuite calcul√©e comme suit :\n",
        "\n",
        "$$ \\text{Perte de la phrase} = \\frac{1}{n} \\sum^{n}_{t=1} \\text{Perte}_t $$\n",
        "\n",
        "o√π $n$ est la longueur de la s√©quence.\n",
        "\n",
        "Ce processus it√©ratif affine finalement les capacit√©s pr√©dictives du mod√®le au fil du temps.\n",
        "\n",
        "**T√¢che de code** : Impl√©mentez la fonction de perte d'entropie crois√©e ci-dessous.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXmjUYdDHseM"
      },
      "outputs": [],
      "source": [
        "def sequence_loss_fn(logits, targets):\n",
        "  '''\n",
        "  Calculer la perte d'entropie crois√©e entre l'ID de token pr√©dit et l'ID r√©el.\n",
        "\n",
        "  Args:\n",
        "    logits: Un tableau de forme [batch_size, sequence_length, vocab_size]\n",
        "    targets: Les IDs de token r√©els que nous essayons de pr√©dire, forme [batch_size, sequence_length]\n",
        "\n",
        "  Returns:\n",
        "    loss: Une valeur scalaire repr√©sentant la perte moyenne du lot\n",
        "  '''\n",
        "\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "\n",
        "  mask = jnp.greater(targets, 0)\n",
        "  loss = ... # FINIR MOI\n",
        "\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cq5_4WN_RGA"
      },
      "outputs": [],
      "source": [
        "# @title Ex√©cutez-moi pour tester votre code\n",
        "VOCAB_SIZE = 25670\n",
        "targets = jnp.array([[0, 2, 0]])\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
        "loss = sequence_loss_fn(X, targets)\n",
        "real_loss = jnp.array(10.966118)\n",
        "assert jnp.allclose(real_loss, loss), \"La valeur retourn√©e n'est pas correcte\"\n",
        "print(\"Il semble correct. Consultez la r√©ponse ci-dessous pour comparer les m√©thodes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cthfcbmC_RGA"
      },
      "outputs": [],
      "source": [
        "# @title R√©ponse √† la t√¢che de code (Essayez de ne pas regarder avant d'avoir bien essay√© !)\n",
        "def sequence_loss_fn(logits, targets):\n",
        "    \"\"\"Calculer la perte de s√©quence entre les logits pr√©dits et les √©tiquettes cibles.\"\"\"\n",
        "\n",
        "    # Convertir les indices cibles en vecteurs encod√©s en one-hot.\n",
        "    # Chaque √©tiquette cible est convertie en un vecteur one-hot de taille VOCAB_SIZE.\n",
        "    target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "\n",
        "    # Assurer que la forme des logits correspond √† la forme des cibles encod√©es en one-hot.\n",
        "    # C'est important car nous devons calculer la perte sur les dimensions correspondantes.\n",
        "    assert logits.shape == target_labels.shape\n",
        "\n",
        "    # Cr√©er un masque qui ignore les jetons de padding dans le calcul de la perte.\n",
        "    # Le masque est True (1) lorsque la valeur cible est sup√©rieure √† 0 et False (0) sinon.\n",
        "    mask = jnp.greater(targets, 0)\n",
        "\n",
        "    # Calculer la perte d'entropie crois√©e pour chaque jeton.\n",
        "    # L'entropie crois√©e est calcul√©e comme le logarithme n√©gatif de la probabilit√© de la classe correcte.\n",
        "    # jax.nn.log_softmax(logits) nous donne les probabilit√©s logarithmiques pour chaque classe.\n",
        "    # Nous multiplions par les target_labels pour s√©lectionner la probabilit√© logarithmique de la classe correcte.\n",
        "    loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "\n",
        "    # Appliquer le masque √† la perte pour ignorer les positions de padding et additionner les pertes.\n",
        "    # Nous normalisons ensuite la perte totale par le nombre de jetons non-padding.\n",
        "    loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CSfvGj__RGA"
      },
      "source": [
        "#### 2.5.2 Entra√Ænement des mod√®les <font color='blue'>Avanc√©</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIQ_aJGW_RGA"
      },
      "source": [
        "Dans la section suivante, nous d√©finissons tous les processus n√©cessaires pour entra√Æner le mod√®le en utilisant l'objectif d√©crit ci-dessus. Une grande partie de cela concerne maintenant le travail requis pour effectuer l'entra√Ænement avec FLAX.\n",
        "\n",
        "Ci-dessous, nous rassemblons le jeu de donn√©es sur lequel nous allons entra√Æner, qui est le jeu de donn√©es de Shakespeare de Karpathy. Il n'est pas si important de comprendre ce code, donc soit ex√©cutez simplement la cellule pour charger les donn√©es, soit consultez le code si vous souhaitez le comprendre.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guMHAaSo_RGB"
      },
      "outputs": [],
      "source": [
        "# @title Cr√©er le jeu de donn√©es Shakespeare et l'it√©rateur (optionnel, mais ex√©cutez la cellule)\n",
        "\n",
        "# Astuce pour √©viter les erreurs lors du t√©l√©chargement de tinyshakespeare.\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "class WordBasedAsciiDatasetForLLM:\n",
        "    \"\"\"Jeu de donn√©es en m√©moire d'un fichier ASCII unique pour un mod√®le de type langage.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Charger un fichier ASCII unique en m√©moire.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokeniser en s√©parant le texte en mots\n",
        "        words = corpus.split()\n",
        "        self.vocab_size = len(set(words))  # Nombre de mots uniques\n",
        "\n",
        "        # Cr√©er un mapping de mots vers des IDs uniques\n",
        "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
        "\n",
        "        # Stocker le mapping inverse des IDs vers les mots\n",
        "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
        "\n",
        "        # Convertir les mots du corpus en leurs IDs correspondants\n",
        "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Seulement {num_batches} lots ; envisagez une s√©quence plus courte \"\n",
        "                \"ou un lot plus petit.\"\n",
        "            )\n",
        "\n",
        "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"G√©n√©rer le prochain mini-lot.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Cr√©er les paires observation/cible pour la mod√©lisation du langage.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        \"\"\"Convertir une s√©quence d'IDs de mots en mots.\"\"\"\n",
        "        return [self.id_to_word[id] for id in ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"R√©p√©ter et m√©langer infiniment les donn√©es de l'it√©rable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclus.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBIFg51oQl0"
      },
      "source": [
        "Lets now look how our data is structured for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvH3XPM5_RGB"
      },
      "outputs": [],
      "source": [
        "# √âchantillonner et examiner les donn√©es\n",
        "batch_size = 2\n",
        "seq_length = 32\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Entr√©e\", \"-\" * 11)\n",
        "    print(\"TEXTE :\", ' '.join(train_dataset.ids_to_words(obs)))\n",
        "    print(\"ASCII :\", obs)\n",
        "    print(\"-\" * 10, \"Cible\", \"-\" * 10)\n",
        "    print(\"TEXTE :\", ' '.join(train_dataset.ids_to_words(target)))\n",
        "    print(\"ASCII :\", target)\n",
        "\n",
        "print(f\"\\n Taille totale du vocabulaire : {train_dataset.vocab_size}\")\n",
        "\n",
        "VOCAB_SIZE = train_dataset.vocab_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9vzee53_RGB"
      },
      "source": [
        "Ensuite, entra√Ænons notre LLM et voyons comment il se comporte pour produire du texte shakespearien. Tout d'abord, nous allons d√©finir ce qui se passe √† chaque √©tape d'entra√Ænement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGuYBCkekgDw"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(3, 4))\n",
        "def train_step(params, optimizer_state, batch, apply_fn, update_fn):\n",
        "    \"\"\"\n",
        "    Effectuer une √©tape d'entra√Ænement.\n",
        "\n",
        "    Args:\n",
        "        params: Les param√®tres actuels du mod√®le.\n",
        "        optimizer_state: L'√©tat actuel de l'optimiseur.\n",
        "        batch: Un dictionnaire contenant les donn√©es d'entr√©e et les √©tiquettes cibles pour le batch.\n",
        "        apply_fn: La fonction utilis√©e pour appliquer le mod√®le aux entr√©es.\n",
        "        update_fn: La fonction utilis√©e pour mettre √† jour les param√®tres du mod√®le en fonction des gradients.\n",
        "\n",
        "    Returns:\n",
        "        Param√®tres mis √† jour, √©tat de l'optimiseur mis √† jour, et la perte calcul√©e pour le batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # Obtenez la longueur de la s√©quence (T) √† partir des donn√©es d'entr√©e.\n",
        "        T = batch['input'].shape[1]\n",
        "\n",
        "        # Appliquez le mod√®le aux donn√©es d'entr√©e, en utilisant un masque triangulaire inf√©rieur pour imposer la causalit√©.\n",
        "        # jnp.tril(np.ones((T, T))) cr√©e une matrice triangulaire inf√©rieure de uns.\n",
        "        logits = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
        "\n",
        "        # Calculez la perte entre les logits pr√©dits et les √©tiquettes cibles.\n",
        "        loss = sequence_loss_fn(logits, batch['target'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Calculez la perte et ses gradients par rapport aux param√®tres.\n",
        "    loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
        "\n",
        "    # Mettez √† jour l'√©tat de l'optimiseur et calculez les mises √† jour des param√®tres en fonction des gradients.\n",
        "    updates, optimizer_state = update_fn(gradients, optimizer_state)\n",
        "\n",
        "    # Appliquez les mises √† jour aux param√®tres.\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    # Retournez les param√®tres mis √† jour, l'√©tat de l'optimiseur, et la perte pour le batch.\n",
        "    return params, optimizer_state, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtKWzKIAkfYU"
      },
      "source": [
        "Nous allons maintenant initialiser notre optimiseur et notre mod√®le. N'h√©sitez pas √† exp√©rimenter avec les hyperparam√®tres pendant la pratique.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o3q-BZX_RGB"
      },
      "outputs": [],
      "source": [
        "# D√©finir tous les hyperparam√®tres\n",
        "d_model = 128            # Dimension des embeddings de tokens (d_m)\n",
        "num_heads = 4            # Nombre de t√™tes d'attention dans Multi-Head Attention\n",
        "num_layers = 1           # Nombre de blocs d√©codeurs dans le mod√®le\n",
        "widening_factor = 2      # Facteur d'√©largissement de la taille de la couche cach√©e dans le MLP\n",
        "LR = 2e-3                # Taux d'apprentissage pour l'optimiseur\n",
        "batch_size = 32          # Nombre d'√©chantillons par lot d'entra√Ænement\n",
        "seq_length = 64          # Longueur de chaque s√©quence d'entr√©e (nombre de tokens)\n",
        "\n",
        "# Pr√©parer les donn√©es d'entra√Ænement\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size  # Obtenir la taille du vocabulaire √† partir du dataset\n",
        "batch = next(train_dataset)            # Obtenir le premier lot de donn√©es d'entr√©e\n",
        "\n",
        "# D√©finir la cl√© du g√©n√©rateur de nombres al√©atoires pour l'initialisation du mod√®le\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "# Initialiser le mod√®le LLM avec les hyperparam√®tres sp√©cifi√©s\n",
        "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
        "\n",
        "# Cr√©er un masque causal pour s'assurer que le mod√®le ne se concentre que sur les tokens pr√©c√©dents\n",
        "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "\n",
        "# Initialiser les param√®tres du mod√®le en utilisant le premier lot de donn√©es d'entr√©e et le masque\n",
        "params = llm.init(rng, batch['input'], mask)\n",
        "\n",
        "# Configurer l'optimiseur en utilisant l'algorithme d'optimisation Adam avec le taux d'apprentissage sp√©cifi√©\n",
        "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)  # Initialiser l'√©tat de l'optimiseur avec les param√®tres du mod√®le"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bPEFakxmvsM"
      },
      "source": [
        "Now we train! This will take a few minutes..\n",
        "While it trains, have you greeted your neighbor yet?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUAS6tie_RGB"
      },
      "outputs": [],
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 3500\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "VOCAB_SIZE = 25670\n",
        "\n",
        "# Boucle d'entra√Ænement\n",
        "for step in range(MAX_STEPS):\n",
        "    batch = next(train_dataset)\n",
        "    params, optimizer_state, loss = train_step(\n",
        "        params, optimizer_state, batch, llm.apply, optimizer.update)\n",
        "    losses.append(loss)\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGv9c2AFmF4V"
      },
      "source": [
        "#### 2.5.3 Inspecter le LLM entra√Æn√© <font color='orange'>D√©butant</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfq61gim_RGB"
      },
      "source": [
        "**Rappel :** n'oubliez pas d'ex√©cuter tout le code pr√©sent√© jusqu'√† pr√©sent dans cette section avant de lancer les cellules ci-dessous !\n",
        "\n",
        "G√©n√©rons maintenant un peu de texte et voyons comment notre mod√®le a perform√©. NE STOPPEZ PAS LA CELLULE UNE FOIS QU'ELLE EST EN COURS D'EX√âCUTION, CELA FERA PLANTER LA SESSION.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lt8HTS__RGC"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(2, ))\n",
        "def generate_prediction(params, input, apply_fn):\n",
        "  logits = apply_fn(params, input)\n",
        "  argmax_out = jnp.argmax(logits, axis=-1)\n",
        "  return argmax_out[0][-1].astype(int)\n",
        "\n",
        "def generate_random_shakespeare(llm, params, id_2_word, word_2_id):\n",
        "    '''\n",
        "    Get the model output\n",
        "    '''\n",
        "\n",
        "    prompt = \"Love\"\n",
        "    print(prompt, end=\"\")\n",
        "    tokens = prompt.split()\n",
        "\n",
        "    # predict and append\n",
        "    for i in range(15):\n",
        "      input = jnp.array([[word_2_id[t] for t in tokens]]).astype(int)\n",
        "      prediction = generate_prediction(params, input, llm.apply)\n",
        "      prediction = id_2_word[int(prediction)]\n",
        "      tokens.append(prediction)\n",
        "      print(\" \"+prediction, end=\"\")\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "id_2_word = train_dataset.id_to_word\n",
        "word_2_id = train_dataset.word_to_id\n",
        "\n",
        "generated_shakespeare = generate_random_shakespeare(llm, params, id_2_word, word_2_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwNuMRf_RGC"
      },
      "source": [
        "Enfin, nous avons impl√©ment√© tout ce qui pr√©c√®de en prenant l'ID de jeton avec la probabilit√© maximale d'√™tre correct. C'est ce qu'on appelle le d√©codage gourmand, car nous avons uniquement pris le jeton le plus probable. Cela a bien fonctionn√© dans ce cas, mais il y a des situations o√π cette approche gourmande peut d√©grader les performances, notamment lorsque nous souhaitons g√©n√©rer un texte r√©aliste.\n",
        "\n",
        "Il existe d'autres m√©thodes pour √©chantillonner √† partir du d√©codeur, avec un algorithme c√©l√®bre √©tant la recherche par faisceau (beam search). Nous fournissons ci-dessous des ressources pour ceux qui souhaitent en savoir plus √† ce sujet.\n",
        "\n",
        "[D√©codage Gourmand](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
        "\n",
        "[Recherche par Faisceau](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## **Conclusion**\n",
        "**R√©sum√© :**\n",
        "\n",
        "Vous avez maintenant ma√Ætris√© l'essentiel du fonctionnement d'un Large Language Model (LLM), depuis les m√©canismes d'attention fondamentaux jusqu'√† l'entra√Ænement de votre propre LLM ! Ces outils puissants ont le potentiel de transformer un large √©ventail de t√¢ches. Cependant, comme tout mod√®le de deep learning, leur efficacit√© r√©side dans leur application aux bons probl√®mes avec les bonnes donn√©es.\n",
        "\n",
        "Pr√™t √† passer au niveau sup√©rieur ? Plongez dans le fine-tuning de vos propres LLMs et lib√©rez encore plus de potentiel ! Je vous recommande vivement d'explorer le tutoriel de l'ann√©e derni√®re sur les m√©thodes de fine-tuning efficaces pour obtenir une vue d'ensemble des techniques avanc√©es. Le voyage ne s'arr√™te pas l√†‚Äîil y a encore tant √† d√©couvrir ! [LLMs pour Tous 2023](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
        "\n",
        "Le monde des LLMs est √† vous‚Äîallez cr√©er quelque chose d'incroyable ! üåüüöÄ\n",
        "\n",
        "**Prochaines √©tapes :**\n",
        "\n",
        "[**Fine-tuning Efficace des LLMs avec Hugging Face**](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
        "\n",
        "**R√©f√©rences :** pour des r√©f√©rences suppl√©mentaires, consultez les liens mentionn√©s dans les sections sp√©cifiques de ce colab.\n",
        "\n",
        "* [Article \"Attention is all you need\"](https://arxiv.org/abs/1706.03762)\n",
        "* [Vid√©os suppl√©mentaires sur les transformers](https://www.youtube.com/playlist?list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s)\n",
        "* [Article LoRA](https://arxiv.org/abs/2106.09685)\n",
        "* [RLHF](https://huggingface.co/blog/rlhf) (comment ChatGPT a √©t√© entra√Æn√©)\n",
        "* [Extension de la longueur du contexte](https://kaiokendev.github.io/context)\n",
        "\n",
        "Pour d'autres pratiques du Deep Learning Indaba, veuillez visiter [ici](https://github.com/deep-learning-indaba/indaba-pracs-2023).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# Retours - Avis - Suggestions\n",
        "\n",
        "Veuillez fournir des commentaires que nous pourrons utiliser pour am√©liorer nos pratiques √† l'avenir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
