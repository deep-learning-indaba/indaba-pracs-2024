{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# LLMs for everyone\n",
        "\n",
        "<img src=\"https://www.marktechpost.com/wp-content/uploads/2023/05/Blog-Banner-3.jpg\" width=\"60%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2024/blob/main/practicals/Foundations_of_LLMs/foundations_of_llms_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "¬© Deep Learning Indaba 2024. Apache License 2.0.\n",
        "\n",
        "**Authors: Jabez Magomere, Harry Mayne, Khalil Mrini, Nabra Rizvi, Doudou Ba**\n",
        "\n",
        "**Reviewers: Seid Muhie Yimam, Foutse Yuehgoh**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Welcome to **\"LLMs for Everyone\"**‚Äîyour gateway to the fascinating world of Large Language Models (LLMs)! To kick things off, here‚Äôs a fun fact: this entire introduction was generated by ChatGPT, one of the many powerful LLMs you'll be learning about. ü§ñ‚ú®\n",
        "\n",
        "In this tutorial, you'll dive into the core principles of transformers, the cutting-edge technology behind models like GPT. You‚Äôll also get hands-on experience training your very own Language Model! Get ready to explore how these impressive AI systems create such realistic and engaging text. Let‚Äôs embark on this exciting journey together and unlock the secrets of LLMs! üöÄüìö\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [<font color='orange'>Hugging Face Introduction</font>, <font color='green'>Attention Mechanism</font>, <font color='green'>Transformer Architecture</font>, <font color='green'>Training your own LLM from scratch</font>, <font color='orange'>Finetuning an LLM for Text Classification</font>]\n",
        "\n",
        "Level: <font color='orange'>Beginner</font>, <font color='green'>Intermediate</font>, <font color='blue'>Advanced</font>\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "* Understand the idea behind [Attention](https://arxiv.org/abs/1706.03762) and why it is used.\n",
        "* Present and describe the fundamental building blocks of the [Transformer Architecture](https://arxiv.org/abs/1706.03762) along with an intuition on such an architecture design.\n",
        "* Build and train a simple Shakespeare-inspired LLM.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "* Basic knowledge of Deep Learning.\n",
        "* Familiarity with Natural Language Processing (NLP).\n",
        "* Understanding of sequence-to-sequence models.\n",
        "* Basic understanding of Linear Algebra.\n",
        "\n",
        "**Outline:**\n",
        "\n",
        ">[LLMs for everyone](#scrollTo=m2s4kN_QPQVe)\n",
        "\n",
        ">>[Installations, Imports and Helper Functions](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">>[Let's kick things off with a Hugging Face Demo! Beginner](#scrollTo=4zu5cg-YG4XU)\n",
        "\n",
        ">>>[Hugging Face](#scrollTo=AwjIIipOG4fz)\n",
        "\n",
        ">>>[Time for a Demo! ‚è∞‚ö° Loading a Hugging Face Model and Running a Sample](#scrollTo=eq46TV_0G4f0)\n",
        "\n",
        ">[LLMs for everyone](#scrollTo=m2s4kN_QPQVe)\n",
        "\n",
        ">>[Installations, Imports and Helper Functions](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">>[Let's kick things off with a Hugging Face Demo! Beginner](#scrollTo=4zu5cg-YG4XU)\n",
        "\n",
        ">>>[Hugging Face](#scrollTo=AwjIIipOG4fz)\n",
        "\n",
        ">>>[Time for a Demo! ‚è∞‚ö° Loading a Hugging Face Model and Running a Sample](#scrollTo=eq46TV_0G4f0)\n",
        "\n",
        ">>[1. Attention](#scrollTo=-ZUp8i37dFbU)\n",
        "\n",
        ">>>[Intuition - Beginner](#scrollTo=ygdi884ugGcu)\n",
        "\n",
        ">>>[Understanding Attention in Simple Terms](#scrollTo=ygdi884ugGcu)\n",
        "\n",
        ">>>[Sequence to sequence attenion mechanisms - Intermediate](#scrollTo=aQfqM1EJyDXI)\n",
        "\n",
        ">>>[Self-attention to Multihead Attention - Intermediate](#scrollTo=J-MU6rrny8Nj)\n",
        "\n",
        ">>>>[Self-attention](#scrollTo=0AFUEFZGzCTv)\n",
        "\n",
        ">>>>>[Queries, keys and values](#scrollTo=pwOIMtdZzdTf)\n",
        "\n",
        ">>>>>[Scaled dot product attention](#scrollTo=OhGZHFsHz_Qp)\n",
        "\n",
        ">>>>>[Masked attention](#scrollTo=D7B-AgO80gIt)\n",
        "\n",
        ">>>>>[Multi-head attention](#scrollTo=OWDubQwCs4zG)\n",
        "\n",
        ">>[2. Building your own LLM](#scrollTo=e9NW58_3hAg2)\n",
        "\n",
        ">>>[2.1 High-level overvierw Beginner](#scrollTo=bA_2coZvhAg3)\n",
        "\n",
        ">>>[2.2 Tokenization + Positional encoding Beginner](#scrollTo=fbTsk0MdhAhC)\n",
        "\n",
        ">>>>[2.2.1 Tokenization](#scrollTo=DehUpfym_RF8)\n",
        "\n",
        ">>>>[2.2.2 Positional encodings](#scrollTo=639s7Zuk_RF9)\n",
        "\n",
        ">>>>>[Sine and cosine functions](#scrollTo=rklY-aL-_RF9)\n",
        "\n",
        ">>>[Group Activity:](#scrollTo=1mjHEDPO_RF-)\n",
        "\n",
        ">>>[2.3 Transformer block   Intermediate](#scrollTo=SdNPg0pnhAhG)\n",
        "\n",
        ">>>>[2.3.1 Feed Forward Network (FFN) / Multilayer perceptron (MLP) Beginner](#scrollTo=kTURbfr__RF-)\n",
        "\n",
        ">>>>[2.3.2 Add and Norm block Beginner](#scrollTo=Sts5Vr4i_RF-)\n",
        "\n",
        ">>>[2.4 Building the Transformer Decoder / LLM Intermediate](#scrollTo=91dXd29b_RF_)\n",
        "\n",
        ">>>[2.5 Training your LLM](#scrollTo=wmt3tp38G90A)\n",
        "\n",
        ">>>>[2.5.1 Training objective Intermediate](#scrollTo=agLIpsoh_RGA)\n",
        "\n",
        ">>>>[2.5.2 Training models Advanced](#scrollTo=4CSfvGj__RGA)\n",
        "\n",
        ">>>>[2.5.3 Inspecting the trained LLM Beginner](#scrollTo=pGv9c2AFmF4V)\n",
        "\n",
        ">>[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        ">[Feedback](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952qogb79nnY"
      },
      "source": [
        "**Suggested experience level in this topic:**\n",
        "\n",
        "| Level         | Experience                            |\n",
        "| --- | --- |\n",
        "`Beginner`      | It is my first time being introduced to this work. |\n",
        "`Intermediate`  | I have done some basic courses/intros on this topic. |\n",
        "`Advanced`      | I work in this area/topic daily. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YBdDHcI_ArCR"
      },
      "outputs": [],
      "source": [
        "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
        "experience = \"beginner\" #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
        "sections_to_follow=\"\"\n",
        "\n",
        "\n",
        "if experience == \"beginner\": sections_to_follow = \"\"\"we recommend you to not attempt to do every coding task but instead, skip through to every section and ensure you interact with the LoRA finetuned LLM presented in the last section as well as with the pretrained LLM to get a practical understanding of how these models behave\"\"\"\n",
        "\n",
        "elif experience == \"intermediate\": sections_to_follow = \"\"\"we recommend you go through every section in this notebook and try the coding tasks tagged as beginner or intermediate. If you get stuck on the code ask a tutor for help or move on to better use the time of the practical\"\"\"\n",
        "\n",
        "elif experience == \"advanced\": sections_to_follow = \"\"\"we recommend you go through every section and try every coding task until you get it to work\"\"\"\n",
        "\n",
        "\n",
        "print(f\"Based on your experience, {sections_to_follow}.\\nNote: this is just a guideline, feel free to explore the colab as you'd like if you feel comfort able!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installations, Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries for deep learning, NLP, and plotting\n",
        "!pip install transformers datasets  # Transformers and datasets libraries for NLP tasks\n",
        "!pip install seaborn umap-learn     # Seaborn for plotting, UMAP for dimensionality reduction\n",
        "!pip install livelossplot           # LiveLossPlot for tracking model training progress\n",
        "!pip install -q transformers[torch] # Transformers with PyTorch backend\n",
        "!pip install -q peft                # Parameter-Efficient Fine-Tuning library\n",
        "!pip install accelerate -U          # Accelerate library for performance\n",
        "\n",
        "# Install utilities for debugging and console output formatting\n",
        "!pip install -q ipdb                # Interactive Python Debugger\n",
        "!pip install -q colorama            # Colored terminal text output\n",
        "\n",
        "# Import system and math utilities\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# Check for connected accelerators (GPU or TPU) and set up accordingly\n",
        "if os.environ.get(\"COLAB_GPU\") and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"A GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "# Avoid GPU memory allocation to be done by JAX\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
        "\n",
        "# Import libraries for JAX-based deep learning\n",
        "import chex\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import optax\n",
        "\n",
        "# Import NLP and model-related libraries\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "# Import image processing and plotting libraries\n",
        "from PIL import Image\n",
        "from livelossplot import PlotLosses\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Import additional utilities for working with text and models\n",
        "import torch\n",
        "import torchvision\n",
        "import itertools\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# Download an example image to use in the notebook\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
        "    \"cat.png\",\n",
        ")\n",
        "\n",
        "# Import libraries for NLP preprocessing and working with pre-trained models\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "nltk.download(\"word2vec_sample\")\n",
        "\n",
        "# Import Hugging Face tools and IPython widgets\n",
        "import huggingface_hub\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import colorama\n",
        "\n",
        "# Set Matplotlib to output SVG format for better quality plots\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "# @title Helper Plotting Functions. (Run Cell)\n",
        "\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "    \"\"\"\n",
        "    Plots the position encodings matrix.\n",
        "\n",
        "    Args:\n",
        "        P: Position encoding matrix (2D array).\n",
        "        max_tokens: Maximum number of tokens (rows) to plot.\n",
        "        d_model: Dimensionality of the model (columns) to plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the plot size based on the number of tokens and model dimensions\n",
        "    plt.figure(figsize=(20, np.min([8, max_tokens])))\n",
        "\n",
        "    # Plot the position encoding matrix with a color map for better visualization\n",
        "    im = plt.imshow(P, aspect=\"auto\", cmap=\"Blues_r\")\n",
        "\n",
        "    # Add a color bar to indicate the encoding values\n",
        "    plt.colorbar(im, cmap=\"blue\")\n",
        "\n",
        "    # Show embedding indices as ticks if the dimensionality is small\n",
        "    if d_model <= 64:\n",
        "        plt.xticks(range(d_model))\n",
        "\n",
        "    # Show position indices as ticks if the number of tokens is small\n",
        "    if max_tokens <= 32:\n",
        "        plt.yticks(range(max_tokens))\n",
        "\n",
        "    # Label the axes\n",
        "    plt.xlabel(\"Embedding index\")\n",
        "    plt.ylabel(\"Position index\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of patches and plots them.\n",
        "\n",
        "    Args:\n",
        "        patches: A list or array of image patches to plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the figure for plotting patches\n",
        "    fig = plt.figure(figsize=(25, 25))\n",
        "\n",
        "    # Create a subplot for each patch and display it\n",
        "    axes = []\n",
        "    for a in range(patches.shape[1]):\n",
        "        axes.append(fig.add_subplot(1, patches.shape[1], a + 1))\n",
        "        plt.imshow(patches[0][a])\n",
        "\n",
        "    # Adjust layout to prevent overlap and display the plot\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "    \"\"\"\n",
        "    Projects high-dimensional embeddings onto 2D space and plots them.\n",
        "\n",
        "    Args:\n",
        "        embeddings: High-dimensional embedding vectors to project.\n",
        "        labels: Labels corresponding to each embedding for coloring in the plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import UMAP and Seaborn for dimensionality reduction and plotting\n",
        "    import umap\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Reduce the dimensionality of the embeddings to 2D using UMAP\n",
        "    projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "    # Plot the 2D projections with labels using Seaborn for better aesthetics\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.title(\"Projected text embeddings\")\n",
        "    sns.scatterplot(\n",
        "        x=projected_embeddings[:, 0], y=projected_embeddings[:, 1], hue=labels\n",
        "    )\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "    \"\"\"\n",
        "    Plots an attention weight matrix with custom axis ticks.\n",
        "\n",
        "    Args:\n",
        "        weight_matrix: The attention weight matrix to plot.\n",
        "        x_ticks: Labels for the x-axis (typically the query tokens).\n",
        "        y_ticks: Labels for the y-axis (typically the key tokens).\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the plot size\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot the attention weight matrix as a heatmap\n",
        "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
        "\n",
        "    # Set custom ticks on the x and y axes\n",
        "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
        "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
        "\n",
        "    # Label the plot\n",
        "    plt.title(\"Attention matrix\")\n",
        "    plt.xlabel(\"Attention score\")\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMkaKekB_pR4"
      },
      "outputs": [],
      "source": [
        "# @title Helper Text Processing Functions. (Run Cell)\n",
        "\n",
        "def get_word2vec_embedding(words):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of words and returns a list of their embeddings,\n",
        "    based on a pretrained word2vec encoder.\n",
        "    \"\"\"\n",
        "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        word2vec_sample, binary=False\n",
        "    )\n",
        "\n",
        "    output = []\n",
        "    words_pass = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            output.append(jnp.array(model.word_vec(word)))\n",
        "            words_pass.append(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    embeddings = jnp.array(output)\n",
        "    del model  # free up space again\n",
        "    return embeddings, words_pass\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Function that takes in a string and removes all punctuation.\"\"\"\n",
        "    import re\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def print_sample(prompt: str, sample: str):\n",
        "  \"\"\"Function that takes in a prompt instruction and model response and\n",
        "  prints them out in different colors to show a distinction\"\"\"\n",
        "  print(colorama.Fore.MAGENTA + prompt, end=\"\")\n",
        "  print(colorama.Fore.BLUE + sample)\n",
        "  print(colorama.Fore.RESET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zu5cg-YG4XU"
      },
      "source": [
        "## Let's kick things off with a Hugging Face Demo! <font color='orange'>Beginner</font>\n",
        "\n",
        "We're thrilled to have you on board! üéâ Before we dive into the hands-on part of our journey, let's take a quick detour into the fascinating world of [Hugging Face](https://huggingface.co/)‚Äîan incredible open-source platform for building and deploying cutting-edge language models. üåê\n",
        "\n",
        "As a sneak peek into what we'll be creating today, we'll start by loading a *small* large language model (*in comparison to today's models) and prompting it with a simple instruction. This will give you a feel for how to interact with these powerful libraries. üí° Get ready to unlock the potential of language models with just a few lines of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwjIIipOG4fz"
      },
      "source": [
        "### Hugging Face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2DSHiuhG4f0"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png\" width=\"10%\">\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is a startup founded in 2016 and, in their own words: \"are on a mission to democratize good machine learning, one commit at a time.\" Currently they are a treasure trove for tools to work on and with Large Language Model (LLMs).\n",
        "\n",
        "They have developed various open-source packages and allow users to easily interact with a large corpus of pretrained transformer models (across all modalities) and datasets to train or fine-tune pre-trained transformers. Their software is used widely in industry and research. For more details on them and usage, refer to [the 2022 attention and transformer practical](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/practicals/attention_and_transformers.ipynb#scrollTo=qFBw8kRx-4Mk).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xdt9PQ6G4f0"
      },
      "source": [
        "In this colab we print prompts in <font color='HotPink'><b>pink</b></font> and samples generated from a model in <font color='blue'><b>blue</b></font>  like in the example below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-8C9SJCG4f0"
      },
      "outputs": [],
      "source": [
        "print_sample(prompt='My fake prompt', sample=' is awesome!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq46TV_0G4f0"
      },
      "source": [
        "### Time for a Demo! ‚è∞‚ö° Loading a Hugging Face Model and Running a Sample\n",
        "\n",
        "Let's dive into how simple it is to load and interact with a model from Hugging Face!\n",
        "\n",
        "For this tutorial, we've pre-configured two model options:\n",
        "\n",
        "- **`gpt-neo-125M`**: A smaller model with 125 million parameters. It's faster and uses less memory‚Äîperfect for getting started! We recommend trying this one first.\n",
        "- **`gpt2-medium`**: A larger model with 355 million parameters for more advanced use.\n",
        "\n",
        "If you want to switch models, just restart the Colab kernel and update the model name in the cell below.\n",
        "\n",
        "**Note**: The steps we're about to show work not only for these models but also for [all models](https://huggingface.co/models?pipeline_tag=text-generation) on Hugging Face that support text generation pipelines.\n",
        "|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVV28V-TG4f1"
      },
      "outputs": [],
      "source": [
        "# Set the model name to \"EleutherAI/gpt-neo-125M\" (this can be changed via the dropdown options)\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"  # @param [\"gpt2-medium\", \"EleutherAI/gpt-neo-125M\"]\n",
        "\n",
        "# Define the prompt for the text generation model\n",
        "test_prompt = 'What is love?'  # @param {type: \"string\"}\n",
        "\n",
        "# Create a text generation pipeline using the specified model\n",
        "generator = transformers.pipeline('text-generation', model=model_name)\n",
        "\n",
        "# Generate text based on the provided prompt\n",
        "# 'do_sample=True' enables sampling to introduce randomness in generation, and 'min_length=30' ensures at least 30 tokens are generated\n",
        "model_output = generator(test_prompt, do_sample=True, min_length=30)\n",
        "\n",
        "# Print the generated text sample, removing the original prompt from the output\n",
        "print_sample(test_prompt, model_output[0]['generated_text'].split(test_prompt)[1].rstrip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5IEKl4iG4f1"
      },
      "source": [
        "**üí° Tip:** Try running the code above with different prompts or with the same prompt more than once!\n",
        "\n",
        "**ü§î Discussion:** Why do you think the generated text changes every time, even with the same prompt? Write your response in the input field below and discuss with your neighbour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQD5pIYCciI2"
      },
      "outputs": [],
      "source": [
        "# Define the prompt for the text generation model\n",
        "discussion_point = ''  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfV0Qk6yG4f1"
      },
      "source": [
        "Let's create our own `generator` function to make it easier to load different model weights and configure how text generation is done. Simply run the cells below to get started! üòÄ\n",
        "\n",
        "For now, don‚Äôt worry too much about understanding the details of the tokenizer. Just think of it as a step to convert the input into a format that the language model can understand. We‚Äôll dive deeper into tokenization later in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxs5bO_sG4f1"
      },
      "outputs": [],
      "source": [
        "# Check if the model name contains 'gpt2' and load the appropriate tokenizer and model\n",
        "if 'gpt2' in model_name:\n",
        "    # Load the GPT-2 tokenizer and model\n",
        "    tokenizer = transformers.GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = transformers.GPT2LMHeadModel.from_pretrained(model_name)\n",
        "# If the model name is 'EleutherAI/gpt-neo-125M', load the corresponding tokenizer and model\n",
        "elif model_name == \"EleutherAI/gpt-neo-125M\":\n",
        "    # Load the AutoTokenizer and AutoModel for the specified GPT-Neo model\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
        "# Raise an error if the model name is not supported\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "# If a GPU is available, move the model to the GPU for faster processing\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "# Set the padding token ID to be the same as the end-of-sequence token ID\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsZEwoZJG4f1"
      },
      "outputs": [],
      "source": [
        "def run_sample(\n",
        "    model,  # The language model we‚Äôll use to generate text\n",
        "    tokenizer,  # The tokenizer that converts text into a format the model understands\n",
        "    prompt: str,  # The text prompt we'll give to the model to start the text generation\n",
        "    seed: int | None = None,  # Optional: A number to make the results predictable each time\n",
        "    temperature: float = 0.6,  # Controls how random the model‚Äôs output is; lower values make it more focused\n",
        "    top_p: float = 0.9,  # Controls how much of the most likely words are considered; higher values consider more options\n",
        "    max_new_tokens: int = 64,  # The maximum number of words or tokens the model will add to the prompt\n",
        ") -> str:\n",
        "    # This function generates text based on a given prompt using a language model,\n",
        "    # with options to control randomness, the number of tokens generated, and reproducibility.\n",
        "\n",
        "    # Convert the prompt text into tokens that the model can process\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Extract the tokens (input IDs) and attention mask (to focus on important parts) from the inputs\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Move the tokens and attention mask to the same device as the model (like a GPU if available)\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    attention_mask = attention_mask.to(model.device)\n",
        "\n",
        "    # Set up how we want the model to generate text\n",
        "    generation_config = transformers.GenerationConfig(\n",
        "        do_sample=True,  # Allow the model to add some randomness to its text generation\n",
        "        temperature=temperature,  # Adjust how random the output is; lower means more focused\n",
        "        top_p=top_p,  # Consider the most likely words that make up the top 90% of possibilities\n",
        "        pad_token_id=tokenizer.pad_token_id,  # Use the token ID that represents padding (extra space)\n",
        "        top_k=0,  # We're not limiting to the top-k words, so we set this to 0\n",
        "    )\n",
        "\n",
        "    # If a seed is provided, set it so that the results are repeatable (same output each time)\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Generate text using the model with the settings we defined\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,  # Provide the input tokens to the model\n",
        "        attention_mask=attention_mask,  # Provide the attention mask to help the model focus\n",
        "        return_dict_in_generate=True,  # Ask the model to return detailed information\n",
        "        output_scores=True,  # Include the scores (confidence levels) for the generated tokens\n",
        "        max_new_tokens=max_new_tokens,  # Set the maximum number of tokens to generate\n",
        "        generation_config=generation_config,  # Apply our custom text generation settings\n",
        "    )\n",
        "\n",
        "    # Make sure only one sequence (output) is generated, to keep things simple\n",
        "    assert len(generation_output.sequences) == 1\n",
        "\n",
        "    # Get the generated sequence of tokens\n",
        "    output_sequence = generation_output.sequences[0]\n",
        "\n",
        "    # Convert the generated tokens back into readable text\n",
        "    output_string = tokenizer.decode(output_sequence)\n",
        "\n",
        "    # Print the prompt and the generated response\n",
        "    print_sample(prompt, output_string)\n",
        "\n",
        "    # Return the generated text response\n",
        "    return output_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yme6VzW4G4f1"
      },
      "outputs": [],
      "source": [
        "_ = run_sample(model, tokenizer, prompt=\"What is love?\", temperature = 0.5, seed=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vnUawyG4f1"
      },
      "source": [
        "Pretty amazing, right? ü§© Try playing around with the **prompt**, **temperature** and **seed** values above and see what different outputs you get. What do you notice when you increase the temperature? While this might have been mind-blowing back in 2021, by now, most of you have likely interacted with large language models in some way. Today, we're going to take things a step further by training our own **Shakespeare-inspired LLM**. This will give us a hands-on understanding of how these language models work under the hood.\n",
        "\n",
        "But before we jump into training, let‚Äôs first build a solid understanding of what **Large Language Models** are and the key **Machine Learning** concepts that make this groundbreaking technology possible. At the heart of today‚Äôs state-of-the-art (SoTA) LLMs are the **Attention Mechanism** and the **Transformer Architecture**. We‚Äôll explore these essential concepts in the upcoming sections of this tutorial. üöÄüí°\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **1. Attention**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acgW1ofF_RFz"
      },
      "source": [
        "The attention mechanism is inspired by how humans would look at an image or read a sentence.\n",
        "\n",
        "Let us take the image of the dog in human clothes below (image and example [source](https://lilianweng.github.io/posts/2018-06-24-attention/)). When paying *attention* to the red blocks of pixels, we will say that the yellow block of pointy ears is something we expected (correlated) but that the grey blocks of human clothes are unexpected for us (uncorrelated). This is *based on what we have seen in the past* when looking at pictures of dogs, specifically one of a Shiba Inu.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iEU7Cph2D2PCXp3YEHj30-EndhHAeB5T\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Assume we want to identify the dog breed in this image. When we look at the red blocks of pixels, we tend to pay more *attention* to relevant pixels that are more similar or relevant to them, which could be the ones in the yellow box. We almost completely ignore the snow in the background and the¬†human clothing for this task.\n",
        "\n",
        "Alternatively, when we begin looking at the background in an¬†attempt¬†to identify what is in it, we subconsciously ignore the dog pixels because they are irrelevant to the current task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usLBF2g0x5gH"
      },
      "source": [
        "The same thing happens when we read. In order to understand the entire sentence, we will learn to correlate and *attend to* certain words based on the context of the entire sentence.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j23kcfu_c3wINU6DUvxzMYNmp4alhHc9\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        " For instance, in the first sentence in the image above, when looking at the word \"coding\", we pay more attention to the word \"Apple\" and \"computer\" because we know that when we speak about coding, \"Apple\" is actually referring to the company. However, in the second sentence, we realise we should not consider \" apple \" when looking at \"code\" because given the context of the rest of the sentence, we know that this apple is referring to an actual apple and not a computer.\n",
        "\n",
        "We can build better models by developing mechanisms that mimic attention. It will enable our models to learn better representations of our input data by contextualising what it knows about some parts of the input based on other parts. In the following sections, we will explore the mechanisms that enable us to train deep learning models to attend to input data in the context of other input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygdi884ugGcu"
      },
      "source": [
        "### Intuition - <font color='orange'>Beginner</font>\n",
        "\n",
        "Imagine attention as a mechanism that allows a neural network to focus more on certain parts of data. By doing this, the network can enhance its grasp of the problem it's working on, updating its understanding or representations accordingly.\n",
        "\n",
        "### Understanding Attention in Simple Terms\n",
        "\n",
        "One way to implement attention in neural networks is by representing each word (or even parts of a word) as a vector.\n",
        "\n",
        "So, what‚Äôs a vector? A vector is simply an array of numbers (called real-valued numbers) that can have different lengths. Think of it like a list of values that describe certain properties of a word. These vectors allow us to measure how similar two words are to each other. One common way to measure this similarity is by calculating something called the **dot product**.\n",
        "\n",
        "The result of this similarity calculation is what we refer to as **attention.** This attention value helps the model decide how much one word should influence the representation of another word.\n",
        "\n",
        "In simpler terms, if two words have similar vector representations, it means they‚Äôre likely related or important to each other. Because of this relationship, they affect each other‚Äôs representations inside the neural network, allowing the model to understand the context better. üéØ\n",
        "\n",
        "To illustrate how the dot product can create meaningful attention weights, we'll use pre-trained [word2vec](https://jalammar.github.io/illustrated-word2vec/) embeddings. These word2vec embeddings are generated by a neural network that learned to create similar embeddings for words with similar meanings.\n",
        "\n",
        "By calculating the matrix of dot products between all vectors, we get an attention matrix. This will indicate which words are correlated and therefore should \"attend\" to each other.\n",
        "\n",
        "[1] You can find more details about how this is done for LLMs in the \"Building Your Own LLM\" session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvBYShCFk6WC"
      },
      "source": [
        "**Code task** <font color='blue'>Intermediate</font>: Complete the dot product attention function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrbITGPnk7Ce"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "    \"\"\"\n",
        "    Calculate the dot product between the hidden states and previous states.\n",
        "\n",
        "    Args:\n",
        "        hidden_states: A tensor with shape [T_hidden, dm]\n",
        "        previous_state: A tensor with shape [T_previous, dm]\n",
        "    \"\"\"\n",
        "\n",
        "    # Hint: To calculate the attention scores, think about how you can use the `previous_state` vector\n",
        "    # and the `hidden_states` matrix. You want to find out how much each element in `previous_state`\n",
        "    # should \"pay attention\" to each element in `hidden_states`. Remember that in matrix multiplication,\n",
        "    # you can find the relationship between two sets of vectors by multiplying one by the transpose of the other.\n",
        "    # Hint: Use `jnp.matmul` to perform the matrix multiplication between `previous_state` and the\n",
        "    # transpose of `hidden_states` (`hidden_states.T`).\n",
        "    scores = ...  # FINISH ME\n",
        "\n",
        "    # Hint: Now that you have the scores, you need to convert them into probabilities.\n",
        "    # A softmax function is typically used in attention mechanisms to turn raw scores into probabilities\n",
        "    # that sum to 1. This will help in determining how much focus should be placed on each hidden state.\n",
        "    # Hint: Use `jax.nn.softmax` to apply the softmax function to `scores`.\n",
        "    w_n = ...  # FINISH ME\n",
        "\n",
        "    # Multiply the weights by the hidden states to get the context vector\n",
        "    # Hint: Use `jnp.matmul` again to multiply the attention weights `w_n` by `hidden_states`\n",
        "    # to get the context vector.\n",
        "    c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "    return w_n, c_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QARgTrNZlIqH"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [2, 2])\n",
        "\n",
        "try:\n",
        "  w_n, c_t = dot_product_attention(x, x)\n",
        "\n",
        "  w_n_correct = jnp.array([[0.9567678, 0.04323225], [0.00121029, 0.99878967]])\n",
        "  c_t_correct = jnp.array([[0.11144122, 0.95290256], [-1.5571996, -1.5321486]])\n",
        "  assert jnp.allclose(w_n_correct, w_n), \"w_n is not calculated correctly\"\n",
        "  assert jnp.allclose(c_t_correct, c_t), \"c_t is not calculated correctly\"\n",
        "\n",
        "  print(\"It seems correct. Look at the answer below to compare methods.\")\n",
        "except:\n",
        "  print(\"It looks like the function isn't fully implemented yet. Try modifying it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa6PyKYnkzUJ"
      },
      "outputs": [],
      "source": [
        "# when changing these words, note that if the word is not in the original\n",
        "# training corpus it will not be shown in the weight matrix plot.\n",
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def dot_product_attention(hidden_states, previous_state):\n",
        "    # Calculate the attention scores:\n",
        "    # Multiply the previous state vector by the transpose of the hidden states matrix.\n",
        "    # This gives us a matrix of scores that show how much attention each element in the previous state\n",
        "    # should pay to each element in the hidden states.\n",
        "    # The result is a matrix of shape [T, N], where:\n",
        "    # T is the number of elements in the hidden states,\n",
        "    # N is the number of elements in the previous state.\n",
        "    scores = jnp.matmul(previous_state, hidden_states.T)\n",
        "\n",
        "    # Apply the softmax function to the scores to convert them into probabilities.\n",
        "    # This normalizes the scores so that they sum up to 1 for each element,\n",
        "    # allowing us to interpret them as how much attention should be given to each hidden state.\n",
        "    w_n = jax.nn.softmax(scores)\n",
        "\n",
        "    # Calculate the context vector (c_t):\n",
        "    # Multiply the attention weights (w_n) by the hidden states.\n",
        "    # This combines the hidden states based on how much attention each one deserves,\n",
        "    # resulting in a new vector that represents the weighted sum of the hidden states.\n",
        "    # The resulting shape is [T, d], where:\n",
        "    # T is the number of elements in the previous state,\n",
        "    # d is the dimension of the hidden states.\n",
        "    c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "    # Return the attention weights and the context vector.\n",
        "    return w_n, c_t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlHL3e_QhLfq"
      },
      "outputs": [],
      "source": [
        "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
        "word_embeddings, words = get_word2vec_embedding(words)\n",
        "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
        "plot_attention_weight_matrix(weights, words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tItZU09YlhEZ"
      },
      "source": [
        "Looking at the matrix,  we can see which words have similar meanings. The \"royal\" group of words have higher attention scores with each other than the \"food\" words, which all attend to one another. We also see that \"computers\" have very low attention scores for all of them, which shows that they are neither very related to \"royal\" or \"food\" words.  \n",
        "\n",
        "**Group task:**\n",
        "  - Play with the word selections above. See if you can find word combinations whose attention values seem counter-intuitive. Think of possible explanations. Which sense of a word did the attention scores capture?\n",
        "  - Ask your friend if they found examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3iB8hf0hJdX"
      },
      "source": [
        "**Note**: Dot product is only one of the ways to implement the scoring function for attention mechanisms, there is a more extensive list in this [blog](https://lilianweng.github.io/posts/2018-06-24-attention/#summary) post by Dr Lilian Weng.\n",
        "\n",
        "More resources:\n",
        "\n",
        "[A basic encoder-decoder model for machine translation](https://www.youtube.com/watch?v=gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=1)\n",
        "\n",
        "[Training and loss for encoder-decoder models](https://www.youtube.com/watch?v=aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=2)\n",
        "\n",
        "[Basic attention](https://www.youtube.com/watch?v=BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQfqM1EJyDXI"
      },
      "source": [
        "### Sequence to sequence attenion mechanisms - <font color='green'>Intermediate</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68QBeG-4yDZ9"
      },
      "source": [
        "The first attention mechanisms were used in sequence-to-sequence models. These models were usually RNN encoder and decoder structures. The input sequence was processed sequentially by an RNN, encoding the sequence in a single context vector, which is then fed into another RNN that generates a new sequence. Below is an example of this ([source](https://lilianweng.github.io/posts/2018-06-24-attention/)).\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FKfaArN1rsLjzVWaJGpMLEcxEshSLXd6\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "Since there is only one context vector, it is challenging to for the encoder to represent long sequences and information typically gets lost. The attention mechanism introduced in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) was proposed to solve this.\n",
        "\n",
        "Here, instead of relying on one static context vector, which is also only used once in the decoding process, let us provide information on the entire input sequence at every decoding step using a dynamic context vector. By doing this, the decoder can access a larger \"bank\" of memory and attend to the input's required information based on the current decoder RNN output state, $s_t$. This is shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fB5KObXcKo5x35xlIDIcjHTq1q75ejIB\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "In deep learning, attention can be interpreted as a vector of \"importance.\" To predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate how strongly it is correlated with, or \"attends to,\" other elements using the attention vector/weights. These attention weights are then used to generate a new weighted sum of the remaining elements, which represents the target [(source)](https://lilianweng.github.io/posts/2018-06-24-attention/).\n",
        "\n",
        "\n",
        "This, usually, consists of three steps for each decoding step $t$:\n",
        "\n",
        "1. Calculate the score (importance) for each $h_n$, given $s_{t-1}$ and use the softmax function to transform this into an attention vector, $w_{n}$.\n",
        "  - $\\text{score} = a(s_{t‚àí1}, h_{n})$, where $a$ can be any differentiable function, such as the dot product.\n",
        "  - $w_{n} = \\frac{\\exp \\left\\{a\\left(s_{t-1}, h_{n}\\right)\\right\\}}{\\sum_{j=1}^{N} \\exp \\left\\{a\\left(s_{t-1}, h_{j}\\right)\\right\\}}$, where we use the softmax function to transform the raw scores to relative attention weights.\n",
        "2. Generate the final context vector, $c_t$, by summing the products of the attention weights and the encoder context vectors.\n",
        "  - $c_t=\\sum_{n=1}^{N} w_n h_{n}$\n",
        "3. Generate the subsequent decoder state $s_{t+1}$ by combining the current decoder state, $s_t$, with the context vector, $c_t$, via some function, $f$.\n",
        "\n",
        "  - $s_{t+1} = f\\left ( c_t, s_t \\right)$\n",
        "\n",
        "  In Bahdanau et al., 2015, $f$ was a learned feedforward layer taking in the concatenated vector $[c_t; s_t]$, with $a(s_{t‚àí1}, h_{n})$ being the dot product.\n",
        "  \n",
        "Next, let us build up this attention schema, as used in the transformer architecture. We've already calcualed simple dot product attention, where the score was given by $a(s_{t-1}, h_n)=s_{t-1} h_n^\\top$ and we're going to use the same idea again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-MU6rrny8Nj"
      },
      "source": [
        "### Self-attention to Multihead Attention - <font color='blue'>Intermediate</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRuLtxNey_EQ"
      },
      "source": [
        "Self-attention and multi-head attention (MHA) are fundamental components of the transformer architecture. In this section, we'll thoroughly explain the intuition behind these concepts and their implementation. Later, in the **Transformers** section, you'll learn how these attention mechanisms are used to create a sequence-to-sequence model that relies entirely on attention.\n",
        "\n",
        "As we move forward, we'll represent sentences by breaking them down into individual words and encoding each word using the word2vec model discussed earlier. In the Transformers section, we'll explore in more detail how input sequences are transformed into a series of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2z6-NckgNT-"
      },
      "outputs": [],
      "source": [
        "def embed_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Embed a sentence using word2vec; for example use cases only.\n",
        "    \"\"\"\n",
        "    # clean sentence (not necessary if using a proper LLM tokenizer)\n",
        "    sentence = remove_punctuation(sentence)\n",
        "\n",
        "    # extract individual words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # get the word2vec embedding for each word in the sentence\n",
        "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
        "\n",
        "    # return with extra dimension (useful for creating batches later)\n",
        "    return jnp.expand_dims(word_vector_sequence, axis=0), words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AFUEFZGzCTv"
      },
      "source": [
        "#### Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF2V3KI-za9l"
      },
      "source": [
        "Self-attention is an attention mechanism where each vector of a given input sequence attends to the entire sequence. To gain an intuition for why self-attention is important, let us think about the following sentence (example taken from [source](https://jalammar.github.io/illustrated-transformer/)):\n",
        "\n",
        "`\"The animal didn't cross the street because it was too tired.\"`\n",
        "\n",
        "A simple question about this sentence is what the word \"it\" refers to? Even though it might look simple, it can be tough for an algorithm to learn this. This is where self-attention comes in, as it can learn an attention matrix for the word \"it\" where a large weight is assigned to the word \"animal\".\n",
        "\n",
        "Self-attention also allows the model to learn how to interpret words with the same embeddings, such as apple, which can be a company or food, depending on the context. This is very similar to the hidden state found within an RNN, but this process, as you will see, allows the model to attend over the entire sequence in parallel, allowing longer sequences to be utilised.\n",
        "\n",
        "Self-attention consists of three concepts:\n",
        "\n",
        "- Queries, keys and values\n",
        "- Scaled dot product attention\n",
        "- Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwOIMtdZzdTf"
      },
      "source": [
        "##### **Queries, keys and values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEf7QWIWzdo1"
      },
      "source": [
        "Typically all attention mechanisms can be written in terms of `key-value` pairs and `queries` to calculate the attention matrix and new context vector.\n",
        "\n",
        "To gain intuition, one can interpret the `query` vector as containing the information we are interested in obtaining and the `key` vectors as having some information. The `query` vectors are compared to the `key` vectors to get attention scores, where a higher attention score indicates a `key` had relevant information. These attention scores are then used to determine which `values` (which are paired with the `keys`) we should attend to. Or as [Lena Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) puts it:\n",
        "\n",
        "- Query: asking for information\n",
        "- Key: saying that it has some information\n",
        "- Value: giving the information\n",
        "\n",
        "In transformer architectures, we use learnable weights matrices, represented as $W_Q,W_K,W_V$, to project each sequence vector to unique $q$, $k$, and $v$ vectors.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-96YjPxhcqW6FczUYwErGXHp6YpoLltq\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "You will notice that the vectors $q,k,v$ are smaller in size than the input vectors. This will be covered at a later stage, but just know that it is a design choice for transformers and not a requirement to work.\n",
        "\n",
        "This process can also be parallelised, as the input sequence can be represented as a matrix $X$, which can be transformed into query, key, and value matrices $Q$, $K$, and $V$ respectively:\n",
        "\n",
        "$Q=W_QX \\\\ K=W_KX \\\\ V=W_VX$\n",
        "\n",
        "Below we show the code that creates three linear layers, which projects the input data to the $Q,K,V$ matrices, where the output size can be adjusted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc8zjK6eziIV"
      },
      "outputs": [],
      "source": [
        "class SequenceToQKV(nn.Module):\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, X):\n",
        "\n",
        "    # define the method for weight initialisation\n",
        "    initializer = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "\n",
        "    # initialise three linear layers to do the QKV transformations.\n",
        "    # note: this can also be one layer, how do you think you would do it?\n",
        "    q_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    k_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "    v_layer = nn.Dense(self.output_size, kernel_init=initializer)\n",
        "\n",
        "    # transform and return the matrices\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhGZHFsHz_Qp"
      },
      "source": [
        "##### **Scaled dot product attention**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxycHDUW0BVE"
      },
      "source": [
        "Now that we have our `query`, `key` and `value` matrices, it is time to calculate the attention matrix. Remember, in all attention mechanisms; we must first find a score for each vector in the sequence and then use these scores to create a new context vector. In self-attention scoring is done using scaled dot product attention, and then the normalised scores are used as weights to sum the value vectors and create the context vector.\n",
        "\n",
        "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$\n",
        "\n",
        "where the attention scores are calculated by $\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)$ and the scores are then multiplied by $V$ to get the context vector.\n",
        "\n",
        "\n",
        "What happens here is similar to what we did in the dot product attention in the previous section, just applying the mechanism to the sequence itself. For each element in the sequence, we calculate the attention weight matrix between $q_i$ and $K$. We then multiply $V$ by each weight and finally sum all weighted vectors $v_{weighted}$ together to form a new representation for $q_i$. By doing this, we are essentially drowning out irrelevant vectors and bringing up important vectors in the sequence when our focus is on $q_1$.\n",
        "\n",
        "$QK^\\top$ is scaled by the square root of the dimension of the vectors, $\\sqrt{d_k}$, to ensure more stable gradients during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_UYNzrS0Hga"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    \"\"\"\n",
        "    Formula to return scaled dot product attention given QKV matrices\n",
        "    \"\"\"\n",
        "    d_k = key.shape[-1]\n",
        "\n",
        "    # get the raw scores (logits) from dot producting the queries and keys\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "\n",
        "    # scale the raw scores and apply the softmax function to get the attention scores/weights\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "\n",
        "    # multiply the weights by the value matrix to get the output\n",
        "    output = jnp.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuNaEjIm0PhV"
      },
      "source": [
        "Let's now see scaled dot product attention in action. We will take a sentence, embed each word using word2vec, and see what the final self-attention weights look like.\n",
        "\n",
        "We will not use the linear projection layers we would need to train these. Instead, we are going to make things simple and use $X=Q=V=K$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Oy2sWzR0Ok5"
      },
      "outputs": [],
      "source": [
        "# define a sentence\n",
        "sentence = \"I drink coke, but eat steak\"\n",
        "\n",
        "# embed and create QKV matrices\n",
        "word_embeddings, words = embed_sentence(sentence)\n",
        "Q = K = V = word_embeddings\n",
        "\n",
        "# calculate weights and plot\n",
        "outputs, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "# plot the words and the attention weights between them\n",
        "words = remove_punctuation(sentence).split()\n",
        "plot_attention_weight_matrix(attention_weights[0], words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG1Kxljr0Vzw"
      },
      "source": [
        "Keep in mind that we have not trained our attention matrix yet. However, we can see that by utilising the word2vec vectors as our sequence, we can see how scaled dot product attention already is capable of attending to \"eat\" when \"steak\" is our query and that the query \"drink\" attends more to \"coke\" and \"eat\".\n",
        "\n",
        "More resources:\n",
        "\n",
        "[Attention with Q,K,V](https://www.youtube.com/watch?v=k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7B-AgO80gIt"
      },
      "source": [
        "##### **Masked attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdRoKsu70gGW"
      },
      "source": [
        "There are cases where applying self-attention over the entire sequence is not practical. These can include:\n",
        "\n",
        "- Uneven length sequences batched together.\n",
        "  - When sending a batch of sequences through a network, the self-attention expects each sequence to be the same length. One handles this by padding the sequence. When calculating attention, ideally, these padding tokens should not be taken into consideration.\n",
        "- Training a decoder model.\n",
        "  - When training decoder models, such as GPT-3, the decoder has access to the entire target sequence when training (as training is done in parallel). In order to prevent the method from cheating by looking at future tokens, we have to mask the future sequence data so that earlier data can not attend to it.\n",
        "\n",
        "By applying a mask to the final score calculated between queries and keys, we can mitigate the influence of the unwanted sequence vectors. **The vectors are masked by making the score between the query and their respective keys a VERY large negative value.** This results in the softmax function pushing the attention weight very close to zero, and the resulting value will be summed out and not influence the final representation.\n",
        "\n",
        "\n",
        "Putting everything together, masked scaled dot product attention visually looks like this:\n",
        "\n",
        "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"drawing\" width=\"200\"/>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Syx8_5E0eM9"
      },
      "outputs": [],
      "source": [
        "# example of building a mask for tokens of size 32\n",
        "# the mask makes sure that positions only attend to previous positions in the input (causal mask)\n",
        "# we will use this later to insert -inf values into the raw scores\n",
        "mask = jnp.tril(jnp.ones((32, 32)))\n",
        "\n",
        "# plot\n",
        "sns.heatmap(mask, cmap=\"Blues\")\n",
        "plt.title(\"Example of mask that can be applied\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwTJrQ20gDw"
      },
      "source": [
        "Lets now adapt our scaled dot product attention function to implement masked attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVHpyNs_0ePh"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Scaled dot product attention with a causal mask (only allowed to attend to previous positions)\n",
        "    \"\"\"\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "\n",
        "    # get scaled logits using dot product as before\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "\n",
        "    # add optional mask where values along the mask are set to -inf\n",
        "    if mask is not None:\n",
        "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "\n",
        "    # calcualte the attention weights via softmax\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "\n",
        "    # sum with the values to get the output\n",
        "    output = jnp.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWDubQwCs4zG"
      },
      "source": [
        "##### **Multi-head attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHkyjyErsYae"
      },
      "source": [
        "The attention mechanism we've covered so far successfully allows the model to focus on different positions in the input. In practice, the transformer architecture uses a subtle variation of this mechanism, called multi-head attention (MHA).\n",
        "\n",
        "The distinction is minimal; rather than only computing the attention once, the MHA mechanism runs through the scaled dot-product attention multiple times in parallel. According to the paper, *Attention is All You Need*, \"multi-head attention allows the model to **jointly attend** to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\"\n",
        "\n",
        "Multi-head attention can be viewed as a similar strategy to stacking convolution kernels in a CNN layer. This allows the kernels to focus on and learn different features and rules, which is why multiple heads of attention also work.\n",
        "\n",
        "The figure below shows how basic MHA works. The scaled dot product attention discussed earlier is just repeated $N$ times ($N=2$ in this figure), with $3N$ learnable matrices for each head. The outputs from the different heads are then concatenated, whereafter it is fed through a linear projection, which produces the final representation.\n",
        "\n",
        "In practice, MHA significantly out-performs single-head attention.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1q0Oq6IVEkkMfVSpY4LkHBP866mcoIFsh\" alt=\"drawing\" width=\"1000\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtuqNCln9EWW"
      },
      "source": [
        "Let's take a look at how to implement multi-head attention. In simple terms, multi-head attention is like running the attention process multiple times in parallel, using different copies of the Q, K, and V matrices for each \"head.\" This helps the model focus on different parts of the input at the same time. If you're interested in learning more, check out [this blog by Sebastian Raschka](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention) for a detailed explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY2xXLMQ9CB6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int  # Number of attention heads\n",
        "    d_m: int  # Dimension of the model's embeddings\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialize the sequence-to-QKV transformation module\n",
        "        self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "\n",
        "        # Define the initializer for the output linear layer weights\n",
        "        initializer = nn.initializers.variance_scaling(\n",
        "            scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\"\n",
        "        )\n",
        "\n",
        "        # Initialize the output projection layer Wo (used after attention)\n",
        "        self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "    def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "        # If Q, K, or V are not provided, use the input X to generate them\n",
        "        if None in [Q, K, V]:\n",
        "            assert not X is None, \"X has to be provided if either Q, K, or V are not provided\"\n",
        "\n",
        "            # Generate Q, K, and V matrices from the input X\n",
        "            Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "        # Extract the batch size (B), sequence length (T), and embedding size (d_m)\n",
        "        B, T, d_m = K.shape\n",
        "\n",
        "        # Calculate the size of each attention head's embedding (d_m / num_heads)\n",
        "        head_size = d_m // self.num_heads\n",
        "\n",
        "        # Reshape Q, K, V to have separate dimensions for the heads\n",
        "        # B, T, d_m -> B, T, num_heads, head_size -> B, num_heads, T, head_size\n",
        "        q_heads = Q.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        k_heads = K.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        v_heads = V.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "        # Apply scaled dot-product attention to each head\n",
        "        attention, attention_weights = scaled_dot_product_attention(\n",
        "            q_heads, k_heads, v_heads, mask\n",
        "        )\n",
        "\n",
        "        # Reshape the attention output back to its original dimensions\n",
        "        # (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, d_m)\n",
        "        attention = attention.swapaxes(1, 2).reshape(B, T, d_m)\n",
        "\n",
        "        # Apply the output linear transformation Wo to the attention output\n",
        "        X_new = self.Wo(attention)\n",
        "\n",
        "        # If return_weights is True, return both the transformed output and attention weights\n",
        "        if return_weights:\n",
        "            return X_new, attention_weights\n",
        "        else:\n",
        "            # Otherwise, return just the transformed output\n",
        "            return X_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **2. Building your own LLM**  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA_2coZvhAg3"
      },
      "source": [
        "### 2.1 High-level overvierw <font color='orange'>Beginner</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BflycqAw_RF8"
      },
      "source": [
        "The Transformer Architecture was famously introduced in the paper entitled [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) by Vaswani et al.\n",
        "\n",
        "As the title of the paper suggests, such an architecture consists of basically only attention mechanisms along with feed-forward layers and linear layers, as shown in the diagram below.\n",
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"350\" />\n",
        "\n",
        "Transformers and its variations are in the core of Large Language Models and it's not an exaggeration to say that almost all language models out there are Transformer based architectures.\n",
        "\n",
        "As you can see in the diagram the original Transformer architecture consists of two parts, one that receives inputs usually called encoder and another that receives outputs (i.e. targets) called decoder. This is because the transformer was designed for machine translation.\n",
        "\n",
        "The encoder will receive an input sentence in one language and process it through multiple stacked `encoder blocks`. This creates a final representation, which contains helpful information necessary for the decoding task. This output is then fed into stacked `decoder blocks` that produce new outputs in an autoregressive manner.\n",
        "\n",
        "The encoder consists of $N$ identical blocks, which process a sequence of token vectors sequentially. These blocks consist of 3 parts:\n",
        "\n",
        "1. A multi-head attention block. These are the transformer architecture's backbone. They process the data to generate representations for each token, ensuring that the necessary information for the task at hand is represented in the vectors. These are exactly the MHA we covered in the attention section previously.\n",
        "2. An MLP (Multi-Layer Perceptron i.e. a neural network with multiple layers) is applied to each input token separately and identically.\n",
        "3. Residual connection that adds the input tokens to the attended representations and a residual connection between the input to the MLP and its outputs. For both these connections, the result is normalized using layernorm. In certain implementations, these normalization steps are applied to the inputs rather than the outputs. Just like a Resnet, transformers are designed to be very deep models thus, these add and norm blocks are essential for a smooth gradient flow.  \n",
        "\n",
        "Similarly, the decoder block consists of $N$ identical blocks, however there is some variation within these block. Concretely, the different parts are:\n",
        "\n",
        "1. A masked multi-head attention block. This is an MHA block that performs _self-attention_ on the output sequence however this computation is restricted to the inputs that have already been seen. In other words, future tokens are blocked when making predictions.\n",
        "2. A multi-head attention block. This block receives the output of the final encoder block, the transformed tokens, and uses that as the key-value pairs, while using the output of the first MHA block as the query. In doing this, the model attends over the input required to perform the sequence task. This MHA block thus performs _cross-attention_ by looking at the encoder inputs.\n",
        "3. An MLP same as the encoder\n",
        "4. Residual connection same as the encoder.\n",
        "\n",
        "Given this original architecture, there have been several variation with others focusing on the encoder only and others the **decoder only**. Large language models(LLMs) such as GPT-2, GPT-3 and Turing-NLG were born out of decoder only architectures. These architecture look like:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "with the cross attention block missing as no encoder output is available. So to build a language model, we will focus on the decoder only architecture as seen above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbTsk0MdhAhC"
      },
      "source": [
        "### 2.2 Tokenization + Positional encoding <font color='orange'>Beginner</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DehUpfym_RF8"
      },
      "source": [
        "#### 2.2.1 Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBiFpVBu_RF9"
      },
      "source": [
        "\n",
        "Transformers cannot handle raw strings of text. So to process text, the text is first split up into tokens. The tokens are then indexed and each token is assigned an embedding of size $d_{model}$. These embeddings can be learned during training or can come from a pretrained vocabulary of embeddings. This new sequence of token embeddings is then fed into the transformer architecture. This idea is visualised below.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16euh4LADP_mcXywFwKKY3QQQkVplepiI\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "\n",
        "These token IDs are typically predicted when a model generates text, fills in missing words, etc.\n",
        "\n",
        "This process of splitting up text into tokens and assigning an ID to each token is called [tokenisation](https://huggingface.co/docs/transformers/tokenizer_summary). There are various ways to tokenise text, with some methods being trained directly from the data. When using pre-trained transformers, it is crucial to use the same tokeniser that was used to train the model. The previous link has in-depth descriptions of many widely known techniques.\n",
        "\n",
        "Below we show how the [BERT](https://arxiv.org/abs/1810.04805) model's tokeniser tokenises a sentence. We use [Hugging Face](https://huggingface.co/) for this part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJBMvlUA_RF9"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = bert_tokenizer(\"The practical is so much fun\")\n",
        "print(f\"Token IDs: {encoded_input['input_ids']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYbtZTVP_RF9"
      },
      "source": [
        "Here we can see that the tokeniser returns the IDs for each token, as shown in the figure. But counting the number of IDs, we see that it is larger than the number of words in the sentence. Let's print the tokens associated with each ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPZjiLis_RF9"
      },
      "outputs": [],
      "source": [
        "print(f\"Tokens: {bert_tokenizer.decode(encoded_input['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3K8UFlR_RF9"
      },
      "source": [
        "We can see the tokeniser attaches new tokens, `[CLS]` and `[SEP]`, to the start and end of the sequence. This is a BERT-specific requirement for training and inference. Adding special tokens is a very common thing to do. Using special tokens, we can tell a model when a sentence starts or ends or when a new part of the input starts. This can be helpful when performing different tasks.\n",
        "\n",
        "For instance, to pretrain specific transformers, they perform what is known as masked prediction. For this, random tokens in a sequence are replaced by the `[MASK]` token, and the model is trained to predict the correct token ID for the token replaced with that token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djMP4Ijz_RF9"
      },
      "source": [
        "**Drawback of using raw token**:\n",
        "\n",
        "One drawback of using raw tokens is that they lack any indication of the word's position in the sequence. This is evident when considering sentences like \"I am happy\" and \"Am I happy\" - these two phrases have distinct meanings, and the model needs to grasp the word order to understand the intended message accurately.\n",
        "\n",
        "To address this, when converting the inputs into vectors, position vectors are introduced and added to these vectors to indicate the **position** of each word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "639s7Zuk_RF9"
      },
      "source": [
        "#### 2.2.2 Positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-hBFVYo_RF9"
      },
      "source": [
        "In most domains where a transformer can be utilised, there is an underlying order to the tokens produced, be it the order of words in a sentence, the location from which patches are taken in an image or even the steps taken in an RL environment. This order is very important in all cases; just imagine you interpret the sentence \"I have to read this book.\" as \"I have this book to read.\". Both sentences contain the exact same words, yet they have completely different meanings based on the order.\n",
        "\n",
        "As both the encoder and the decoder blocks process all tokens in parallel, the order of tokens is lost in these calculations. To cope with this, the sequence order has to be injected into the tokens directly. This can be done by adding *positional encodings* to the tokens at the start of the encoder and decoder blocks (though some of the latest techniques add positional information in the attention blocks). An example of how positional encodings alter the tokens is shown below.\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eSgnVN2hnEsrjdHygDGwk1kxEi8-dcFo\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "Ideally, these encodings should have these characteristics ([source](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)):\n",
        "* Each time-step should have a unique value\n",
        "* The distance between time steps should stay constant.\n",
        "* The encoding should be able to generalise to longer sequences than seen during training.\n",
        "* The encoding must be deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rklY-aL-_RF9"
      },
      "source": [
        "##### **Sine and cosine functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLcfkMku_RF9"
      },
      "source": [
        "\n",
        "In Attention is All you Need, the authors used a method that can satisfy all these requirements. This involves summing a combination of sine and cosine waves at different frequencies, with the formula for a position encoding at position $D$ shown below, where $i$ is the embedding index and $d_m$ is the token embedding size.\n",
        "\n",
        "\\\\\n",
        "\n",
        "$P_{D}= \\begin{cases}\\sin \\left(\\frac{D}{10000^{i/d_{m}}}\\right), & \\text { if } i \\bmod 2=0 \\\\ \\cos \\left(\\frac{D}{10000^{((i-1)/d_{m}}}\\right), & \\text { otherwise } \\end{cases}$\n",
        "\n",
        "\\\n",
        "\n",
        "Assuming our model as $d_m=8$, the position embedding will look like this:\n",
        "\n",
        "\\\n",
        "$P_{D}=\\left[\\begin{array}{c}\\sin \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{8/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{8/8}}\\right)\\end{array}\\right]$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Let's first create a function that can return these encodings to understand why this will work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5t5D30_RF9"
      },
      "outputs": [],
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "  assert token_embedding % 2 == 0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "  i = jnp.arange(0, token_embedding, 2)\n",
        "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "  frequencies = positions * frequency_steps\n",
        "\n",
        "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "  return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYW-VDOL_RF-"
      },
      "outputs": [],
      "source": [
        "token_sequence_length = 50  # Number of tokens the model will need to process\n",
        "token_embedding = 10000  # token embedding (and positional encoding) dimensions, ensure it is divisible by two\n",
        "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
        "plot_position_encodings(P, token_sequence_length, token_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mjHEDPO_RF-"
      },
      "source": [
        "Looking at the graph above, we can see that for each position index, a unique pattern emerges, where each position index consistently has the same encoding.\n",
        "\n",
        "### **Group Activity**:\n",
        "\n",
        "- <font color='blue'>Take a moment with your friend to explore why this specific pattern appears when `token_sequence_length` is set to 1000, and `token_embedding` is 768.</font>\n",
        "- <font color='blue'>Experiment with smaller values for `token_sequence_length` and `token_embedding` to build a deeper understanding and enhance your discussion.</font>\n",
        "- <font color='blue'>Curious about the constant 10000? Ask your friend why they think it‚Äôs used in the functions above.</font>\n",
        "- <font color='blue'>Now, try setting `token_sequence_length` to 50 and `token_embedding` to a much larger value, like 10000. What do you observe? Do we always need a large token embedding?</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNPg0pnhAhG"
      },
      "source": [
        "### 2.3 Transformer block   <font color='green'>Intermediate</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4vSolF2_RF-"
      },
      "source": [
        "Just like an MLP (a simple neural network that processes input data through multiple layers) or a CNN (a type of neural network that excels at recognizing patterns in images by using convolution layers), transformers are made up of a stack of transformer blocks. In this section, we'll build each of the components needed to create one of these transformer blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTURbfr__RF-"
      },
      "source": [
        "\n",
        "#### 2.3.1 Feed Forward Network (FFN) / Multilayer perceptron (MLP) <font color='orange'>Beginner</font>\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H1pVFxJiSpM_Ozj1eKWNdcFQ5Hn5XsZz\" alt=\"drawing\" width=\"260\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTtFi9AZ_RF-"
      },
      "source": [
        "In the original model, these blocks consist of a simple 2-layer MLP (Multi-Layer Perceptron) that uses ReLU activation. However, GeLU (Gaussian Error Linear Unit) has become very popular, and we will be using it throughout this practical. The formula below represents the feedforward neural network (FFN) with GeLU activation. In this network, the input `x` is first passed through two linear layers with weights `W1` and `W2`, followed by bias terms `b1` and `b2`. The ReLU activation function, often represented by the `max` function, is replaced by the GeLU activation function in this case.\n",
        "\n",
        "$$\n",
        "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
        "$$\n",
        "\n",
        "One can interpret this block as processing what the MHA block has produced and then projecting these new token representations to a space that the next block can use more optimally. Usually, the first layer is very wide, in the range of 2-8 times the size of the token representations. They do this as it is easier to parallelize computations for a single wider layer during training than to parallelize a feedforward block with multiple layers. Thus they can add in more complexity but keep training and inference optimized.\n",
        "\n",
        "**Code task:** Code up a Flax Module that implements the feed forward block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsho1CnW_RF-"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  A 2-layer MLP which widens then narrows the input.\n",
        "\n",
        "  Args:\n",
        "    widening_factor [optional, default=4]: The size of the hidden layer will be d_model * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  widening_factor: int = 4\n",
        "  init_scale: float = 0.25\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      x: [B, T, d_m]\n",
        "    '''\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self.widening_factor * d_m\n",
        "\n",
        "    initializer = nn.initializers.variance_scaling(\n",
        "        scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "    )\n",
        "\n",
        "    # Hint: Layer 1 is a dense layer (fully connected layer) that increases the size of the input by the widening factor.\n",
        "    # Use nn.Dense to create this layer with layer1_size as the output size.\n",
        "    layer1 = # FINISH ME\n",
        "\n",
        "    # Hint: Layer 2 is another dense layer that reduces the size back to the original dimension d_m.\n",
        "    # Use nn.Dense with d_m as the output size to create this layer.\n",
        "    layer2 = # FINISH ME\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))  # Apply the GeLU activation function to the output of layer 1\n",
        "    x = layer2(x)  # Pass the result through layer 2\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qj0nfhH_RF-"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    \"\"\"A 2-layer MLP (Multi-Layer Perceptron) that first expands the input size and then reduces it back.\"\"\"\n",
        "\n",
        "    # widening_factor controls how much the input dimension is expanded in the first layer.\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    # init_scale controls the scaling factor for weight initialization.\n",
        "    init_scale: float = 0.25\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # Get the size of the last dimension of the input (embedding size).\n",
        "        d_m = x.shape[-1]\n",
        "\n",
        "        # Calculate the size of the first layer by multiplying the embedding size by the widening factor.\n",
        "        layer1_size = self.widening_factor * d_m\n",
        "\n",
        "        # Initialize the weights for both layers using a variance scaling initializer.\n",
        "        initializer = nn.initializers.variance_scaling(\n",
        "            scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "        )\n",
        "\n",
        "        # Define the first dense layer, which expands the input size.\n",
        "        layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
        "\n",
        "        # Define the second dense layer, which reduces the size back to the original dimension.\n",
        "        layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
        "\n",
        "        # Apply the first dense layer followed by a GELU activation function.\n",
        "        x = jax.nn.gelu(layer1(x))\n",
        "\n",
        "        # Apply the second dense layer to project the data back to its original dimension.\n",
        "        x = layer2(x)\n",
        "\n",
        "        # Return the final output.\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sts5Vr4i_RF-"
      },
      "source": [
        "#### 2.3.2 Add and Norm block <font color='orange'>Beginner</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWUpf8wt_RF-"
      },
      "source": [
        "In order to get transformers to go deeper, the residual connections are very important to allow an easier flow of gradients through the network. For normalisation, `layer norm` is used. This normalises each token vector independently in the batch. It is found that normalising the vectors improves the convergence and stability of transformers.\n",
        "\n",
        "There are two learnable parameters in layernorm, `scale` and `bias`, which rescales the normalised value. Thus, for each input token in a batch, we calculate the mean, $\\mu_{i}$ and variance $\\sigma_i^2$. We then normalise the token with:\n",
        "\n",
        "$\\hat{x}_i = \\frac{x_i-\\mu_{i}}{\\sigma_i^2 + œµ}$.\n",
        "\n",
        "Then $\\hat{x}$ is rescaled using the learned `scale`, $Œ≥$, and `bias` $Œ≤$, with:\n",
        "\n",
        "$y_i = Œ≥\\hat{x}_i + Œ≤ = LN_{Œ≥,Œ≤}(x_i)$.\n",
        "\n",
        "So our add norm block can be represented as $LN(x+f(x))$, where $f(x)$ is either a MLP or MHA block.\n",
        "\n",
        "**Code task:** Code up a Flax Module that implements the add norm block. It should take as input the processed and unprocessed tokens. Hint: `hk.LayerNorm `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5bLb5Ly_RF_"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, processed_x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: Sequence of tokens before feeding into MHA or FF blocks, with shape [B, T, d_m]\n",
        "      x: Sequence of after being processed by MHA or FF blocks, with shape [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      add_norm_x: Transformed tokens with shape [B, T, d_m]\n",
        "    '''\n",
        "    # Hint: Step 1 involves adding the original input `x` to the processed input `processed_x`.\n",
        "    added = # FINISH ME\n",
        "\n",
        "    # Hint: Step 2 requires applying layer normalization to the result of the addition.\n",
        "    # Use `nn.LayerNorm`, and set `reduction_axes=-1` to apply normalization across the last dimension.\n",
        "    normalised = #FINISH ME\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXSi7BXZ_RF_"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "class AddNorm(nn.Module):\n",
        "    \"\"\"A block that implements the 'Add and Norm' operation used in transformers.\"\"\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, processed_x):\n",
        "        # Step 1: Add the original input (x) to the processed input (processed_x).\n",
        "        added = x + processed_x\n",
        "\n",
        "        # Step 2: Apply layer normalization to the result of the addition.\n",
        "        # - LayerNorm helps to stabilize and improve the training process by normalizing the output.\n",
        "        # - reduction_axes=-1 indicates that normalization is applied across the last dimension (typically the embedding dimension).\n",
        "        # - use_scale=True and use_bias=True allow the layer to learn scaling and bias parameters for further fine-tuning.\n",
        "        normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
        "\n",
        "        # Return the normalized result.\n",
        "        return normalised(added)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91dXd29b_RF_"
      },
      "source": [
        "### 2.4 Building the Transformer Decoder / LLM <font color='green'>Intermediate</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl0UAyvM_RF_"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1MubUcshJTHCqOPTRHixLhrYYLXX9vP_h\" alt=\"drawing\" width=\"260\"/>\n",
        "\n",
        "Most of the groundwork has happened. We have built the positional encoding block, the MHA block, the feed-forward block and the add&norm block.\n",
        "\n",
        "The only part needed is passing inputs to each decoder block and applying the masked MHA block found in the decoder blocks.\n",
        "\n",
        "**Code task:** Code up a FLAX Module that implements the (FFN(norm(MHA(norm(X))))) for the decoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVmSFKZK_RF_"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer decoder block.\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    d_m: Token embedding size\n",
        "    widening factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  num_heads: int\n",
        "  d_m: int\n",
        "  widening_factor: int = 4\n",
        "\n",
        "  def setup(self):\n",
        "    self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      encoder_output: Batch of tokens with was processed by the encoder, with shape [B, T_encoder, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    \"\"\"\n",
        "\n",
        "    attention, attention_weights_1 = # FINISH ME\n",
        "\n",
        "    X = # FINISH ME\n",
        "\n",
        "    projection = # FINISH ME\n",
        "    X = # FINISH ME\n",
        "\n",
        "    return (X, attention_weights_1) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stNZVVv3_RF_"
      },
      "outputs": [],
      "source": [
        "#@title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder block.\n",
        "\n",
        "    Args:\n",
        "        num_heads: The number of attention heads in the Multi-Head Attention (MHA) block.\n",
        "        d_m: The size of the token embeddings.\n",
        "        widening_factor: The factor by which the hidden layer size is expanded in the MLP.\n",
        "    \"\"\"\n",
        "\n",
        "    num_heads: int\n",
        "    d_m: int\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialize the Multi-Head Attention (MHA) block\n",
        "        self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "\n",
        "        # Initialize the AddNorm blocks for residual connections and normalization\n",
        "        self.add_norm1 = AddNorm()  # First AddNorm block after MHA\n",
        "        self.add_norm2 = AddNorm()  # Second AddNorm block after the MLP\n",
        "\n",
        "        # Initialize the FeedForwardBlock (MLP) which processes the data after attention\n",
        "        self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weight=True):\n",
        "        \"\"\"\n",
        "        Forward pass through the DecoderBlock.\n",
        "\n",
        "        Args:\n",
        "            X: Batch of input tokens fed into the decoder, shape [B, T_decoder, d_m]\n",
        "            mask [optional, default=None]: Mask to control which positions the attention is allowed to consider, shape [T_decoder, T_decoder].\n",
        "            return_att_weight [optional, default=True]: If True, returns the attention weights along with the output.\n",
        "\n",
        "        Returns:\n",
        "            If return_att_weight is True, returns a tuple (X, attention_weights_1).\n",
        "            Otherwise, returns the processed token representations X.\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply Multi-Head Attention to the input tokens (X) with optional masking\n",
        "        attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "\n",
        "        # Apply the first AddNorm block (adds the original input X and normalizes)\n",
        "        X = self.add_norm1(X, attention)\n",
        "\n",
        "        # Pass the result through the FeedForwardBlock (MLP) to further process the data\n",
        "        projection = self.MLP(X)\n",
        "\n",
        "        # Apply the second AddNorm block (adds the input from the previous step and normalizes)\n",
        "        X = self.add_norm2(X, projection)\n",
        "\n",
        "        # Return the final output X, and optionally the attention weights\n",
        "        return (X, attention_weights_1) if return_att_weight else X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXXVWd7_RF_"
      },
      "source": [
        "Next, we just put everything together, adding in the positional encodings as well as stacking multiple transformer blocks and adding our prediction layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XBG24Qs_RF_"
      },
      "outputs": [],
      "source": [
        "class LLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer model consisting of several layers of decoder blocks.\n",
        "\n",
        "    Args:\n",
        "        num_heads: Number of attention heads in each Multi-Head Attention (MHA) block.\n",
        "        num_layers: Number of decoder blocks in the model.\n",
        "        d_m: Dimensionality of the token embeddings.\n",
        "        vocab_size: Size of the vocabulary (number of unique tokens).\n",
        "        widening_factor: Factor by which the hidden layer size is expanded in the MLP.\n",
        "    \"\"\"\n",
        "    num_heads: int\n",
        "    num_layers: int\n",
        "    d_m: int\n",
        "    vocab_size: int\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialize a list of decoder blocks, one for each layer in the model\n",
        "        self.blocks = [\n",
        "            DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        # Initialize an embedding layer to convert token IDs into token embeddings\n",
        "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m)\n",
        "\n",
        "        # Initialize a dense layer for predicting the next token in the sequence\n",
        "        self.pred_layer = nn.Dense(self.vocab_size)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weights=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the LLM model.\n",
        "\n",
        "        Args:\n",
        "            X: Batch of input token IDs, shape [B, T_decoder] where B is batch size and T_decoder is sequence length.\n",
        "            mask [optional, default=None]: Mask to control which positions the attention can focus on, shape [T_decoder, T_decoder].\n",
        "            return_att_weights [optional, default=False]: Whether to return the attention weights.\n",
        "\n",
        "        Returns:\n",
        "            logits: The predicted probabilities for each token in the vocabulary.\n",
        "            If return_att_weights is True, also returns the attention weights.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert token IDs to embeddings (shape [B, T_decoder, d_m])\n",
        "        X = self.embedding(X)\n",
        "\n",
        "        # Get the sequence length of the input\n",
        "        sequence_len = X.shape[-2]\n",
        "\n",
        "        # Generate positional encodings and add them to the token embeddings\n",
        "        positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "        X = X + positions\n",
        "\n",
        "        # Initialize a list to store attention weights if needed\n",
        "        if return_att_weights:\n",
        "            att_weights = []\n",
        "\n",
        "        # Pass the embeddings through each decoder block in sequence\n",
        "        for block in self.blocks:\n",
        "            out = block(X, mask, return_att_weights)\n",
        "            if return_att_weights:\n",
        "                # If returning attention weights, unpack the output\n",
        "                X = out[0]\n",
        "                att_weights.append(out[1])\n",
        "            else:\n",
        "                # Otherwise, just update the input for the next block\n",
        "                X = out\n",
        "\n",
        "        # Apply a dense layer followed by a log softmax to get logits (predicted token probabilities)\n",
        "        logits = nn.log_softmax(self.pred_layer(X))\n",
        "\n",
        "        # Return the logits, and optionally, the attention weights\n",
        "        return logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sClFLLkU_RF_"
      },
      "source": [
        "If everything is correct, then if we run the code below, everything should run without any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82CWEa5m_RGA"
      },
      "outputs": [],
      "source": [
        "B, T, d_m, N, vocab_size = 18, 32, 16, 8, 25670\n",
        "\n",
        "llm = LLM(num_heads=1, num_layers=1, d_m=d_m, vocab_size=vocab_size, widening_factor=4)\n",
        "mask = jnp.tril(np.ones((T, T)))\n",
        "\n",
        "# initialise module and get dummy output\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.randint(key, [B, T], 0, vocab_size)\n",
        "params = llm.init(key, X, mask=mask)\n",
        "\n",
        "# extract output from decoder\n",
        "logits, decoder_att_weights = llm.apply(\n",
        "    params,\n",
        "    X,\n",
        "    mask=mask,\n",
        "    return_att_weights=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gve7ssD__RGA"
      },
      "source": [
        "As a final sanity check, we can confirm that our attention weights are working correctly. As shown in the figure below, the decoder's attention weights only focus on previous tokens, as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4NpywYv_RGA"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "plt.suptitle(\"LLM attention weights\")\n",
        "sns.heatmap(decoder_att_weights[0, 0, 0, ...], ax=ax, cmap=\"Blues\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt3tp38G90A"
      },
      "source": [
        "### 2.5 Training your LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agLIpsoh_RGA"
      },
      "source": [
        "#### 2.5.1 Training objective <font color='green'>Intermediate</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOSv1-3B_RGA"
      },
      "source": [
        "A sentence is nothing but a string of words. A LLM aims to predict the next word by considering the current context, namely the words that have come before.\n",
        "\n",
        "Here's the basic idea:\n",
        "\n",
        "To calculate the probability of a full sentence \"word1, word2, ..., last word\" appearing in a given context $c$, the procedure is to break down the sentence into individual words and consider the probability of each word given the words that precede it. These individual probabilities are then multiplied together:\n",
        "\n",
        "$$\\text{Probability of sentence} = \\text{Probability of word1} \\times \\text{Probability of word2} \\times \\ldots \\times \\text{Probability of last word}$$\n",
        "\n",
        "This method is akin to building up a narrative one piece at a time based on the preceding storyline.\n",
        "\n",
        "Mathematically, this is expressed as the likelihood (probability) of a sequence of words $y_1, y_2, ..., y_n$ in a given context $c$, which is achieved by multiplying the probabilities of each word $y_t$ calculated given the predecessors ($y_{<t}$) and the context $c$:\n",
        "\n",
        "$$\n",
        "P\\left(y_{1}, y_{2}, \\ldots, y_{n}, \\mid c\\right)=\\prod_{t=1}^{n} P\\left(y_{t} \\mid y_{<t}, c\\right)\n",
        "$$\n",
        "\n",
        "Here $y_{<t}$ stands for the sequence $y_1, y_2, ..., y_{t-1}$, while $c$ represents the context.\n",
        "\n",
        "This is analogous to solving a jigsaw puzzle where the next piece is predictively placed based on what's already in place.\n",
        "\n",
        "Remember just when training a transformer, we do not work in words, but in tokens. During the training process, the model's parameters are fine-tuned by computing the cross-entropy loss across the predicted token, and the correct token, and then performing backpropagation. The loss for time step \"t\" is computed as:\n",
        "\n",
        "$$ \\text{Loss}_t = - \\sum_{w \\in V} y_t\\log (\\hat{y}_t) $$\n",
        "\n",
        "Here $y_t$ is the actual token at time step $t$, and $\\hat{y}_t$ is the token predicted by the model at the same time step. The loss for the entire sentence is then computed as:\n",
        "\n",
        "$$ \\text{Sentence Loss} = \\frac{1}{n} \\sum^{n}_{t=1} \\text{Loss}_t $$\n",
        "\n",
        "where $n$ is the length of the sequence.\n",
        "\n",
        "This iterative process ultimately hones the model's predictive capabilities over time.\n",
        "\n",
        "**Code task**: Implement the cross-entropy loss function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXmjUYdDHseM"
      },
      "outputs": [],
      "source": [
        "def sequence_loss_fn(logits, targets):\n",
        "  '''\n",
        "  Compute the cross-entropy loss between predicted token ID and true ID.\n",
        "\n",
        "  Args:\n",
        "    logits: An array of shape [batch_size, sequence_length, vocab_size]\n",
        "    targets: The targets we are trying to predict\n",
        "\n",
        "  Returns:\n",
        "    loss: A scalar value representing the mean batch loss\n",
        "  '''\n",
        "\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "\n",
        "  mask = jnp.greater(targets, 0)\n",
        "\n",
        "  # Hint: Compute the cross-entropy loss by first applying `jax.nn.log_softmax(logits)`\n",
        "  # to get the log probabilities for each class. Then, multiply these log probabilities\n",
        "  # by the `target_labels` to focus on the correct class's probability. Sum this result\n",
        "  # along the last axis to get the loss for each token. Finally, apply the mask to the loss,\n",
        "  # sum the masked losses, and normalize by the number of non-padding tokens.\n",
        "  loss = # FINISH ME\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cq5_4WN_RGA"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "VOCAB_SIZE = 25670\n",
        "targets = jnp.array([[0, 2, 0]])\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
        "loss = sequence_loss_fn(X, targets)\n",
        "real_loss = jnp.array(10.966118)\n",
        "assert jnp.allclose(real_loss, loss), \"Not returning the correct value\"\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cthfcbmC_RGA"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def sequence_loss_fn(logits, targets):\n",
        "    \"\"\"Compute the sequence loss between predicted logits and target labels.\"\"\"\n",
        "\n",
        "    # Convert the target indices to one-hot encoded vectors.\n",
        "    # Each target label is converted into a one-hot vector of size VOCAB_SIZE.\n",
        "    target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "\n",
        "    # Ensure that the shape of logits matches the shape of the one-hot encoded targets.\n",
        "    # This is important because we need to compute the loss across matching dimensions.\n",
        "    assert logits.shape == target_labels.shape\n",
        "\n",
        "    # Create a mask that ignores padding tokens in the loss calculation.\n",
        "    # The mask is True (1) where the target value is greater than 0 and False (0) otherwise.\n",
        "    mask = jnp.greater(targets, 0)\n",
        "\n",
        "    # Compute the cross-entropy loss for each token.\n",
        "    # Cross-entropy is calculated as the negative log probability of the correct class.\n",
        "    # jax.nn.log_softmax(logits) gives us the log probabilities for each class.\n",
        "    # We multiply by the target_labels to select the log probability of the correct class.\n",
        "    loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "\n",
        "    # Apply the mask to the loss to ignore padding positions and sum up the losses.\n",
        "    # We then normalize the total loss by the number of non-padding tokens.\n",
        "    loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CSfvGj__RGA"
      },
      "source": [
        "#### 2.5.2 Training models <font color='blue'>Advanced</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIQ_aJGW_RGA"
      },
      "source": [
        "In the next section, we define all the processes required to train the model using the objective described above. A lot of this is now the work required to do training using FLAX.\n",
        "\n",
        "Below we gather the dataset and we shall be training on, which is Karpathy's shakespeare dataset. Its not so important to understand this code, so either just run the cell to load the data, or view the code if you want to understand it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guMHAaSo_RGB"
      },
      "outputs": [],
      "source": [
        "# @title Create Shakespeare dataset and iterator (optional, but run the cell)\n",
        "\n",
        "# Trick to avoid errors when downloading tinyshakespeare.\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "class WordBasedAsciiDatasetForLLM:\n",
        "    \"\"\"In-memory dataset of a single-file ASCII dataset for language-like model.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokenize by splitting the text into words\n",
        "        words = corpus.split()\n",
        "        self.vocab_size = len(set(words))  # Number of unique words\n",
        "\n",
        "        # Create a mapping from words to unique IDs\n",
        "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
        "\n",
        "        # Store the inverse mapping from IDs to words\n",
        "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
        "\n",
        "        # Convert the words in the corpus to their corresponding IDs\n",
        "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Only {num_batches} batches; consider a shorter \"\n",
        "                \"sequence or a smaller batch.\"\n",
        "            )\n",
        "\n",
        "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Yield next mini-batch.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Create the language modeling observation/target pairs.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        \"\"\"Convert a sequence of word IDs to words.\"\"\"\n",
        "        return [self.id_to_word[id] for id in ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBIFg51oQl0"
      },
      "source": [
        "Lets now look how our data is structured for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvH3XPM5_RGB"
      },
      "outputs": [],
      "source": [
        "# sample and look at the data\n",
        "batch_size = 2\n",
        "seq_length = 32\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(obs)))\n",
        "    print(\"ASCII:\", obs)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(\"TEXT:\", ' '.join(train_dataset.ids_to_words(target)))\n",
        "    print(\"ASCII:\", target)\n",
        "\n",
        "print(f\"\\n Total vocabulary size: {train_dataset.vocab_size}\")\n",
        "\n",
        "VOCAB_SIZE = train_dataset.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9vzee53_RGB"
      },
      "source": [
        "Next, let us train our LLM and see how it performs in producing Shakespearian text. First, we will define what happens for every training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGuYBCkekgDw"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(3, 4))\n",
        "def train_step(params, optimizer_state, batch, apply_fn, update_fn):\n",
        "    \"\"\"\n",
        "    Perform a single training step.\n",
        "\n",
        "    Args:\n",
        "        params: The current parameters of the model.\n",
        "        optimizer_state: The current state of the optimizer.\n",
        "        batch: A dictionary containing the input data and target labels for the batch.\n",
        "        apply_fn: The function used to apply the model to the inputs.\n",
        "        update_fn: The function used to update the model parameters based on the gradients.\n",
        "\n",
        "    Returns:\n",
        "        Updated parameters, updated optimizer state, and the computed loss for the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # Get the sequence length (T) from the input data.\n",
        "        T = batch['input'].shape[1]\n",
        "\n",
        "        # Apply the model to the input data, using a lower triangular mask to enforce causality.\n",
        "        # jnp.tril(np.ones((T, T))) creates a lower triangular matrix of ones.\n",
        "        logits = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
        "\n",
        "        # Calculate the loss between the predicted logits and the target labels.\n",
        "        loss = sequence_loss_fn(logits, batch['target'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Compute the loss and its gradients with respect to the parameters.\n",
        "    loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
        "\n",
        "    # Update the optimizer state and calculate the parameter updates based on the gradients.\n",
        "    updates, optimizer_state = update_fn(gradients, optimizer_state)\n",
        "\n",
        "    # Apply the updates to the parameters.\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    # Return the updated parameters, optimizer state, and the loss for the batch.\n",
        "    return params, optimizer_state, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtKWzKIAkfYU"
      },
      "source": [
        "Next we initialise our optimizer and model. Feel free to play with the hyperparameters during the practical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o3q-BZX_RGB"
      },
      "outputs": [],
      "source": [
        "# Define all hyperparameters\n",
        "d_model = 128            # Dimension of token embeddings (d_m)\n",
        "num_heads = 4            # Number of attention heads in Multi-Head Attention\n",
        "num_layers = 1           # Number of decoder blocks in the model\n",
        "widening_factor = 2      # Factor to widen the hidden layer size in the MLP\n",
        "LR = 2e-3                # Learning rate for the optimizer\n",
        "batch_size = 32          # Number of samples per training batch\n",
        "seq_length = 64          # Length of each input sequence (number of tokens)\n",
        "\n",
        "# Set up the training data\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size  # Get the size of the vocabulary from the dataset\n",
        "batch = next(train_dataset)            # Get the first batch of input data\n",
        "\n",
        "# Set the random number generator key for model initialization\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "# Initialize the LLM model with the specified hyperparameters\n",
        "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
        "\n",
        "# Create a causal mask to ensure that the model only attends to previous tokens\n",
        "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "\n",
        "# Initialize the model parameters using the first batch of input data and the mask\n",
        "params = llm.init(rng, batch['input'], mask)\n",
        "\n",
        "# Set up the optimizer using the Adam optimization algorithm with the specified learning rate\n",
        "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)  # Initialize the optimizer state with the model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bPEFakxmvsM"
      },
      "source": [
        "Now we train! This will take a few minutes.. While it trains, have you greeted your neighbour yet?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUAS6tie_RGB"
      },
      "outputs": [],
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 3500\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "VOCAB_SIZE = 25670\n",
        "\n",
        "# Training loop\n",
        "for step in range(MAX_STEPS):\n",
        "    batch = next(train_dataset)\n",
        "    params, optimizer_state, loss = train_step(\n",
        "        params, optimizer_state, batch, llm.apply, optimizer.update)\n",
        "    losses.append(loss)\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGv9c2AFmF4V"
      },
      "source": [
        "#### 2.5.3 Inspecting the trained LLM <font color='orange'>Beginner</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfq61gim_RGB"
      },
      "source": [
        "**Reminder:** remember to run all code presented so far in this section before runnning the cells below!\n",
        "\n",
        "Lets generate some text now and see how our model did. DO NOT STOP THE CELL ONCE IT IS RUNNING, THIS WILL CHRASH THE SESSION."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lt8HTS__RGC"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(2, ))\n",
        "def generate_prediction(params, input, apply_fn):\n",
        "  logits = apply_fn(params, input)\n",
        "  argmax_out = jnp.argmax(logits, axis=-1)\n",
        "  return argmax_out[0][-1].astype(int)\n",
        "\n",
        "def generate_random_shakespeare(llm, params, id_2_word, word_2_id):\n",
        "    '''\n",
        "    Get the model output\n",
        "    '''\n",
        "\n",
        "    prompt = \"Love\"\n",
        "    print(prompt, end=\"\")\n",
        "    tokens = prompt.split()\n",
        "\n",
        "    # predict and append\n",
        "    for i in range(15):\n",
        "      input = jnp.array([[word_2_id[t] for t in tokens]]).astype(int)\n",
        "      prediction = generate_prediction(params, input, llm.apply)\n",
        "      prediction = id_2_word[int(prediction)]\n",
        "      tokens.append(prediction)\n",
        "      print(\" \"+prediction, end=\"\")\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "id_2_word = train_dataset.id_to_word\n",
        "word_2_id = train_dataset.word_to_id\n",
        "\n",
        "generated_shakespeare = generate_random_shakespeare(llm, params, id_2_word, word_2_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwNuMRf_RGC"
      },
      "source": [
        "Finally, we implemented everything above by taking the token ID with the maximum probability of being correct. This is greedy decoding, as we only took the most likely token. It worked well in this use case, but there are cases where we will see a degrading performance when taking this greedy approach, specifically when we are interested in generating realistic text.\n",
        "\n",
        "Other methods exist for sampling from the decoder, with a famous algorithm being beam search. We provide resources below for anyone interested in learning more about this.\n",
        "\n",
        "[Greedy Decoding](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
        "\n",
        "[Beam Search](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## **Conclusion**\n",
        "**Summary:**\n",
        "\n",
        "You've now mastered the essentials of how a Large Language Model (LLM) works, from the fundamentals of attention mechanisms to training your own LLM! These powerful tools have the potential to transform a wide range of tasks. However, like any deep learning model, their magic lies in applying them to the right problems with the right data.\n",
        "\n",
        "Ready to take your skills to the next level? Dive into fine-tuning your own LLMs and unleash even more potential! I highly recommend exploring last year's practical on Parameter Efficient Fine-Tuning Methods for a comprehensive overview of advanced techniques. The journey doesn't stop here‚Äîthere's so much more to discover! [LLMs for Everyone 2023](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
        "\n",
        "The world of LLMs is yours to explore‚Äîgo ahead and create something amazing! üåüüöÄ\n",
        "\n",
        "---\n",
        "\n",
        "**Next Steps:**\n",
        "[**Efficiently Finetuning LLMs with Hugging Face**](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
        "\n",
        "\n",
        "**References:** for further references check the links referenced throughout\n",
        "specific sections of this colab.\n",
        "\n",
        "* [Attention is all you need paper](https://arxiv.org/abs/1706.03762)\n",
        "* [Additional videos on transformers](https://www.youtube.com/playlist?list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s)\n",
        "* [LoRA paper](https://arxiv.org/abs/2106.09685)\n",
        "* [RLHF](https://huggingface.co/blog/rlhf) (how ChatGPT was trained)\n",
        "* [Extending context length](https://kaiokendev.github.io/context):\n",
        "\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2023)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
